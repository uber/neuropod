{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Neuropod \u00b6 What is Neuropod? \u00b6 Neuropod is a library that provides a uniform interface to run deep learning models from multiple frameworks in C++ and Python. Neuropod makes it easy for researchers to build models in a framework of their choosing while also simplifying productionization of these models. It currently supports TensorFlow, PyTorch, TorchScript, Keras and Ludwig . For more information: Uber Engineering blog post introducing Neuropod Talk at NVIDIA GTC Spring 2021 Why use Neuropod? \u00b6 Run models from any supported framework using one API \u00b6 Running a TensorFlow model looks exactly like running a PyTorch model. x = np . array ([ 1 , 2 , 3 , 4 ]) y = np . array ([ 5 , 6 , 7 , 8 ]) for model_path in [ TF_ADDITION_MODEL_PATH , PYTORCH_ADDITION_MODEL_PATH ]: # Load the model neuropod = load_neuropod ( model_path ) # Run inference results = neuropod . infer ({ \"x\" : x , \"y\" : y }) # array([6, 8, 10, 12]) print results [ \"out\" ] See the tutorial , Python guide , or C++ guide for more examples. Some benefits of this include: All of your inference code is framework agnostic. You can easily switch between deep learning frameworks if necessary without changing runtime code. Avoid the learning curve of using the C++ libtorch API and the C/C++ TF API Any Neuropod model can be run from both C++ and Python (even PyTorch models that have not been converted to TorchScript). Define a Problem API \u00b6 This lets you focus more on the problem you're solving rather than the framework you're using to solve it. For example, if you define a problem API for 2d object detection, any model that implements it can reuse all the existing inference code and infrastructure for that problem. INPUT_SPEC = [ # BGR image { \"name\" : \"image\" , \"dtype\" : \"uint8\" , \"shape\" : ( 1200 , 1920 , 3 )}, ] OUTPUT_SPEC = [ # shape: (num_detections, 4): (xmin, ymin, xmax, ymax) # These values are in units of pixels. The origin is the top left corner # with positive X to the right and positive Y towards the bottom of the image { \"name\" : \"boxes\" , \"dtype\" : \"float32\" , \"shape\" : ( \"num_detections\" , 4 )}, # The list of classes that the network can output # This must be some subset of ['vehicle', 'person', 'motorcycle', 'bicycle'] { \"name\" : \"supported_object_classes\" , \"dtype\" : \"string\" , \"shape\" : ( \"num_classes\" ,)}, # The probability of each class for each detection # These should all be floats between 0 and 1 { \"name\" : \"object_class_probability\" , \"dtype\" : \"float32\" , \"shape\" : ( \"num_detections\" , \"num_classes\" )}, ] This lets you Build a single metrics pipeline for a problem Easily compare models solving the same problem (even if they're in different frameworks) Build optimized inference code that can run any model that solves a particular problem Swap out models that solve the same problem at runtime with no code change (even if the models are from different frameworks) Run fast experiments See the tutorial for more details. Build generic tools and pipelines \u00b6 If you have several models that take in a similar set of inputs, you can build and optimize one framework-agnostic input generation pipeline and share it across models. Other benefits \u00b6 Fully self-contained models (including custom ops) Efficient zero-copy operations Tested on platforms including Mac, Linux, Linux (GPU) Four or five versions of each supported framework Five versions of Python Model isolation with out-of-process execution Use multiple different versions of frameworks in the same application Ex: Experimental models using Torch nightly along with models using Torch 1.1.0 Switch from running in-process to running out-of-process with one line of code Getting started \u00b6 See the basic introduction tutorial for an overview of how to get started with Neuropod. The Python guide and C++ guide go into more detail on running Neuropod models.","title":"Welcome"},{"location":"#neuropod","text":"","title":"Neuropod"},{"location":"#what-is-neuropod","text":"Neuropod is a library that provides a uniform interface to run deep learning models from multiple frameworks in C++ and Python. Neuropod makes it easy for researchers to build models in a framework of their choosing while also simplifying productionization of these models. It currently supports TensorFlow, PyTorch, TorchScript, Keras and Ludwig . For more information: Uber Engineering blog post introducing Neuropod Talk at NVIDIA GTC Spring 2021","title":"What is Neuropod?"},{"location":"#why-use-neuropod","text":"","title":"Why use Neuropod?"},{"location":"#run-models-from-any-supported-framework-using-one-api","text":"Running a TensorFlow model looks exactly like running a PyTorch model. x = np . array ([ 1 , 2 , 3 , 4 ]) y = np . array ([ 5 , 6 , 7 , 8 ]) for model_path in [ TF_ADDITION_MODEL_PATH , PYTORCH_ADDITION_MODEL_PATH ]: # Load the model neuropod = load_neuropod ( model_path ) # Run inference results = neuropod . infer ({ \"x\" : x , \"y\" : y }) # array([6, 8, 10, 12]) print results [ \"out\" ] See the tutorial , Python guide , or C++ guide for more examples. Some benefits of this include: All of your inference code is framework agnostic. You can easily switch between deep learning frameworks if necessary without changing runtime code. Avoid the learning curve of using the C++ libtorch API and the C/C++ TF API Any Neuropod model can be run from both C++ and Python (even PyTorch models that have not been converted to TorchScript).","title":"Run models from any supported framework using one API"},{"location":"#define-a-problem-api","text":"This lets you focus more on the problem you're solving rather than the framework you're using to solve it. For example, if you define a problem API for 2d object detection, any model that implements it can reuse all the existing inference code and infrastructure for that problem. INPUT_SPEC = [ # BGR image { \"name\" : \"image\" , \"dtype\" : \"uint8\" , \"shape\" : ( 1200 , 1920 , 3 )}, ] OUTPUT_SPEC = [ # shape: (num_detections, 4): (xmin, ymin, xmax, ymax) # These values are in units of pixels. The origin is the top left corner # with positive X to the right and positive Y towards the bottom of the image { \"name\" : \"boxes\" , \"dtype\" : \"float32\" , \"shape\" : ( \"num_detections\" , 4 )}, # The list of classes that the network can output # This must be some subset of ['vehicle', 'person', 'motorcycle', 'bicycle'] { \"name\" : \"supported_object_classes\" , \"dtype\" : \"string\" , \"shape\" : ( \"num_classes\" ,)}, # The probability of each class for each detection # These should all be floats between 0 and 1 { \"name\" : \"object_class_probability\" , \"dtype\" : \"float32\" , \"shape\" : ( \"num_detections\" , \"num_classes\" )}, ] This lets you Build a single metrics pipeline for a problem Easily compare models solving the same problem (even if they're in different frameworks) Build optimized inference code that can run any model that solves a particular problem Swap out models that solve the same problem at runtime with no code change (even if the models are from different frameworks) Run fast experiments See the tutorial for more details.","title":"Define a Problem API"},{"location":"#build-generic-tools-and-pipelines","text":"If you have several models that take in a similar set of inputs, you can build and optimize one framework-agnostic input generation pipeline and share it across models.","title":"Build generic tools and pipelines"},{"location":"#other-benefits","text":"Fully self-contained models (including custom ops) Efficient zero-copy operations Tested on platforms including Mac, Linux, Linux (GPU) Four or five versions of each supported framework Five versions of Python Model isolation with out-of-process execution Use multiple different versions of frameworks in the same application Ex: Experimental models using Torch nightly along with models using Torch 1.1.0 Switch from running in-process to running out-of-process with one line of code","title":"Other benefits"},{"location":"#getting-started","text":"See the basic introduction tutorial for an overview of how to get started with Neuropod. The Python guide and C++ guide go into more detail on running Neuropod models.","title":"Getting started"},{"location":"cppguide/","text":"C++ Guide \u00b6 This guide walks through the Neuropod C++ API in detail and goes over many different ways of working with models. Tip The Neuropod runtime interface is identical for all frameworks so this guide applies for models from all supported frameworks (including TensorFlow, PyTorch, Keras, and TorchScript) Loading a Neuropod \u00b6 The simplest way to load a neuropod is as follows: #include \"neuropod/neuropod.hh\" Neuropod neuropod ( PATH_TO_MY_MODEL ); where PATH_TO_MY_MODEL is the path to a model exported using one of the packagers. Options \u00b6 You can also provide runtime options when loading a model. To select what device to run the model on, you can specify the visible_device option: neuropod :: RuntimeOptions opts ; // Set the visible device to the first GPU opts . visible_device = Device :: GPU0 ; Neuropod neuropod ( PATH_TO_MY_MODEL , opts ); This defaults to GPU0 . If no GPUs are available, Neuropod will attempt to fallback to CPU. Setting opts.visible_device = Device::CPU will force the model to run on CPU. For more details, see all the options here Get the inputs and outputs of a model \u00b6 To get the inputs and outputs of a model, you can do this: Neuropod neuropod ( PATH_TO_MY_MODEL ); // Both of these are `std::vector<TensorSpec>` const auto & inputs = neuropod . get_inputs (); const auto & outputs = neuropod . get_outputs (); for ( const auto & item : inputs ) { // A `TensorSpec` is a struct with a `name`, `dims` and `type` std :: cout << \"Tensor name: \" << item . name << std :: endl ; } For more details, see TensorSpec Tensor Types \u00b6 The following tensor types are supported: float double string int8 int16 int32 int64 uint8 uint16 uint32 uint64 Note uint16 , uint32 , and uint64 are not supported by PyTorch or TorchScript. See the supported type list in the PyTorch documentation. Note TorchScript does not have support for string tensors so we represent them as lists of strings. Therefore TorchScript Neuropod models only support 1D string \"tensors\". See here for example usage. Creating tensors \u00b6 Tip See the Efficient Tensor Creation page for a guide on the approach that best fits your use case. There are many different ways to create NeuropodTensor s, but all of them start with an allocator . To get an allocator for a loaded model, you can do something like: Neuropod neuropod ( PATH_TO_MY_MODEL ); auto allocator = neuropod . get_tensor_allocator (); For scenarios where a model isn't loaded (e.g. unit tests), you can use a generic tensor allocator: #include \"neuropod/core/generic_tensor.hh\" auto allocator = neuropod :: get_generic_tensor_allocator (); Allocate new memory \u00b6 For this, we just need the dimensions and type of the tensor we want to allocate. auto tensor = allocator -> allocate_tensor < float > ({ 1 , 2 , 3 }); You can also manually specify the type without using a templated function auto tensor = allocator -> allocate_tensor ({ 1 , 2 , 3 }, neuropod :: FLOAT_TENSOR ); To do something useful with these tensors, see the Interacting with Tensors section below. From existing memory \u00b6 Neuropod provides a way to wrap existing memory and use it in a zero-copy way. To do this, it needs four things: The dimension of the tensor to create The type of tensor to create A pointer to the data to wrap Note: this should be 64 byte aligned A deleter function This deleter function is called once Neuropod (and the underlying libraries) are done using this data. Until this function is called, it is not safe to deallocate the data. It is very important to pass in a correct deleter to make sure the memory doesn't get prematurely deallocated. Some examples are below. cv::Mat \u00b6 cv :: Mat image = ... // An image from somewhere auto tensor = allocator -> tensor_from_memory < uint8_t > ( // Dimensions { 1 , image . rows , image . cols , image . channels ()}, // Data image . data , // Deleter [ image ]( void * unused ) { // By capturing `image` in this deleter, we ensure // that the underlying data does not get deallocated // before we're done with the tensor. } ); Tip You can also specify the type without using a templated function cv :: Mat image = ... // An image from somewhere auto tensor = allocator -> tensor_from_memory ( // Dimensions { 1 , image . rows , image . cols , image . channels ()}, // Tensor Type get_tensor_type_from_cv_mat ( image ), // Data image . data , // Deleter [ image ]( void * unused ) {} ); Note Utilities for wrapping types from common libraries will be added in a future release. Eigen \u00b6 #include \"neuropod/conversions/eigen.hh\" auto tensor = allocator -> allocate_tensor < float > ({ 1 , 2 , 3 }); // Returns an `Eigen::Map` auto eigen_map = neuropod :: as_eigen ( * tensor ); See the Eigen docs for more details. Note If you're not using the features of Eigen and just need simple element access, use accessors instead. Factory functions \u00b6 These functions are useful for creating test data. zeros \u00b6 Returns a tensor of type T and shape input_dims filled with zeros auto zeros = allocator -> zeros < T > ( input_dims ); ones \u00b6 Returns a tensor of type T and shape input_dims filled with ones auto ones = allocator -> ones < T > ( input_dims ); full \u00b6 Returns a tensor of type T and shape input_dims filled with fill_value auto full = allocator -> full < T > ( input_dims , fill_value ); randn \u00b6 Returns a tensor of type T and shape input_dims filled with random numbers from a normal distribution with mean mean and standard deviation stddev . auto full = allocator -> randn < T > ( input_dims , mean = 0 , stddev = 1 ); arange \u00b6 Returns a 1D tensor of type T containing a sequence of numbers starting at start with a step size of step . auto range1 = allocator -> arange < T > ( end ); auto range2 = allocator -> arange < T > ( start , end , step = 1 ); Example: // 0, 1, 2, 3, 4 auto range1 = allocator -> arange < float > ( 5 ); // 2, 3, 4, 5 auto range2 = allocator -> arange < float > ( 2 , 6 ); // 0, 2, 4, 6, 8 auto range3 = allocator -> arange < float > ( 0 , 10 , 2 ); eye \u00b6 Returns an identity matrix of type T and shape ( M , N ). This matrix has ones on the diagonal and zeros everywhere else. auto eye1 = allocator -> eye < T > ( M , N ); Example: // 1, 0, 0, 0, // 0, 1, 0, 0, // 0, 0, 1, 0, // 0, 0, 0, 1 auto eye1 = allocator -> eye < float > ( 4 , 4 ); // 1, 0, 0, 0, 0, 0, 0, // 0, 1, 0, 0, 0, 0, 0, // 0, 0, 1, 0, 0, 0, 0 auto eye2 = allocator -> eye < float > ( 3 , 7 ); Interacting with Tensors \u00b6 This section goes over various ways of interacting with existing tensors. Types of tensors \u00b6 Neuropod has a few different ways of representing tensors: NeuropodValue , NeuropodTensor , and TypedNeuropodTensor<T> NeuropodValue is the base type and represents any value that the library can store and pass around. A NeuropodTensor is a NeuropodValue that is a tensor. This adds metadata functionality (dimensions, type, num elements, etc.), but does not allow data access. A TypedNeuropodTensor<T> is a NeuropodTensor of a specific type. This level of the hierarchy adds type-safe data access. This is what the class hierarchy looks like: graph TD; NeuropodValue --> NeuropodTensor; NeuropodValue -.-> non_tensor[Non-Tensor types]; NeuropodTensor --> TypedNeuropodTensor&ltT&gt; style non_tensor opacity:0.1,fill:#aaa,stroke:#333,stroke-width:4px To convert from a NeuropodValue to a NeuropodTensor , you can use as_tensor() . auto my_value = ... auto my_tensor = my_value -> as_tensor (); To convert from a NeuropodValue or NeuropodTensor to a TypedNeuropodTensor of a specific type, you can use as_typed_tensor<T>() . This does a type-checked downcast to the requested type and throws an error if the requested type does not match the actual type of the tensor. auto my_value = ... auto my_float_tensor = my_value -> as_typed_tensor < float > (); // This will throw an error auto my_uint_tensor = my_value -> as_typed_tensor < uint8_t > (); The sections below will go over some more usage and examples. Note Most use cases will not require usage of these methods (as the factories and templated allocators already return TypedNeuropodTensor s). Tip Generally, data access requires a TypedNeuropodTensor<T> and metadata access requires at least a NeuropodTensor Copy data into a tensor \u00b6 Requires TypedNeuropodTensor<T> If you want to copy in data (and can't wrap it using the tensor_from_memory API above), you can do something like this: float * my_data = ...; size_t num_elements = ...; tensor -> copy_from ( my_data , num_elements ); You can also copy data from a vector: std :: vector < float > my_data ; tensor -> copy_from ( my_data ); Directly set/get data \u00b6 Requires TypedNeuropodTensor<T> You can do this using the accessor interface, which is very similar to PyTorch's accessor interface. auto tensor = allocator -> allocate_tensor < float > ({ 6 , 6 }); // 2 is the number of dimensions of this tensor auto accessor = tensor -> accessor < 2 > (); accessor [ 5 ][ 3 ] = 1.0 ; Range-based for loops work with accessors as well: auto tensor = allocator -> allocate_tensor < float > ({ 3 , 5 }); // 2 is the number of dimensions of this tensor auto accessor = tensor -> accessor < 2 > (); for ( const auto & row : accessor ) { for ( const auto & item : row ) { // Do something } } Example with string tensors: auto tensor = allocator -> allocate_tensor < std :: string > ({ 3 , 5 }); // 2 is the number of dimensions of this tensor auto accessor = tensor -> accessor < 2 > (); for ( int i = 0 ; i < 3 ; i ++ ) { for ( int j = 0 ; j < 5 ; j ++ ) { accessor [ i ][ j ] = std :: to_string ( i * 5 + j ); } } Individual element access with accessors is very efficient and comparable to raw pointer operations during an optimized build. Note See the Efficient Tensor Creation page for a guide on the approach that best fits your use case. Using Get the dimensions of a tensor \u00b6 Requires NeuropodTensor const auto & dims = tensor -> get_dims (); Get the number of elements in a tensor \u00b6 Requires NeuropodTensor auto num_elements = tensor -> get_num_elements (); Get the type of a tensor \u00b6 Requires NeuropodTensor auto tensor_type = tensor -> get_tensor_type (); Get a raw pointer to the underlying data \u00b6 Requires TypedNeuropodTensor<T> auto data = tensor -> get_raw_data_ptr (); Note This method does not work for String tensors. Use accessors instead Get the data as a vector \u00b6 Requires TypedNeuropodTensor<T> auto data = tensor -> get_data_as_vector (); Warning This method performs a copy Inference \u00b6 The basic inference method of a neuropod looks like this: std :: unique_ptr < NeuropodValueMap > infer ( const NeuropodValueMap & inputs ); A NeuropodValueMap is just a map from std::string to std::shared_ptr<NeuropodValue> Interacting with it is identical to interacting with a std::unordered_map . For example: // Get an allocator auto alloctor = neuropod . get_tensor_allocator (); // Create some tensors auto x = allocator -> randn < float > ({ 5 , 5 }); auto y = allocator -> ones < float > ({ 5 , 5 }); // Run inference const auto output_data = neuropod . infer ({ { \"x\" , x }, { \"y\" , y } }); // Get the outputs auto z = output_data -> at ( \"z\" ) -> as_typed_tensor < float > (); You can also get a subset of the model's output by providing a list of requested outputs: std :: unique_ptr < NeuropodValueMap > infer ( const NeuropodValueMap & inputs , const std :: vector < std :: string > requested_outputs ); For example, if you wanted to return a map containing only the tensor \"z\", you could do this: const auto output_data = neuropod . infer ( input_data , { \"z\" }); Serialization \u00b6 All built-in NeuropodValue types are serializable. Furthermore, NeuropodValueMap is also serializable. // A stream to serialize to. Any ostream is allowed, but we use a // stringstream in this example std :: stringstream ss ; neuropod :: NeuropodValueMap data = ...; neuropod :: serialize ( my_stream , data ); Similarly, deserializing is just as easy. auto deserialized = neuropod :: deserialize < neuropod :: NeuropodValueMap > ( ss , allocator ); Note Serialization and deserialization works across Python and C++. See the Python bindings docs for more Warning The goal for this API is to support transient serialization. There are no guarantees about backwards compatibility so this API should not be used for long term storage of data","title":"C++ Guide"},{"location":"cppguide/#c-guide","text":"This guide walks through the Neuropod C++ API in detail and goes over many different ways of working with models. Tip The Neuropod runtime interface is identical for all frameworks so this guide applies for models from all supported frameworks (including TensorFlow, PyTorch, Keras, and TorchScript)","title":"C++ Guide"},{"location":"cppguide/#loading-a-neuropod","text":"The simplest way to load a neuropod is as follows: #include \"neuropod/neuropod.hh\" Neuropod neuropod ( PATH_TO_MY_MODEL ); where PATH_TO_MY_MODEL is the path to a model exported using one of the packagers.","title":"Loading a Neuropod"},{"location":"cppguide/#options","text":"You can also provide runtime options when loading a model. To select what device to run the model on, you can specify the visible_device option: neuropod :: RuntimeOptions opts ; // Set the visible device to the first GPU opts . visible_device = Device :: GPU0 ; Neuropod neuropod ( PATH_TO_MY_MODEL , opts ); This defaults to GPU0 . If no GPUs are available, Neuropod will attempt to fallback to CPU. Setting opts.visible_device = Device::CPU will force the model to run on CPU. For more details, see all the options here","title":"Options"},{"location":"cppguide/#get-the-inputs-and-outputs-of-a-model","text":"To get the inputs and outputs of a model, you can do this: Neuropod neuropod ( PATH_TO_MY_MODEL ); // Both of these are `std::vector<TensorSpec>` const auto & inputs = neuropod . get_inputs (); const auto & outputs = neuropod . get_outputs (); for ( const auto & item : inputs ) { // A `TensorSpec` is a struct with a `name`, `dims` and `type` std :: cout << \"Tensor name: \" << item . name << std :: endl ; } For more details, see TensorSpec","title":"Get the inputs and outputs of a model"},{"location":"cppguide/#tensor-types","text":"The following tensor types are supported: float double string int8 int16 int32 int64 uint8 uint16 uint32 uint64 Note uint16 , uint32 , and uint64 are not supported by PyTorch or TorchScript. See the supported type list in the PyTorch documentation. Note TorchScript does not have support for string tensors so we represent them as lists of strings. Therefore TorchScript Neuropod models only support 1D string \"tensors\". See here for example usage.","title":"Tensor Types"},{"location":"cppguide/#creating-tensors","text":"Tip See the Efficient Tensor Creation page for a guide on the approach that best fits your use case. There are many different ways to create NeuropodTensor s, but all of them start with an allocator . To get an allocator for a loaded model, you can do something like: Neuropod neuropod ( PATH_TO_MY_MODEL ); auto allocator = neuropod . get_tensor_allocator (); For scenarios where a model isn't loaded (e.g. unit tests), you can use a generic tensor allocator: #include \"neuropod/core/generic_tensor.hh\" auto allocator = neuropod :: get_generic_tensor_allocator ();","title":"Creating tensors"},{"location":"cppguide/#allocate-new-memory","text":"For this, we just need the dimensions and type of the tensor we want to allocate. auto tensor = allocator -> allocate_tensor < float > ({ 1 , 2 , 3 }); You can also manually specify the type without using a templated function auto tensor = allocator -> allocate_tensor ({ 1 , 2 , 3 }, neuropod :: FLOAT_TENSOR ); To do something useful with these tensors, see the Interacting with Tensors section below.","title":"Allocate new memory"},{"location":"cppguide/#from-existing-memory","text":"Neuropod provides a way to wrap existing memory and use it in a zero-copy way. To do this, it needs four things: The dimension of the tensor to create The type of tensor to create A pointer to the data to wrap Note: this should be 64 byte aligned A deleter function This deleter function is called once Neuropod (and the underlying libraries) are done using this data. Until this function is called, it is not safe to deallocate the data. It is very important to pass in a correct deleter to make sure the memory doesn't get prematurely deallocated. Some examples are below.","title":"From existing memory"},{"location":"cppguide/#cvmat","text":"cv :: Mat image = ... // An image from somewhere auto tensor = allocator -> tensor_from_memory < uint8_t > ( // Dimensions { 1 , image . rows , image . cols , image . channels ()}, // Data image . data , // Deleter [ image ]( void * unused ) { // By capturing `image` in this deleter, we ensure // that the underlying data does not get deallocated // before we're done with the tensor. } ); Tip You can also specify the type without using a templated function cv :: Mat image = ... // An image from somewhere auto tensor = allocator -> tensor_from_memory ( // Dimensions { 1 , image . rows , image . cols , image . channels ()}, // Tensor Type get_tensor_type_from_cv_mat ( image ), // Data image . data , // Deleter [ image ]( void * unused ) {} ); Note Utilities for wrapping types from common libraries will be added in a future release.","title":"cv::Mat"},{"location":"cppguide/#eigen","text":"#include \"neuropod/conversions/eigen.hh\" auto tensor = allocator -> allocate_tensor < float > ({ 1 , 2 , 3 }); // Returns an `Eigen::Map` auto eigen_map = neuropod :: as_eigen ( * tensor ); See the Eigen docs for more details. Note If you're not using the features of Eigen and just need simple element access, use accessors instead.","title":"Eigen"},{"location":"cppguide/#factory-functions","text":"These functions are useful for creating test data.","title":"Factory functions"},{"location":"cppguide/#zeros","text":"Returns a tensor of type T and shape input_dims filled with zeros auto zeros = allocator -> zeros < T > ( input_dims );","title":"zeros"},{"location":"cppguide/#ones","text":"Returns a tensor of type T and shape input_dims filled with ones auto ones = allocator -> ones < T > ( input_dims );","title":"ones"},{"location":"cppguide/#full","text":"Returns a tensor of type T and shape input_dims filled with fill_value auto full = allocator -> full < T > ( input_dims , fill_value );","title":"full"},{"location":"cppguide/#randn","text":"Returns a tensor of type T and shape input_dims filled with random numbers from a normal distribution with mean mean and standard deviation stddev . auto full = allocator -> randn < T > ( input_dims , mean = 0 , stddev = 1 );","title":"randn"},{"location":"cppguide/#arange","text":"Returns a 1D tensor of type T containing a sequence of numbers starting at start with a step size of step . auto range1 = allocator -> arange < T > ( end ); auto range2 = allocator -> arange < T > ( start , end , step = 1 ); Example: // 0, 1, 2, 3, 4 auto range1 = allocator -> arange < float > ( 5 ); // 2, 3, 4, 5 auto range2 = allocator -> arange < float > ( 2 , 6 ); // 0, 2, 4, 6, 8 auto range3 = allocator -> arange < float > ( 0 , 10 , 2 );","title":"arange"},{"location":"cppguide/#eye","text":"Returns an identity matrix of type T and shape ( M , N ). This matrix has ones on the diagonal and zeros everywhere else. auto eye1 = allocator -> eye < T > ( M , N ); Example: // 1, 0, 0, 0, // 0, 1, 0, 0, // 0, 0, 1, 0, // 0, 0, 0, 1 auto eye1 = allocator -> eye < float > ( 4 , 4 ); // 1, 0, 0, 0, 0, 0, 0, // 0, 1, 0, 0, 0, 0, 0, // 0, 0, 1, 0, 0, 0, 0 auto eye2 = allocator -> eye < float > ( 3 , 7 );","title":"eye"},{"location":"cppguide/#interacting-with-tensors","text":"This section goes over various ways of interacting with existing tensors.","title":"Interacting with Tensors"},{"location":"cppguide/#types-of-tensors","text":"Neuropod has a few different ways of representing tensors: NeuropodValue , NeuropodTensor , and TypedNeuropodTensor<T> NeuropodValue is the base type and represents any value that the library can store and pass around. A NeuropodTensor is a NeuropodValue that is a tensor. This adds metadata functionality (dimensions, type, num elements, etc.), but does not allow data access. A TypedNeuropodTensor<T> is a NeuropodTensor of a specific type. This level of the hierarchy adds type-safe data access. This is what the class hierarchy looks like: graph TD; NeuropodValue --> NeuropodTensor; NeuropodValue -.-> non_tensor[Non-Tensor types]; NeuropodTensor --> TypedNeuropodTensor&ltT&gt; style non_tensor opacity:0.1,fill:#aaa,stroke:#333,stroke-width:4px To convert from a NeuropodValue to a NeuropodTensor , you can use as_tensor() . auto my_value = ... auto my_tensor = my_value -> as_tensor (); To convert from a NeuropodValue or NeuropodTensor to a TypedNeuropodTensor of a specific type, you can use as_typed_tensor<T>() . This does a type-checked downcast to the requested type and throws an error if the requested type does not match the actual type of the tensor. auto my_value = ... auto my_float_tensor = my_value -> as_typed_tensor < float > (); // This will throw an error auto my_uint_tensor = my_value -> as_typed_tensor < uint8_t > (); The sections below will go over some more usage and examples. Note Most use cases will not require usage of these methods (as the factories and templated allocators already return TypedNeuropodTensor s). Tip Generally, data access requires a TypedNeuropodTensor<T> and metadata access requires at least a NeuropodTensor","title":"Types of tensors"},{"location":"cppguide/#copy-data-into-a-tensor","text":"Requires TypedNeuropodTensor<T> If you want to copy in data (and can't wrap it using the tensor_from_memory API above), you can do something like this: float * my_data = ...; size_t num_elements = ...; tensor -> copy_from ( my_data , num_elements ); You can also copy data from a vector: std :: vector < float > my_data ; tensor -> copy_from ( my_data );","title":"Copy data into a tensor"},{"location":"cppguide/#directly-setget-data","text":"Requires TypedNeuropodTensor<T> You can do this using the accessor interface, which is very similar to PyTorch's accessor interface. auto tensor = allocator -> allocate_tensor < float > ({ 6 , 6 }); // 2 is the number of dimensions of this tensor auto accessor = tensor -> accessor < 2 > (); accessor [ 5 ][ 3 ] = 1.0 ; Range-based for loops work with accessors as well: auto tensor = allocator -> allocate_tensor < float > ({ 3 , 5 }); // 2 is the number of dimensions of this tensor auto accessor = tensor -> accessor < 2 > (); for ( const auto & row : accessor ) { for ( const auto & item : row ) { // Do something } } Example with string tensors: auto tensor = allocator -> allocate_tensor < std :: string > ({ 3 , 5 }); // 2 is the number of dimensions of this tensor auto accessor = tensor -> accessor < 2 > (); for ( int i = 0 ; i < 3 ; i ++ ) { for ( int j = 0 ; j < 5 ; j ++ ) { accessor [ i ][ j ] = std :: to_string ( i * 5 + j ); } } Individual element access with accessors is very efficient and comparable to raw pointer operations during an optimized build. Note See the Efficient Tensor Creation page for a guide on the approach that best fits your use case. Using","title":"Directly set/get data"},{"location":"cppguide/#get-the-dimensions-of-a-tensor","text":"Requires NeuropodTensor const auto & dims = tensor -> get_dims ();","title":"Get the dimensions of a tensor"},{"location":"cppguide/#get-the-number-of-elements-in-a-tensor","text":"Requires NeuropodTensor auto num_elements = tensor -> get_num_elements ();","title":"Get the number of elements in a tensor"},{"location":"cppguide/#get-the-type-of-a-tensor","text":"Requires NeuropodTensor auto tensor_type = tensor -> get_tensor_type ();","title":"Get the type of a tensor"},{"location":"cppguide/#get-a-raw-pointer-to-the-underlying-data","text":"Requires TypedNeuropodTensor<T> auto data = tensor -> get_raw_data_ptr (); Note This method does not work for String tensors. Use accessors instead","title":"Get a raw pointer to the underlying data"},{"location":"cppguide/#get-the-data-as-a-vector","text":"Requires TypedNeuropodTensor<T> auto data = tensor -> get_data_as_vector (); Warning This method performs a copy","title":"Get the data as a vector"},{"location":"cppguide/#inference","text":"The basic inference method of a neuropod looks like this: std :: unique_ptr < NeuropodValueMap > infer ( const NeuropodValueMap & inputs ); A NeuropodValueMap is just a map from std::string to std::shared_ptr<NeuropodValue> Interacting with it is identical to interacting with a std::unordered_map . For example: // Get an allocator auto alloctor = neuropod . get_tensor_allocator (); // Create some tensors auto x = allocator -> randn < float > ({ 5 , 5 }); auto y = allocator -> ones < float > ({ 5 , 5 }); // Run inference const auto output_data = neuropod . infer ({ { \"x\" , x }, { \"y\" , y } }); // Get the outputs auto z = output_data -> at ( \"z\" ) -> as_typed_tensor < float > (); You can also get a subset of the model's output by providing a list of requested outputs: std :: unique_ptr < NeuropodValueMap > infer ( const NeuropodValueMap & inputs , const std :: vector < std :: string > requested_outputs ); For example, if you wanted to return a map containing only the tensor \"z\", you could do this: const auto output_data = neuropod . infer ( input_data , { \"z\" });","title":"Inference"},{"location":"cppguide/#serialization","text":"All built-in NeuropodValue types are serializable. Furthermore, NeuropodValueMap is also serializable. // A stream to serialize to. Any ostream is allowed, but we use a // stringstream in this example std :: stringstream ss ; neuropod :: NeuropodValueMap data = ...; neuropod :: serialize ( my_stream , data ); Similarly, deserializing is just as easy. auto deserialized = neuropod :: deserialize < neuropod :: NeuropodValueMap > ( ss , allocator ); Note Serialization and deserialization works across Python and C++. See the Python bindings docs for more Warning The goal for this API is to support transient serialization. There are no guarantees about backwards compatibility so this API should not be used for long term storage of data","title":"Serialization"},{"location":"developing/","text":"Building Neuropod \u00b6 Neuropod uses Bazel as a build system. There are a few ways to build the project: Natively on Linux and Mac In Docker (preferred) Natively on Linux and Mac \u00b6 The following sets up a local environment for building and testing: # Install system dependencies (e.g. bazel) ./build/install_system_deps.sh # Install python dependencies (e.g. numpy) # Note: This creates a virtualenv for neuropod and installs all deps in it ./build/install_python_deps.sh After the above steps, you can run the following scripts: # Build ./build/build.sh # Run tests ./build/test.sh # Or ./build/test_gpu.sh to run all tests Note The above commands run all python code in the virtualenv created by install_python_deps.sh . You do not need to manually activate the virtualenv. In Docker (preferred) \u00b6 ./build/docker_build.sh Internally, this uses the build scripts mentioned above, but provides better isolation between builds. Also, compared to a naive docker build, this command preserves the bazel cache. This ensures that subsequent builds run as quickly as possible. Debugging/interactively building \u00b6 In order to debug and/or experiment, it may be useful to build interactively within Docker: # Will launch bash in a docker container containing all Neuropod dependencies ./build/docker_build.sh -i # Run these commands inside the container in order to build and test ./build/build.sh ./build/test.sh Tests \u00b6 Neuropod has a set of tests implemented in C++ and a set of tests implemented in Python. Test coverage is described below: Location Covers C++ Library Covers Python Library C++ Tests source/neuropod/tests/* x Python Tests source/python/neuropod/tests/test_* x x GPU-only Python Tests source/python/neuropod/tests/gpu_test_* x x The Python tests run against both the Python and C++ libraries by using python bindings. This means that many tests only need to be written in Python. C++ tests can have the following tags: gpu : Only run this test when running GPU tests requires_ld_library_path : Set the LD_LIBRARY_PATH and PATH environment variables so the backends and multiprocess worker are available. This is useful for tests that run a model using OPE. no_trace_logging : Don't set the log level to TRACE when running this test. This is useful to avoid lots of output when running benchmarks. CI \u00b6 Build Matrix \u00b6 Our build matrix is defined as all combinations of the following: Platform: Ubuntu 16.04 GPU (in Docker) - Buildkite Ubuntu 16.04 CPU (in Docker) - Buildkite Mac CPU (native) - GitHub Actions Framework versions (each row of the table): CUDA TF Torch Python 9.0 1.12.0 1.1.0 2.7 10.0 1.13.1 1.2.0 3.5 10.0 1.14.0 1.3.0 3.6 10.0 1.15.0 1.4.0 3.7 10.1 2.2.0 1.5.0 3.8 10.1 - 1.6.0 3.8 10.1 - 1.7.0 3.8 11.2.1 2.5.0 - 3.8 11.2.1 2.6.2 - 3.8 We also have the following ad-hoc builds: A lint + docs + static analysis build (Buildkite) A native Ubuntu 16.04 build (outside of docker) to make sure that we don't accidentally break workflows of users not using docker This is a total of 17 builds (3 * 5 + 2) running in CI The current build matrix is defined in build/ci_matrix.py Code coverage is run on all Buildkite Linux builds Lint and Static Analysis \u00b6 We run the following lint tools in CI: clang-format for C++ formatting Buildifier for Bazel BUILD and .bzl file formatting Black and Flake8 for Python lint To show all lint errors and warnings locally, you can run ./tools/lint.sh . To attempt to automatically fix any issues that can be automatically fixed, run ./tools/autofix.sh . We also run the following static analysis tools in CI: Infer for C++ static analysis clang-tidy for C++ lint (Not yet enabled. See here ) These tools tend to be fairly slow so we don't currently provide a way to run them locally. Future additions \u00b6 We're also planning on adding the following configurations to the build matrix: Configs: ASAN Contributing \u00b6 See the contributing guide here","title":"Developing Neuropod"},{"location":"developing/#building-neuropod","text":"Neuropod uses Bazel as a build system. There are a few ways to build the project: Natively on Linux and Mac In Docker (preferred)","title":"Building Neuropod"},{"location":"developing/#natively-on-linux-and-mac","text":"The following sets up a local environment for building and testing: # Install system dependencies (e.g. bazel) ./build/install_system_deps.sh # Install python dependencies (e.g. numpy) # Note: This creates a virtualenv for neuropod and installs all deps in it ./build/install_python_deps.sh After the above steps, you can run the following scripts: # Build ./build/build.sh # Run tests ./build/test.sh # Or ./build/test_gpu.sh to run all tests Note The above commands run all python code in the virtualenv created by install_python_deps.sh . You do not need to manually activate the virtualenv.","title":"Natively on Linux and Mac"},{"location":"developing/#in-docker-preferred","text":"./build/docker_build.sh Internally, this uses the build scripts mentioned above, but provides better isolation between builds. Also, compared to a naive docker build, this command preserves the bazel cache. This ensures that subsequent builds run as quickly as possible.","title":"In Docker (preferred)"},{"location":"developing/#debugginginteractively-building","text":"In order to debug and/or experiment, it may be useful to build interactively within Docker: # Will launch bash in a docker container containing all Neuropod dependencies ./build/docker_build.sh -i # Run these commands inside the container in order to build and test ./build/build.sh ./build/test.sh","title":"Debugging/interactively building"},{"location":"developing/#tests","text":"Neuropod has a set of tests implemented in C++ and a set of tests implemented in Python. Test coverage is described below: Location Covers C++ Library Covers Python Library C++ Tests source/neuropod/tests/* x Python Tests source/python/neuropod/tests/test_* x x GPU-only Python Tests source/python/neuropod/tests/gpu_test_* x x The Python tests run against both the Python and C++ libraries by using python bindings. This means that many tests only need to be written in Python. C++ tests can have the following tags: gpu : Only run this test when running GPU tests requires_ld_library_path : Set the LD_LIBRARY_PATH and PATH environment variables so the backends and multiprocess worker are available. This is useful for tests that run a model using OPE. no_trace_logging : Don't set the log level to TRACE when running this test. This is useful to avoid lots of output when running benchmarks.","title":"Tests"},{"location":"developing/#ci","text":"","title":"CI"},{"location":"developing/#build-matrix","text":"Our build matrix is defined as all combinations of the following: Platform: Ubuntu 16.04 GPU (in Docker) - Buildkite Ubuntu 16.04 CPU (in Docker) - Buildkite Mac CPU (native) - GitHub Actions Framework versions (each row of the table): CUDA TF Torch Python 9.0 1.12.0 1.1.0 2.7 10.0 1.13.1 1.2.0 3.5 10.0 1.14.0 1.3.0 3.6 10.0 1.15.0 1.4.0 3.7 10.1 2.2.0 1.5.0 3.8 10.1 - 1.6.0 3.8 10.1 - 1.7.0 3.8 11.2.1 2.5.0 - 3.8 11.2.1 2.6.2 - 3.8 We also have the following ad-hoc builds: A lint + docs + static analysis build (Buildkite) A native Ubuntu 16.04 build (outside of docker) to make sure that we don't accidentally break workflows of users not using docker This is a total of 17 builds (3 * 5 + 2) running in CI The current build matrix is defined in build/ci_matrix.py Code coverage is run on all Buildkite Linux builds","title":"Build Matrix"},{"location":"developing/#lint-and-static-analysis","text":"We run the following lint tools in CI: clang-format for C++ formatting Buildifier for Bazel BUILD and .bzl file formatting Black and Flake8 for Python lint To show all lint errors and warnings locally, you can run ./tools/lint.sh . To attempt to automatically fix any issues that can be automatically fixed, run ./tools/autofix.sh . We also run the following static analysis tools in CI: Infer for C++ static analysis clang-tidy for C++ lint (Not yet enabled. See here ) These tools tend to be fairly slow so we don't currently provide a way to run them locally.","title":"Lint and Static Analysis"},{"location":"developing/#future-additions","text":"We're also planning on adding the following configurations to the build matrix: Configs: ASAN","title":"Future additions"},{"location":"developing/#contributing","text":"See the contributing guide here","title":"Contributing"},{"location":"installing/","text":"Installing Neuropod \u00b6 Note Neuropod requires macOS or Linux. Python \u00b6 Neuropod can be installed using pip: pip install neuropod To run models, you must also install packages for \"backends\". These are fully self-contained packages that let Neuropod run models with specific versions of frameworks regardless of the version installed in your python environment. See the Backends section below for instructions on installing backends. C++ \u00b6 Prebuilts can be downloaded from the releases page. The libneuropod-[os]-[version].tar.gz files contain header files and prebuilt binaries for the main Neuropod library. To run models, you must also install packages for \"backends\". These are fully self-contained packages that let Neuropod run models with specific versions of frameworks regardless of the version installed on your system. See the Backends section below for instructions on installing backends. Backends \u00b6 The following commands can be used to install the official backends. Backends implement support for a particular framework within Neuropod (e.g. Torch 1.7.0 on GPU, TensorFlow 1.15.0 on CPU, etc) Once a backend is installed, Neuropod can use it from any supported language. # Create a folder to store backends. # The location of the folder Neuropod expects backends to be installed into defaults to \"/usr/local/lib/neuropod\", # but can be overridden by setting the NEUROPOD_BASE_DIR environment variable at runtime NEUROPOD_BASE_DIR = \"/usr/local/lib/neuropod\" sudo mkdir -p \" $NEUROPOD_BASE_DIR \" # Find URLs of backends you want to install from the releases page (https://github.com/uber/neuropod/releases) and install them # by untarring them in your NEUROPOD_BASE_DIR directory. # For example, to install a GPU enabled Torch 1.7 backend for CUDA 10.1, run curl -L https://github.com/uber/neuropod/releases/download/v0.3.0-rc7/libneuropod-gpu-cuda-10.1-linux-v0.3.0-rc7-torchscript-1.7.0-backend.tar.gz | sudo tar -xz -C \" $NEUROPOD_BASE_DIR \" Multiple backends can be installed for a given framework and Neuropod will select the correct one when loading a model. An error will be thrown if none of the installed backends match the model's requirements. See the basic introduction for more information on getting started.","title":"Installing"},{"location":"installing/#installing-neuropod","text":"Note Neuropod requires macOS or Linux.","title":"Installing Neuropod"},{"location":"installing/#python","text":"Neuropod can be installed using pip: pip install neuropod To run models, you must also install packages for \"backends\". These are fully self-contained packages that let Neuropod run models with specific versions of frameworks regardless of the version installed in your python environment. See the Backends section below for instructions on installing backends.","title":"Python"},{"location":"installing/#c","text":"Prebuilts can be downloaded from the releases page. The libneuropod-[os]-[version].tar.gz files contain header files and prebuilt binaries for the main Neuropod library. To run models, you must also install packages for \"backends\". These are fully self-contained packages that let Neuropod run models with specific versions of frameworks regardless of the version installed on your system. See the Backends section below for instructions on installing backends.","title":"C++"},{"location":"installing/#backends","text":"The following commands can be used to install the official backends. Backends implement support for a particular framework within Neuropod (e.g. Torch 1.7.0 on GPU, TensorFlow 1.15.0 on CPU, etc) Once a backend is installed, Neuropod can use it from any supported language. # Create a folder to store backends. # The location of the folder Neuropod expects backends to be installed into defaults to \"/usr/local/lib/neuropod\", # but can be overridden by setting the NEUROPOD_BASE_DIR environment variable at runtime NEUROPOD_BASE_DIR = \"/usr/local/lib/neuropod\" sudo mkdir -p \" $NEUROPOD_BASE_DIR \" # Find URLs of backends you want to install from the releases page (https://github.com/uber/neuropod/releases) and install them # by untarring them in your NEUROPOD_BASE_DIR directory. # For example, to install a GPU enabled Torch 1.7 backend for CUDA 10.1, run curl -L https://github.com/uber/neuropod/releases/download/v0.3.0-rc7/libneuropod-gpu-cuda-10.1-linux-v0.3.0-rc7-torchscript-1.7.0-backend.tar.gz | sudo tar -xz -C \" $NEUROPOD_BASE_DIR \" Multiple backends can be installed for a given framework and Neuropod will select the correct one when loading a model. An error will be thrown if none of the installed backends match the model's requirements. See the basic introduction for more information on getting started.","title":"Backends"},{"location":"pyguide/","text":"Python Guide \u00b6 This guide walks through loading a Neuropod and running inference from Python Tip The Neuropod runtime interface is identical for all frameworks so this guide applies for models from all supported frameworks (including TensorFlow, PyTorch, Keras, and TorchScript) Packaging a Neuropod \u00b6 See the basic introduction guide for examples of how to create Neuropod models in all the supported frameworks. Loading a Neuropod \u00b6 from neuropod.loader import load_neuropod neuropod = load_neuropod ( PATH_TO_MY_MODEL ) You can also use load_neuropod as a context manager: from neuropod.loader import load_neuropod with load_neuropod ( PATH_TO_MY_MODEL ) as neuropod : # Do something here pass Options \u00b6 You can also provide runtime options when loading a model. To select what device to run the model on, you can supply a visible_gpu argument. This is the index of the GPU that this Neuropod should run on (if any). It can either be None or a nonnegative integer. Setting this to None will attempt to run this model on CPU. # Run on CPU neuropod = load_neuropod ( PATH_TO_MY_MODEL , visible_gpu = None ) # Run on the second GPU neuropod = load_neuropod ( PATH_TO_MY_MODEL , visible_gpu = 1 ) Get the inputs and outputs of a model \u00b6 The inputs and outputs of a model are available via the inputs and outputs property. with load_neuropod ( PATH_TO_MY_MODEL ) as neuropod : # This is a list of dicts containing the \"name\", \"dtype\", and \"shape\" # of the input print ( neuropod . inputs , neuropod . outputs ) Inference \u00b6 The infer method of a model is used to run inference. The input to this method is a dict mapping input names to values. This must match the input spec in the neuropod config for the loaded model. Note All the keys in this dict must be strings and all the values must be numpy arrays The output of infer is a dict mapping output names to values. This is checked to ensure that it matches the spec in the neuropod config for the loaded model. All the keys in this dict are strings and all the values are numpy arrays. x = np . array ([ 1 , 2 , 3 , 4 ]) y = np . array ([ 5 , 6 , 7 , 8 ]) with load_neuropod ( ADDITION_MODEL_PATH ) as neuropod : results = neuropod . infer ({ \"x\" : x , \"y\" : y }) # array([6, 8, 10, 12]) print results [ \"out\" ] Serialization \u00b6 import numpy as np from neuropod import neuropod_native # An array to serialize tensor = np . arange ( 5 ) # Convert a numpy array to a NeuropodTensor and serialize it serialized_bytes = neuropod_native . serialize ( tensor ) # Deserialize a string of bytes to a NeuropodTensor # (and return it as a numpy array) deserialized = neuropod_native . deserialize ( serialized_bytes ) # array([0, 1, 2, 3, 4]) print ( deserialized ) Under the hood, the serialization code converts between numpy arrays and C++ NeuropodTensor objects (in a zero-copy way). It then uses the C++ serialization functionality to serialize/deserialize. Note Serialization and deserialization works across Python and C++. This means you can serialize tensors in C++ and deserialize in Python or vice-versa Warning The goal for this API is to support transient serialization. There are no guarantees about backwards compatibility so this API should not be used for long term storage of data","title":"Python Guide"},{"location":"pyguide/#python-guide","text":"This guide walks through loading a Neuropod and running inference from Python Tip The Neuropod runtime interface is identical for all frameworks so this guide applies for models from all supported frameworks (including TensorFlow, PyTorch, Keras, and TorchScript)","title":"Python Guide"},{"location":"pyguide/#packaging-a-neuropod","text":"See the basic introduction guide for examples of how to create Neuropod models in all the supported frameworks.","title":"Packaging a Neuropod"},{"location":"pyguide/#loading-a-neuropod","text":"from neuropod.loader import load_neuropod neuropod = load_neuropod ( PATH_TO_MY_MODEL ) You can also use load_neuropod as a context manager: from neuropod.loader import load_neuropod with load_neuropod ( PATH_TO_MY_MODEL ) as neuropod : # Do something here pass","title":"Loading a Neuropod"},{"location":"pyguide/#options","text":"You can also provide runtime options when loading a model. To select what device to run the model on, you can supply a visible_gpu argument. This is the index of the GPU that this Neuropod should run on (if any). It can either be None or a nonnegative integer. Setting this to None will attempt to run this model on CPU. # Run on CPU neuropod = load_neuropod ( PATH_TO_MY_MODEL , visible_gpu = None ) # Run on the second GPU neuropod = load_neuropod ( PATH_TO_MY_MODEL , visible_gpu = 1 )","title":"Options"},{"location":"pyguide/#get-the-inputs-and-outputs-of-a-model","text":"The inputs and outputs of a model are available via the inputs and outputs property. with load_neuropod ( PATH_TO_MY_MODEL ) as neuropod : # This is a list of dicts containing the \"name\", \"dtype\", and \"shape\" # of the input print ( neuropod . inputs , neuropod . outputs )","title":"Get the inputs and outputs of a model"},{"location":"pyguide/#inference","text":"The infer method of a model is used to run inference. The input to this method is a dict mapping input names to values. This must match the input spec in the neuropod config for the loaded model. Note All the keys in this dict must be strings and all the values must be numpy arrays The output of infer is a dict mapping output names to values. This is checked to ensure that it matches the spec in the neuropod config for the loaded model. All the keys in this dict are strings and all the values are numpy arrays. x = np . array ([ 1 , 2 , 3 , 4 ]) y = np . array ([ 5 , 6 , 7 , 8 ]) with load_neuropod ( ADDITION_MODEL_PATH ) as neuropod : results = neuropod . infer ({ \"x\" : x , \"y\" : y }) # array([6, 8, 10, 12]) print results [ \"out\" ]","title":"Inference"},{"location":"pyguide/#serialization","text":"import numpy as np from neuropod import neuropod_native # An array to serialize tensor = np . arange ( 5 ) # Convert a numpy array to a NeuropodTensor and serialize it serialized_bytes = neuropod_native . serialize ( tensor ) # Deserialize a string of bytes to a NeuropodTensor # (and return it as a numpy array) deserialized = neuropod_native . deserialize ( serialized_bytes ) # array([0, 1, 2, 3, 4]) print ( deserialized ) Under the hood, the serialization code converts between numpy arrays and C++ NeuropodTensor objects (in a zero-copy way). It then uses the C++ serialization functionality to serialize/deserialize. Note Serialization and deserialization works across Python and C++. This means you can serialize tensors in C++ and deserialize in Python or vice-versa Warning The goal for this API is to support transient serialization. There are no guarantees about backwards compatibility so this API should not be used for long term storage of data","title":"Serialization"},{"location":"tutorial/","text":"Neuropod Tutorial \u00b6 In this tutorial, we\u2019re going to build a simple Neuropod model for addition in TensorFlow, PyTorch, and TorchScript. We'll also show how to run inference from Python and C++. Almost all of the examples/code in this tutorial come from the Neuropod unit and integration tests. Please read through them for complete working examples. The Neuropod packaging and inference interfaces also have comprehensive docstrings and provide a more detailed usage of the API than this tutorial. Make sure to follow the instructions for installing Neuropod before continuing Package a Model \u00b6 The first step for packaging a model is to define a \u201cproblem\u201d (e.g. 2d object detection). A \u201cproblem\u201d is composed of 4 things: an input_spec A list of dicts specifying the name, datatype, and shape of an input tensor an output_spec A list of dicts specifying the name, datatype, and shape of an output tensor test_input_data (optional) If provided, Neuropod will run inference immediately after packaging to verify that the model was packaged correctly. Must be provided if test_output_data is provided test_output_data (optional) If provided, Neuropod will test that the output of inference with test_input_data matches test_output_data The shape of a tensor can include None in which case any value is acceptable. You can also use \u201csymbols\u201d in these shape definitions. Every instance of that symbol must resolve to the same value at runtime. For example, here\u2019s a problem definition for our addition model: INPUT_SPEC = [ # A one dimensional tensor of any size with dtype float32 { \"name\" : \"x\" , \"dtype\" : \"float32\" , \"shape\" : ( \"num_inputs\" ,)}, # A one dimensional tensor of the same size with dtype float32 { \"name\" : \"y\" , \"dtype\" : \"float32\" , \"shape\" : ( \"num_inputs\" ,)}, ] OUTPUT_SPEC = [ # The sum of the two tensors { \"name\" : \"out\" , \"dtype\" : \"float32\" , \"shape\" : ( None ,)}, ] TEST_INPUT_DATA = { \"x\" : np . arange ( 5 , dtype = np . float32 ), \"y\" : np . arange ( 5 , dtype = np . float32 ), } TEST_EXPECTED_OUT = { \"out\" : np . arange ( 5 ) + np . arange ( 5 ) } The symbol num_inputs in the shapes of x and y must resolve to the same value at runtime. For a definition of a \u201creal\u201d problem, see the example problem definitions section in the appendix. Now that we have a problem defined, we\u2019re going to see how to package a model in each of the currently supported DL frameworks. TensorFlow \u00b6 There are two ways to package a TensorFlow model. One is with a GraphDef the other is with a path to a frozen graph. Both of these require a node_name_mapping that maps a tensor name in the problem definition (see above) to a node in the TensorFlow graph. See the examples below for more detail. GraphDef \u00b6 If we have a function that returns a GraphDef like: import tensorflow as tf def create_tf_addition_model (): \"\"\" A simple addition model \"\"\" g = tf . Graph () with g . as_default (): with tf . name_scope ( \"some_namespace\" ): x = tf . placeholder ( tf . float32 , name = \"in_x\" ) y = tf . placeholder ( tf . float32 , name = \"in_y\" ) out = tf . add ( x , y , name = \"out\" ) return g . as_graph_def () we can package the model as follows: from neuropod.packagers import create_tensorflow_neuropod create_tensorflow_neuropod ( neuropod_path = neuropod_path , model_name = \"addition_model\" , graph_def = create_tf_addition_model (), node_name_mapping = { \"x\" : \"some_namespace/in_x:0\" , \"y\" : \"some_namespace/in_y:0\" , \"out\" : \"some_namespace/out:0\" , }, input_spec = addition_problem_definition . INPUT_SPEC , output_spec = addition_problem_definition . OUTPUT_SPEC , test_input_data = addition_problem_definition . TEST_INPUT_DATA , test_expected_out = addition_problem_definition . TEST_EXPECTED_OUT , ) Note create_tensorflow_neuropod runs inference with the test data immediately after creating the neuropod. Raises a ValueError if the model output does not match the expected output. Path to a Frozen Graph \u00b6 If you already have a frozen graph, you can package the model like this: from neuropod.packagers import create_tensorflow_neuropod create_tensorflow_neuropod ( neuropod_path = neuropod_path , model_name = \"addition_model\" , frozen_graph_path = \"/path/to/my/frozen.graph\" , node_name_mapping = { \"x\" : \"some_namespace/in_x:0\" , \"y\" : \"some_namespace/in_y:0\" , \"out\" : \"some_namespace/out:0\" , }, input_spec = addition_problem_definition . INPUT_SPEC , output_spec = addition_problem_definition . OUTPUT_SPEC , test_input_data = addition_problem_definition . TEST_INPUT_DATA , test_expected_out = addition_problem_definition . TEST_EXPECTED_OUT , ) Note create_tensorflow_neuropod runs inference with the test data immediately after creating the neuropod. Raises a ValueError if the model output does not match the expected output. PyTorch \u00b6 Tip Packaging a PyTorch model is a bit more complicated because you need python code and the weights in order to run the network. Converting your model to TorchScript is recommended if possible. In order to create a PyTorch neuropod package, we need to follow a few guidelines: Absolute imports (e.g. import torch ) are okay as long as your runtime environment has the package installed For Python 3, all other imports within your package must be relative This type of neuropod package is a bit less flexible than TensorFlow/TorchScript/Keras packages because absolute imports introduce a dependency on the runtime environment. This will be improved in a future release. Let's say our addition model looks like this (and is stored at /my/model/code/dir/main.py ): import torch import torch.nn as nn class AdditionModel ( nn . Module ): def forward ( self , x , y ): return { \"out\" : x + y } def get_model ( data_root ): return AdditionModel () In order to package it, we need 4 things: The paths to any data we want to store (e.g. the model weights) The path to the python_root of the code along with relative paths for any dirs within the python_root we want to package An entrypoint function that returns a model given a path to the packaged data. See the docstring for create_pytorch_neuropod for more details and examples. The dependencies of our model. These should be python packages. Tip See the API docs for create_pytorch_neuropod for detailed descriptions of every parameter For our model: We don't need to store any data (because our model has no weights) Our python root is /my/model/code/dir and we want to store all the code in it Our entrypoint function is get_model and our entrypoint_package is main (because the code is in main.py in the python root) This translates to the following: from neuropod.packagers import create_pytorch_neuropod create_pytorch_neuropod ( neuropod_path = neuropod_path , model_name = \"addition_model\" , data_paths = [], code_path_spec = [{ \"python_root\" : '/my/model/code/dir' , \"dirs_to_package\" : [ \"\" # Package everything in the python_root ], }], entrypoint_package = \"main\" , entrypoint = \"get_model\" , input_spec = addition_problem_definition . INPUT_SPEC , output_spec = addition_problem_definition . OUTPUT_SPEC , test_input_data = addition_problem_definition . TEST_INPUT_DATA , test_expected_out = addition_problem_definition . TEST_EXPECTED_OUT , ) Note create_pytorch_neuropod runs inference with the test data immediately after creating the neuropod. Raises a ValueError if the model output does not match the expected output. TorchScript \u00b6 TorchScript is much easier to package than PyTorch (since we don't need to store any python code). If we have an addition model that looks like: import torch class AdditionModel ( torch . jit . ScriptModule ): \"\"\" A simple addition model \"\"\" @torch . jit . script_method def forward ( self , x , y ): return { \"out\" : x + y } We can package it by running: from neuropod.packagers import create_torchscript_neuropod create_torchscript_neuropod ( neuropod_path = neuropod_path , model_name = \"addition_model\" , module = AdditionModel (), input_spec = addition_problem_definition . INPUT_SPEC , output_spec = addition_problem_definition . OUTPUT_SPEC , test_input_data = addition_problem_definition . TEST_INPUT_DATA , test_expected_out = addition_problem_definition . TEST_EXPECTED_OUT , ) Note create_torchscript_neuropod runs inference with the test data immediately after creating the neuropod. Raises a ValueError if the model output does not match the expected output. Keras \u00b6 If we have a Keras addition model that looks like: def create_keras_addition_model (): \"\"\" A simple addition model \"\"\" x = Input ( batch_shape = ( None ,), name = \"x\" ) y = Input ( batch_shape = ( None ,), name = \"y\" ) out = Add ( name = \"out\" )([ x , y ]) model = Model ( inputs = [ x , y ], outputs = [ out ]) return model We can package it by running: from neuropod.packagers import create_keras_neuropod create_keras_neuropod ( neuropod_path = neuropod_path , model_name = \"addition_model\" , sess = tf . keras . backend . get_session (), model = create_keras_addition_model (), input_spec = addition_problem_definition . INPUT_SPEC , output_spec = addition_problem_definition . OUTPUT_SPEC , test_input_data = addition_problem_definition . TEST_INPUT_DATA , test_expected_out = addition_problem_definition . TEST_EXPECTED_OUT , ) Note create_keras_neuropod runs inference with the test data immediately after creating the neuropod. Raises a ValueError if the model output does not match the expected output. Ludwig \u00b6 Ludwig contains a utility to export a model as a neuropod. If we train a Ludwig model like this: ludwig train --data_csv <MY_DATA>PATH> --model_definition <MY_MODEL_DEFINITION> --output_directory <LUDWIG_OUTPUT_DIRECTORY> We can package it as a neuropod by running: python -m ludwig.utils.neuropod_utils --ludwig_model_path <LUDWIG_OUTPUT_DIRECTORY>/<MODEL_NAME>/model --neuropod_path <NEUROPOD_OUTPUT_PATH> By default the package will be a ZIP file, but you can specify --package_as_dir if you prefer the output Neuropod package to be a directory. Also note that the Python version should be 3.7+. Note Currently, running a Ludwig model requires having all of Ludwig's dependencies installed in your runtime environment (including additional dependencies for the datatypes used by the model). This will be improved in a future release. For more details check the Ludwig User Guide . Python \u00b6 Packaging aribtrary Python code has the same interface as packaging PyTorch above. For an example, see the PyTorch section above and use create_python_neuropod instead of create_pytorch_neuropod Run Inference \u00b6 Inference is the exact same no matter what the underlying DL framework is From Python \u00b6 x = np . array ([ 1 , 2 , 3 , 4 ]) y = np . array ([ 5 , 6 , 7 , 8 ]) with load_neuropod ( ADDITION_MODEL_PATH ) as neuropod : results = neuropod . infer ({ \"x\" : x , \"y\" : y }) # array([6, 8, 10, 12]) print results [ \"out\" ] From C++ \u00b6 #include \"neuropod/neuropod.hh\" int main () { const std :: vector < int64_t > shape = { 4 }; // To show two different ways of adding data, one of our inputs is an array // and the other is a vector. const float [] x_data = { 1 , 2 , 3 , 4 }; const std :: vector < float > y_data = { 5 , 6 , 7 , 8 }; // Load the neuropod Neuropod neuropod ( ADDITION_MODEL_PATH ); // Add the input data using two different signatures of `copy_from` // (one with a pointer and size, one with a vector) auto x_tensor = neuropod . allocate_tensor < float > ( shape ); x_tensor -> copy_from ( x_data , 4 ); auto y_tensor = neuropod . allocate_tensor < float > ( shape ); y_tensor -> copy_from ( y_data ); // Run inference const auto output_data = neuropod . infer ({ { \"x\" , x_tensor }, { \"y\" , y_tensor } }); const auto out_tensor = output_data -> at ( \"out\" ); // {6, 8, 10, 12} const auto out_vector = out_tensor -> as_typed_tensor < float > () -> get_data_as_vector (); // {4} const auto out_shape = out_tensor -> get_dims (); } Note This shows basic usage of the C++ API. For more flexible and memory-efficient usage, please see the C++ API docs Appendix \u00b6 Example Problem Definitions \u00b6 The problem definition for 2d object detection may look something like this: INPUT_SPEC = [ # BGR image { \"name\" : \"image\" , \"dtype\" : \"uint8\" , \"shape\" : ( 1200 , 1920 , 3 )}, ] OUTPUT_SPEC = [ # shape: (num_detections, 4): (xmin, ymin, xmax, ymax) # These values are in units of pixels. The origin is the top left corner # with positive X to the right and positive Y towards the bottom of the image { \"name\" : \"boxes\" , \"dtype\" : \"float32\" , \"shape\" : ( \"num_detections\" , 4 )}, # The list of classes that the network can output # This must be some subset of ['vehicle', 'person', 'motorcycle', 'bicycle'] { \"name\" : \"supported_object_classes\" , \"dtype\" : \"string\" , \"shape\" : ( \"num_classes\" ,)}, # The probability of each class for each detection # These should all be floats between 0 and 1 { \"name\" : \"object_class_probability\" , \"dtype\" : \"float32\" , \"shape\" : ( \"num_detections\" , \"num_classes\" )}, ]","title":"Basic Introduction"},{"location":"tutorial/#neuropod-tutorial","text":"In this tutorial, we\u2019re going to build a simple Neuropod model for addition in TensorFlow, PyTorch, and TorchScript. We'll also show how to run inference from Python and C++. Almost all of the examples/code in this tutorial come from the Neuropod unit and integration tests. Please read through them for complete working examples. The Neuropod packaging and inference interfaces also have comprehensive docstrings and provide a more detailed usage of the API than this tutorial. Make sure to follow the instructions for installing Neuropod before continuing","title":"Neuropod Tutorial"},{"location":"tutorial/#package-a-model","text":"The first step for packaging a model is to define a \u201cproblem\u201d (e.g. 2d object detection). A \u201cproblem\u201d is composed of 4 things: an input_spec A list of dicts specifying the name, datatype, and shape of an input tensor an output_spec A list of dicts specifying the name, datatype, and shape of an output tensor test_input_data (optional) If provided, Neuropod will run inference immediately after packaging to verify that the model was packaged correctly. Must be provided if test_output_data is provided test_output_data (optional) If provided, Neuropod will test that the output of inference with test_input_data matches test_output_data The shape of a tensor can include None in which case any value is acceptable. You can also use \u201csymbols\u201d in these shape definitions. Every instance of that symbol must resolve to the same value at runtime. For example, here\u2019s a problem definition for our addition model: INPUT_SPEC = [ # A one dimensional tensor of any size with dtype float32 { \"name\" : \"x\" , \"dtype\" : \"float32\" , \"shape\" : ( \"num_inputs\" ,)}, # A one dimensional tensor of the same size with dtype float32 { \"name\" : \"y\" , \"dtype\" : \"float32\" , \"shape\" : ( \"num_inputs\" ,)}, ] OUTPUT_SPEC = [ # The sum of the two tensors { \"name\" : \"out\" , \"dtype\" : \"float32\" , \"shape\" : ( None ,)}, ] TEST_INPUT_DATA = { \"x\" : np . arange ( 5 , dtype = np . float32 ), \"y\" : np . arange ( 5 , dtype = np . float32 ), } TEST_EXPECTED_OUT = { \"out\" : np . arange ( 5 ) + np . arange ( 5 ) } The symbol num_inputs in the shapes of x and y must resolve to the same value at runtime. For a definition of a \u201creal\u201d problem, see the example problem definitions section in the appendix. Now that we have a problem defined, we\u2019re going to see how to package a model in each of the currently supported DL frameworks.","title":"Package a Model"},{"location":"tutorial/#tensorflow","text":"There are two ways to package a TensorFlow model. One is with a GraphDef the other is with a path to a frozen graph. Both of these require a node_name_mapping that maps a tensor name in the problem definition (see above) to a node in the TensorFlow graph. See the examples below for more detail.","title":"TensorFlow"},{"location":"tutorial/#graphdef","text":"If we have a function that returns a GraphDef like: import tensorflow as tf def create_tf_addition_model (): \"\"\" A simple addition model \"\"\" g = tf . Graph () with g . as_default (): with tf . name_scope ( \"some_namespace\" ): x = tf . placeholder ( tf . float32 , name = \"in_x\" ) y = tf . placeholder ( tf . float32 , name = \"in_y\" ) out = tf . add ( x , y , name = \"out\" ) return g . as_graph_def () we can package the model as follows: from neuropod.packagers import create_tensorflow_neuropod create_tensorflow_neuropod ( neuropod_path = neuropod_path , model_name = \"addition_model\" , graph_def = create_tf_addition_model (), node_name_mapping = { \"x\" : \"some_namespace/in_x:0\" , \"y\" : \"some_namespace/in_y:0\" , \"out\" : \"some_namespace/out:0\" , }, input_spec = addition_problem_definition . INPUT_SPEC , output_spec = addition_problem_definition . OUTPUT_SPEC , test_input_data = addition_problem_definition . TEST_INPUT_DATA , test_expected_out = addition_problem_definition . TEST_EXPECTED_OUT , ) Note create_tensorflow_neuropod runs inference with the test data immediately after creating the neuropod. Raises a ValueError if the model output does not match the expected output.","title":"GraphDef"},{"location":"tutorial/#path-to-a-frozen-graph","text":"If you already have a frozen graph, you can package the model like this: from neuropod.packagers import create_tensorflow_neuropod create_tensorflow_neuropod ( neuropod_path = neuropod_path , model_name = \"addition_model\" , frozen_graph_path = \"/path/to/my/frozen.graph\" , node_name_mapping = { \"x\" : \"some_namespace/in_x:0\" , \"y\" : \"some_namespace/in_y:0\" , \"out\" : \"some_namespace/out:0\" , }, input_spec = addition_problem_definition . INPUT_SPEC , output_spec = addition_problem_definition . OUTPUT_SPEC , test_input_data = addition_problem_definition . TEST_INPUT_DATA , test_expected_out = addition_problem_definition . TEST_EXPECTED_OUT , ) Note create_tensorflow_neuropod runs inference with the test data immediately after creating the neuropod. Raises a ValueError if the model output does not match the expected output.","title":"Path to a Frozen Graph"},{"location":"tutorial/#pytorch","text":"Tip Packaging a PyTorch model is a bit more complicated because you need python code and the weights in order to run the network. Converting your model to TorchScript is recommended if possible. In order to create a PyTorch neuropod package, we need to follow a few guidelines: Absolute imports (e.g. import torch ) are okay as long as your runtime environment has the package installed For Python 3, all other imports within your package must be relative This type of neuropod package is a bit less flexible than TensorFlow/TorchScript/Keras packages because absolute imports introduce a dependency on the runtime environment. This will be improved in a future release. Let's say our addition model looks like this (and is stored at /my/model/code/dir/main.py ): import torch import torch.nn as nn class AdditionModel ( nn . Module ): def forward ( self , x , y ): return { \"out\" : x + y } def get_model ( data_root ): return AdditionModel () In order to package it, we need 4 things: The paths to any data we want to store (e.g. the model weights) The path to the python_root of the code along with relative paths for any dirs within the python_root we want to package An entrypoint function that returns a model given a path to the packaged data. See the docstring for create_pytorch_neuropod for more details and examples. The dependencies of our model. These should be python packages. Tip See the API docs for create_pytorch_neuropod for detailed descriptions of every parameter For our model: We don't need to store any data (because our model has no weights) Our python root is /my/model/code/dir and we want to store all the code in it Our entrypoint function is get_model and our entrypoint_package is main (because the code is in main.py in the python root) This translates to the following: from neuropod.packagers import create_pytorch_neuropod create_pytorch_neuropod ( neuropod_path = neuropod_path , model_name = \"addition_model\" , data_paths = [], code_path_spec = [{ \"python_root\" : '/my/model/code/dir' , \"dirs_to_package\" : [ \"\" # Package everything in the python_root ], }], entrypoint_package = \"main\" , entrypoint = \"get_model\" , input_spec = addition_problem_definition . INPUT_SPEC , output_spec = addition_problem_definition . OUTPUT_SPEC , test_input_data = addition_problem_definition . TEST_INPUT_DATA , test_expected_out = addition_problem_definition . TEST_EXPECTED_OUT , ) Note create_pytorch_neuropod runs inference with the test data immediately after creating the neuropod. Raises a ValueError if the model output does not match the expected output.","title":"PyTorch"},{"location":"tutorial/#torchscript","text":"TorchScript is much easier to package than PyTorch (since we don't need to store any python code). If we have an addition model that looks like: import torch class AdditionModel ( torch . jit . ScriptModule ): \"\"\" A simple addition model \"\"\" @torch . jit . script_method def forward ( self , x , y ): return { \"out\" : x + y } We can package it by running: from neuropod.packagers import create_torchscript_neuropod create_torchscript_neuropod ( neuropod_path = neuropod_path , model_name = \"addition_model\" , module = AdditionModel (), input_spec = addition_problem_definition . INPUT_SPEC , output_spec = addition_problem_definition . OUTPUT_SPEC , test_input_data = addition_problem_definition . TEST_INPUT_DATA , test_expected_out = addition_problem_definition . TEST_EXPECTED_OUT , ) Note create_torchscript_neuropod runs inference with the test data immediately after creating the neuropod. Raises a ValueError if the model output does not match the expected output.","title":"TorchScript"},{"location":"tutorial/#keras","text":"If we have a Keras addition model that looks like: def create_keras_addition_model (): \"\"\" A simple addition model \"\"\" x = Input ( batch_shape = ( None ,), name = \"x\" ) y = Input ( batch_shape = ( None ,), name = \"y\" ) out = Add ( name = \"out\" )([ x , y ]) model = Model ( inputs = [ x , y ], outputs = [ out ]) return model We can package it by running: from neuropod.packagers import create_keras_neuropod create_keras_neuropod ( neuropod_path = neuropod_path , model_name = \"addition_model\" , sess = tf . keras . backend . get_session (), model = create_keras_addition_model (), input_spec = addition_problem_definition . INPUT_SPEC , output_spec = addition_problem_definition . OUTPUT_SPEC , test_input_data = addition_problem_definition . TEST_INPUT_DATA , test_expected_out = addition_problem_definition . TEST_EXPECTED_OUT , ) Note create_keras_neuropod runs inference with the test data immediately after creating the neuropod. Raises a ValueError if the model output does not match the expected output.","title":"Keras"},{"location":"tutorial/#ludwig","text":"Ludwig contains a utility to export a model as a neuropod. If we train a Ludwig model like this: ludwig train --data_csv <MY_DATA>PATH> --model_definition <MY_MODEL_DEFINITION> --output_directory <LUDWIG_OUTPUT_DIRECTORY> We can package it as a neuropod by running: python -m ludwig.utils.neuropod_utils --ludwig_model_path <LUDWIG_OUTPUT_DIRECTORY>/<MODEL_NAME>/model --neuropod_path <NEUROPOD_OUTPUT_PATH> By default the package will be a ZIP file, but you can specify --package_as_dir if you prefer the output Neuropod package to be a directory. Also note that the Python version should be 3.7+. Note Currently, running a Ludwig model requires having all of Ludwig's dependencies installed in your runtime environment (including additional dependencies for the datatypes used by the model). This will be improved in a future release. For more details check the Ludwig User Guide .","title":"Ludwig"},{"location":"tutorial/#python","text":"Packaging aribtrary Python code has the same interface as packaging PyTorch above. For an example, see the PyTorch section above and use create_python_neuropod instead of create_pytorch_neuropod","title":"Python"},{"location":"tutorial/#run-inference","text":"Inference is the exact same no matter what the underlying DL framework is","title":"Run Inference"},{"location":"tutorial/#from-python","text":"x = np . array ([ 1 , 2 , 3 , 4 ]) y = np . array ([ 5 , 6 , 7 , 8 ]) with load_neuropod ( ADDITION_MODEL_PATH ) as neuropod : results = neuropod . infer ({ \"x\" : x , \"y\" : y }) # array([6, 8, 10, 12]) print results [ \"out\" ]","title":"From Python"},{"location":"tutorial/#from-c","text":"#include \"neuropod/neuropod.hh\" int main () { const std :: vector < int64_t > shape = { 4 }; // To show two different ways of adding data, one of our inputs is an array // and the other is a vector. const float [] x_data = { 1 , 2 , 3 , 4 }; const std :: vector < float > y_data = { 5 , 6 , 7 , 8 }; // Load the neuropod Neuropod neuropod ( ADDITION_MODEL_PATH ); // Add the input data using two different signatures of `copy_from` // (one with a pointer and size, one with a vector) auto x_tensor = neuropod . allocate_tensor < float > ( shape ); x_tensor -> copy_from ( x_data , 4 ); auto y_tensor = neuropod . allocate_tensor < float > ( shape ); y_tensor -> copy_from ( y_data ); // Run inference const auto output_data = neuropod . infer ({ { \"x\" , x_tensor }, { \"y\" , y_tensor } }); const auto out_tensor = output_data -> at ( \"out\" ); // {6, 8, 10, 12} const auto out_vector = out_tensor -> as_typed_tensor < float > () -> get_data_as_vector (); // {4} const auto out_shape = out_tensor -> get_dims (); } Note This shows basic usage of the C++ API. For more flexible and memory-efficient usage, please see the C++ API docs","title":"From C++"},{"location":"tutorial/#appendix","text":"","title":"Appendix"},{"location":"tutorial/#example-problem-definitions","text":"The problem definition for 2d object detection may look something like this: INPUT_SPEC = [ # BGR image { \"name\" : \"image\" , \"dtype\" : \"uint8\" , \"shape\" : ( 1200 , 1920 , 3 )}, ] OUTPUT_SPEC = [ # shape: (num_detections, 4): (xmin, ymin, xmax, ymax) # These values are in units of pixels. The origin is the top left corner # with positive X to the right and positive Y towards the bottom of the image { \"name\" : \"boxes\" , \"dtype\" : \"float32\" , \"shape\" : ( \"num_detections\" , 4 )}, # The list of classes that the network can output # This must be some subset of ['vehicle', 'person', 'motorcycle', 'bicycle'] { \"name\" : \"supported_object_classes\" , \"dtype\" : \"string\" , \"shape\" : ( \"num_classes\" ,)}, # The probability of each class for each detection # These should all be floats between 0 and 1 { \"name\" : \"object_class_probability\" , \"dtype\" : \"float32\" , \"shape\" : ( \"num_detections\" , \"num_classes\" )}, ]","title":"Example Problem Definitions"},{"location":"advanced/efficient_tensor_creation/","text":"There are a handful of ways of creating tensors from exisiting data in C++ and they all have different tradeoffs between simplicity and performance. This document goes over some approaches and their tradeoffs. Tip Make sure to read the C++ guide before continuing Writing your data directly into a tensor \u00b6 This is preferable if you can do it. Instead of copying data or wrapping existing data, write your data directly into a tensor. Pros \u00b6 This'll work without copies under the hood for both in-process and out-of-process execution Has no memory alignment requirements No need to work with deleters Cons \u00b6 It can require a lot of refactoring of an existing application in order to make this work well Examples \u00b6 You could receive data directly into a tensor's underlying buffer: // Allocate a tensor auto tensor = allocator -> allocate_tensor < float > ({ 6 , 6 }); // Get a pointer to the underlying buffer auto data = tensor -> get_raw_data_ptr (); // Some function that writes data directly into this buffer recv_message_into_buffer ( data ); Or you could manually fill in a tensor: // Allocate a tensor auto tensor = allocator -> allocate_tensor < float > ({ 256 , 256 }); const auto & dims = tensor -> get_dims (); // Get an accessor auto accessor = tensor -> accessor < 2 > (); // Write data directly into it for ( int i = 0 ; i < dims [ 0 ]; i ++ ) { for ( int j = 0 ; j < dims [ 1 ]; j ++ ) { accessor [ i ][ j ] = i * j ; } } You could even parallelize that with TBB: // Allocate a tensor auto tensor = allocator -> allocate_tensor < float > ({ 256 , 256 }); const auto & dims = tensor -> get_dims (); // Get an accessor auto accessor = tensor -> accessor < 2 > (); // Write data into the tensor in parallel tbb :: parallel_for ( // Parallelize in blocks of 16 by 16 tbb : blocked_range2d < size_t > ( 0 , dims [ 0 ], 16 , 0 , dims [ 1 ], 16 ), // Run this lambda in parallel for each block in the range above [ & ]( const blocked_range2d < size_t >& r ) { for ( size_t i = r . rows (). begin (); i != r . rows (). end (); i ++ ) { for ( size_t j = r . cols (). begin (); j != r . cols (). end (); j ++ ) { accessor [ i ][ j ] = i * j ; } } } ); Wrapping existing memory \u00b6 This works well if you already have your data in a buffer somewhere. Pros \u00b6 This'll work without copies during in-process execution Easy to do if you already have data Cons \u00b6 Need an understanding of what deleters are and how to use them correctly For efficient usage with TF, the data needs to be 64 byte aligned Note: this isn't a hard requirement, but TF may copy unaligned data under the hood Compared to #1, this makes an extra copy during out-of-process execution Examples \u00b6 Wrapping data from a cv::Mat : cv :: Mat image = ... // An image from somewhere auto tensor = allocator -> tensor_from_memory < uint8_t > ( // Dimensions { 1 , image . rows , image . cols , image . channels ()}, // Data image . data , // Deleter [ image ]( void * unused ) { // By capturing `image` in this deleter, we ensure // that the underlying data does not get deallocated // before we're done with the tensor. } ); Copying data into a tensor \u00b6 Pros \u00b6 Very easy to do No memory alignment requirements No need to work with deleters Cons \u00b6 Always makes an extra copy during in-process execution Compared to #1, this makes an extra copy during out-of-process execution (although this copy is explicitly written by the user) Examples \u00b6 Copying from a cv::Mat : cv :: Mat image = ... // An image from somewhere auto tensor = allocator -> allocate_tensor < uint8_t > ( // Dimensions { 1 , image . rows , image . cols , image . channels ()} ); // Copy data into the tensor tensor -> copy_from ( image . data , tensor -> get_num_elements ()); Which one should I use? \u00b6 In general, the order of approaches in terms of performance is the following: Writing data directly into a tensor Wrapping existing memory Copying data into a tensor That said, profiling is your friend. The tradeoff between simplicity and performance is also different for large vs small tensors since copies are cheaper for small tensors.","title":"Efficient Tensor Creation"},{"location":"advanced/efficient_tensor_creation/#writing-your-data-directly-into-a-tensor","text":"This is preferable if you can do it. Instead of copying data or wrapping existing data, write your data directly into a tensor.","title":"Writing your data directly into a tensor"},{"location":"advanced/efficient_tensor_creation/#pros","text":"This'll work without copies under the hood for both in-process and out-of-process execution Has no memory alignment requirements No need to work with deleters","title":"Pros"},{"location":"advanced/efficient_tensor_creation/#cons","text":"It can require a lot of refactoring of an existing application in order to make this work well","title":"Cons"},{"location":"advanced/efficient_tensor_creation/#examples","text":"You could receive data directly into a tensor's underlying buffer: // Allocate a tensor auto tensor = allocator -> allocate_tensor < float > ({ 6 , 6 }); // Get a pointer to the underlying buffer auto data = tensor -> get_raw_data_ptr (); // Some function that writes data directly into this buffer recv_message_into_buffer ( data ); Or you could manually fill in a tensor: // Allocate a tensor auto tensor = allocator -> allocate_tensor < float > ({ 256 , 256 }); const auto & dims = tensor -> get_dims (); // Get an accessor auto accessor = tensor -> accessor < 2 > (); // Write data directly into it for ( int i = 0 ; i < dims [ 0 ]; i ++ ) { for ( int j = 0 ; j < dims [ 1 ]; j ++ ) { accessor [ i ][ j ] = i * j ; } } You could even parallelize that with TBB: // Allocate a tensor auto tensor = allocator -> allocate_tensor < float > ({ 256 , 256 }); const auto & dims = tensor -> get_dims (); // Get an accessor auto accessor = tensor -> accessor < 2 > (); // Write data into the tensor in parallel tbb :: parallel_for ( // Parallelize in blocks of 16 by 16 tbb : blocked_range2d < size_t > ( 0 , dims [ 0 ], 16 , 0 , dims [ 1 ], 16 ), // Run this lambda in parallel for each block in the range above [ & ]( const blocked_range2d < size_t >& r ) { for ( size_t i = r . rows (). begin (); i != r . rows (). end (); i ++ ) { for ( size_t j = r . cols (). begin (); j != r . cols (). end (); j ++ ) { accessor [ i ][ j ] = i * j ; } } } );","title":"Examples"},{"location":"advanced/efficient_tensor_creation/#wrapping-existing-memory","text":"This works well if you already have your data in a buffer somewhere.","title":"Wrapping existing memory"},{"location":"advanced/efficient_tensor_creation/#pros_1","text":"This'll work without copies during in-process execution Easy to do if you already have data","title":"Pros"},{"location":"advanced/efficient_tensor_creation/#cons_1","text":"Need an understanding of what deleters are and how to use them correctly For efficient usage with TF, the data needs to be 64 byte aligned Note: this isn't a hard requirement, but TF may copy unaligned data under the hood Compared to #1, this makes an extra copy during out-of-process execution","title":"Cons"},{"location":"advanced/efficient_tensor_creation/#examples_1","text":"Wrapping data from a cv::Mat : cv :: Mat image = ... // An image from somewhere auto tensor = allocator -> tensor_from_memory < uint8_t > ( // Dimensions { 1 , image . rows , image . cols , image . channels ()}, // Data image . data , // Deleter [ image ]( void * unused ) { // By capturing `image` in this deleter, we ensure // that the underlying data does not get deallocated // before we're done with the tensor. } );","title":"Examples"},{"location":"advanced/efficient_tensor_creation/#copying-data-into-a-tensor","text":"","title":"Copying data into a tensor"},{"location":"advanced/efficient_tensor_creation/#pros_2","text":"Very easy to do No memory alignment requirements No need to work with deleters","title":"Pros"},{"location":"advanced/efficient_tensor_creation/#cons_2","text":"Always makes an extra copy during in-process execution Compared to #1, this makes an extra copy during out-of-process execution (although this copy is explicitly written by the user)","title":"Cons"},{"location":"advanced/efficient_tensor_creation/#examples_2","text":"Copying from a cv::Mat : cv :: Mat image = ... // An image from somewhere auto tensor = allocator -> allocate_tensor < uint8_t > ( // Dimensions { 1 , image . rows , image . cols , image . channels ()} ); // Copy data into the tensor tensor -> copy_from ( image . data , tensor -> get_num_elements ());","title":"Examples"},{"location":"advanced/efficient_tensor_creation/#which-one-should-i-use","text":"In general, the order of approaches in terms of performance is the following: Writing data directly into a tensor Wrapping existing memory Copying data into a tensor That said, profiling is your friend. The tradeoff between simplicity and performance is also different for large vs small tensors since copies are cheaper for small tensors.","title":"Which one should I use?"},{"location":"advanced/ope/","text":"Out-of-Process Execution \u00b6 Neuropod can run models in different processes using an optimized shared memory implementation with extremely low overhead (~100 to 500 microseconds). To run a model in another process, set the use_ope option when loading a model: neuropod :: RuntimeOptions opts ; opts . use_ope = true ; Neuropod model ( neuropod_path , opts ); Nothing else should need to change. There are many potential benefits of this approach: Run models that require different versions of Torch or TF from the same \"master\" process ( in progress ) Pin the worker process to a specific core to reduce variability in inference time ( in progress ) Isolate models from each other and from the rest of your program Avoid sharing the GIL between multiple python models in the same process The worker process can also be run in a docker container to provide even more isolation. For more details and options, see the OPEOptions struct inside RuntimeOptions .","title":"Out-of-process Execution"},{"location":"advanced/ope/#out-of-process-execution","text":"Neuropod can run models in different processes using an optimized shared memory implementation with extremely low overhead (~100 to 500 microseconds). To run a model in another process, set the use_ope option when loading a model: neuropod :: RuntimeOptions opts ; opts . use_ope = true ; Neuropod model ( neuropod_path , opts ); Nothing else should need to change. There are many potential benefits of this approach: Run models that require different versions of Torch or TF from the same \"master\" process ( in progress ) Pin the worker process to a specific core to reduce variability in inference time ( in progress ) Isolate models from each other and from the rest of your program Avoid sharing the GIL between multiple python models in the same process The worker process can also be run in a docker container to provide even more isolation. For more details and options, see the OPEOptions struct inside RuntimeOptions .","title":"Out-of-Process Execution"},{"location":"packagers/keras/","text":"create_keras_neuropod \u00b6 Packages a Keras model as a neuropod package. Currently, only the TensorFlow backend is supported. create_keras_neuropod( neuropod_path, model_name, sess, model, node_name_mapping = None, platform_version_semver = *, input_spec = None, output_spec = None, input_tensor_device = None, default_input_tensor_device = GPU, custom_ops = [], package_as_zip = True, test_input_data = None, test_expected_out = None, persist_test_data = True, ) Params: \u00b6 neuropod_path \u00b6 The output neuropod path model_name \u00b6 The name of the model sess \u00b6 A TensorFlow session containing weights (usually keras.backend.get_session() ). model \u00b6 A Keras model object. node_name_mapping \u00b6 default: None Optional mapping from a neuropod input/output name to a name of Keras input/output Example : { \"x\": \"input_1\", \"out\": \"fc1000\", } Defaults to using Keras input/output names as neuropod input/output names. platform_version_semver \u00b6 default: * The versions of the platform (e.g. Torch, TensorFlow, etc) that this model is compatible with specified as semver range. See https://semver.org/, https://docs.npmjs.com/misc/semver#ranges or https://docs.npmjs.com/misc/semver#advanced-range-syntax for examples and more info. Default is * (any version is okay). When this model is loaded, Neuropod will load it with a backend that is compatible with the specified versions ranges or throw an error if no compatible backends are installed. This can be used to ensure a model always runs with a particular version of a framework. Example : 1.13.1 or > 1.13.1 or 1.4.0 - 1.6.0 input_spec \u00b6 default: None A list of dicts specifying the input to the model. For each input, if shape is set to None , no validation is done on the shape. If shape is a tuple, the dimensions of the input are validated against that tuple. A value of None for any of the dimensions means that dimension will not be checked. dtype can be any valid numpy datatype string. Example : [ {\"name\": \"x\", \"dtype\": \"float32\", \"shape\": (None,)}, {\"name\": \"y\", \"dtype\": \"float32\", \"shape\": (None,)}, ] output_spec \u00b6 default: None A list of dicts specifying the output of the model. See the documentation for the input_spec parameter for more details. Example : [ {\"name\": \"out\", \"dtype\": \"float32\", \"shape\": (None,)}, ] input_tensor_device \u00b6 default: None A dict mapping input tensor names to the device that the model expects them to be on. This can either be GPU or CPU . Any tensors in input_spec not specified in this mapping will use the default_input_tensor_device specified below. If a GPU is selected at inference time, Neuropod will move tensors to the appropriate devices before running the model. Otherwise, it will attempt to run the model on CPU and move all tensors (and the model) to CPU. See the docstring for load_neuropod for more info. Example : {\"x\": \"GPU\"} default_input_tensor_device \u00b6 default: GPU The default device that input tensors are expected to be on. This can either be GPU or CPU . custom_ops \u00b6 default: [] A list of paths to custom op shared libraries to include in the packaged neuropod. Note: Including custom ops ties your neuropod to the specific platform (e.g. Mac, Linux) that the custom ops were built for. It is the user's responsibility to ensure that their custom ops are built for the correct platform. Example : [\"/path/to/my/custom_op.so\"] package_as_zip \u00b6 default: True Whether to package the neuropod as a single file or as a directory. test_input_data \u00b6 default: None Optional sample input data. This is a dict mapping input names to values. If this is provided, inference will be run in an isolated environment immediately after packaging to ensure that the neuropod was created successfully. Must be provided if test_expected_out is provided. Throws a ValueError if inference failed. Example : { \"x\": np.arange(5), \"y\": np.arange(5), } test_expected_out \u00b6 default: None Optional expected output. Throws a ValueError if the output of model inference does not match the expected output. Example : { \"out\": np.arange(5) + np.arange(5) } persist_test_data \u00b6 default: True Optionally saves the test data within the packaged neuropod.","title":"Keras"},{"location":"packagers/keras/#create_keras_neuropod","text":"Packages a Keras model as a neuropod package. Currently, only the TensorFlow backend is supported. create_keras_neuropod( neuropod_path, model_name, sess, model, node_name_mapping = None, platform_version_semver = *, input_spec = None, output_spec = None, input_tensor_device = None, default_input_tensor_device = GPU, custom_ops = [], package_as_zip = True, test_input_data = None, test_expected_out = None, persist_test_data = True, )","title":"create_keras_neuropod"},{"location":"packagers/keras/#params","text":"","title":"Params:"},{"location":"packagers/keras/#neuropod_path","text":"The output neuropod path","title":"neuropod_path"},{"location":"packagers/keras/#model_name","text":"The name of the model","title":"model_name"},{"location":"packagers/keras/#sess","text":"A TensorFlow session containing weights (usually keras.backend.get_session() ).","title":"sess"},{"location":"packagers/keras/#model","text":"A Keras model object.","title":"model"},{"location":"packagers/keras/#node_name_mapping","text":"default: None Optional mapping from a neuropod input/output name to a name of Keras input/output Example : { \"x\": \"input_1\", \"out\": \"fc1000\", } Defaults to using Keras input/output names as neuropod input/output names.","title":"node_name_mapping"},{"location":"packagers/keras/#platform_version_semver","text":"default: * The versions of the platform (e.g. Torch, TensorFlow, etc) that this model is compatible with specified as semver range. See https://semver.org/, https://docs.npmjs.com/misc/semver#ranges or https://docs.npmjs.com/misc/semver#advanced-range-syntax for examples and more info. Default is * (any version is okay). When this model is loaded, Neuropod will load it with a backend that is compatible with the specified versions ranges or throw an error if no compatible backends are installed. This can be used to ensure a model always runs with a particular version of a framework. Example : 1.13.1 or > 1.13.1 or 1.4.0 - 1.6.0","title":"platform_version_semver"},{"location":"packagers/keras/#input_spec","text":"default: None A list of dicts specifying the input to the model. For each input, if shape is set to None , no validation is done on the shape. If shape is a tuple, the dimensions of the input are validated against that tuple. A value of None for any of the dimensions means that dimension will not be checked. dtype can be any valid numpy datatype string. Example : [ {\"name\": \"x\", \"dtype\": \"float32\", \"shape\": (None,)}, {\"name\": \"y\", \"dtype\": \"float32\", \"shape\": (None,)}, ]","title":"input_spec"},{"location":"packagers/keras/#output_spec","text":"default: None A list of dicts specifying the output of the model. See the documentation for the input_spec parameter for more details. Example : [ {\"name\": \"out\", \"dtype\": \"float32\", \"shape\": (None,)}, ]","title":"output_spec"},{"location":"packagers/keras/#input_tensor_device","text":"default: None A dict mapping input tensor names to the device that the model expects them to be on. This can either be GPU or CPU . Any tensors in input_spec not specified in this mapping will use the default_input_tensor_device specified below. If a GPU is selected at inference time, Neuropod will move tensors to the appropriate devices before running the model. Otherwise, it will attempt to run the model on CPU and move all tensors (and the model) to CPU. See the docstring for load_neuropod for more info. Example : {\"x\": \"GPU\"}","title":"input_tensor_device"},{"location":"packagers/keras/#default_input_tensor_device","text":"default: GPU The default device that input tensors are expected to be on. This can either be GPU or CPU .","title":"default_input_tensor_device"},{"location":"packagers/keras/#custom_ops","text":"default: [] A list of paths to custom op shared libraries to include in the packaged neuropod. Note: Including custom ops ties your neuropod to the specific platform (e.g. Mac, Linux) that the custom ops were built for. It is the user's responsibility to ensure that their custom ops are built for the correct platform. Example : [\"/path/to/my/custom_op.so\"]","title":"custom_ops"},{"location":"packagers/keras/#package_as_zip","text":"default: True Whether to package the neuropod as a single file or as a directory.","title":"package_as_zip"},{"location":"packagers/keras/#test_input_data","text":"default: None Optional sample input data. This is a dict mapping input names to values. If this is provided, inference will be run in an isolated environment immediately after packaging to ensure that the neuropod was created successfully. Must be provided if test_expected_out is provided. Throws a ValueError if inference failed. Example : { \"x\": np.arange(5), \"y\": np.arange(5), }","title":"test_input_data"},{"location":"packagers/keras/#test_expected_out","text":"default: None Optional expected output. Throws a ValueError if the output of model inference does not match the expected output. Example : { \"out\": np.arange(5) + np.arange(5) }","title":"test_expected_out"},{"location":"packagers/keras/#persist_test_data","text":"default: True Optionally saves the test data within the packaged neuropod.","title":"persist_test_data"},{"location":"packagers/pytorch/","text":"create_python_neuropod \u00b6 Packages arbitrary python code as a neuropod package. create_python_neuropod( neuropod_path, model_name, data_paths, code_path_spec, entrypoint_package, entrypoint, input_spec, output_spec, requirements = None, platform_version_semver = *, input_tensor_device = None, default_input_tensor_device = GPU, custom_ops = [], package_as_zip = True, test_input_data = None, test_expected_out = None, persist_test_data = True, ) Params: \u00b6 neuropod_path \u00b6 The output neuropod path model_name \u00b6 The name of the model data_paths \u00b6 A list of dicts containing the paths to any data files that needs to be packaged. Example : [{ path: \"/path/to/myfile.txt\", packaged_name: \"newfilename.txt\" }] code_path_spec \u00b6 The folder paths of all the code that will be packaged. Note that *.pyc files are ignored. This is specified as follows: [{ \"python_root\": \"/some/path/to/a/python/root\", \"dirs_to_package\": [\"relative/path/to/package\"] }, ...] entrypoint_package \u00b6 The python package containing the entrypoint (e.g. some.package.something). This must contain the entrypoint function specified below. entrypoint \u00b6 The name of a function contained in the entrypoint_package . This function must return a callable that takes in the inputs specified in input_spec and returns a dict containing the outputs specified in output_spec . The entrypoint function will be provided the path to a directory containing the packaged data as its first parameter. For example, a function like: def neuropod_init(data_path): def addition_model(x, y): return { \"output\": x + y } return addition_model contained in the package 'my.awesome.addition_model' would have entrypoint_package='my.awesome.addition_model' and entrypoint='neuropod_init' input_spec \u00b6 A list of dicts specifying the input to the model. For each input, if shape is set to None , no validation is done on the shape. If shape is a tuple, the dimensions of the input are validated against that tuple. A value of None for any of the dimensions means that dimension will not be checked. dtype can be any valid numpy datatype string. Example : [ {\"name\": \"x\", \"dtype\": \"float32\", \"shape\": (None,)}, {\"name\": \"y\", \"dtype\": \"float32\", \"shape\": (None,)}, ] output_spec \u00b6 A list of dicts specifying the output of the model. See the documentation for the input_spec parameter for more details. Example : [ {\"name\": \"out\", \"dtype\": \"float32\", \"shape\": (None,)}, ] requirements \u00b6 default: None An optional string containing the runtime requirements of this model (specified in a format that pip understands) Example : tensorflow=1.15.0 numpy=1.8 platform_version_semver \u00b6 default: * The versions of the platform (e.g. Torch, TensorFlow, etc) that this model is compatible with specified as semver range. See https://semver.org/, https://docs.npmjs.com/misc/semver#ranges or https://docs.npmjs.com/misc/semver#advanced-range-syntax for examples and more info. Default is * (any version is okay). When this model is loaded, Neuropod will load it with a backend that is compatible with the specified versions ranges or throw an error if no compatible backends are installed. This can be used to ensure a model always runs with a particular version of a framework. Example : 1.13.1 or > 1.13.1 or 1.4.0 - 1.6.0 input_tensor_device \u00b6 default: None A dict mapping input tensor names to the device that the model expects them to be on. This can either be GPU or CPU . Any tensors in input_spec not specified in this mapping will use the default_input_tensor_device specified below. If a GPU is selected at inference time, Neuropod will move tensors to the appropriate devices before running the model. Otherwise, it will attempt to run the model on CPU and move all tensors (and the model) to CPU. See the docstring for load_neuropod for more info. Example : {\"x\": \"GPU\"} default_input_tensor_device \u00b6 default: GPU The default device that input tensors are expected to be on. This can either be GPU or CPU . custom_ops \u00b6 default: [] A list of paths to custom op shared libraries to include in the packaged neuropod. Note: Including custom ops ties your neuropod to the specific platform (e.g. Mac, Linux) that the custom ops were built for. It is the user's responsibility to ensure that their custom ops are built for the correct platform. Example : [\"/path/to/my/custom_op.so\"] package_as_zip \u00b6 default: True Whether to package the neuropod as a single file or as a directory. test_input_data \u00b6 default: None Optional sample input data. This is a dict mapping input names to values. If this is provided, inference will be run in an isolated environment immediately after packaging to ensure that the neuropod was created successfully. Must be provided if test_expected_out is provided. Throws a ValueError if inference failed. Example : { \"x\": np.arange(5), \"y\": np.arange(5), } test_expected_out \u00b6 default: None Optional expected output. Throws a ValueError if the output of model inference does not match the expected output. Example : { \"out\": np.arange(5) + np.arange(5) } persist_test_data \u00b6 default: True Optionally saves the test data within the packaged neuropod.","title":"PyTorch"},{"location":"packagers/pytorch/#create_python_neuropod","text":"Packages arbitrary python code as a neuropod package. create_python_neuropod( neuropod_path, model_name, data_paths, code_path_spec, entrypoint_package, entrypoint, input_spec, output_spec, requirements = None, platform_version_semver = *, input_tensor_device = None, default_input_tensor_device = GPU, custom_ops = [], package_as_zip = True, test_input_data = None, test_expected_out = None, persist_test_data = True, )","title":"create_python_neuropod"},{"location":"packagers/pytorch/#params","text":"","title":"Params:"},{"location":"packagers/pytorch/#neuropod_path","text":"The output neuropod path","title":"neuropod_path"},{"location":"packagers/pytorch/#model_name","text":"The name of the model","title":"model_name"},{"location":"packagers/pytorch/#data_paths","text":"A list of dicts containing the paths to any data files that needs to be packaged. Example : [{ path: \"/path/to/myfile.txt\", packaged_name: \"newfilename.txt\" }]","title":"data_paths"},{"location":"packagers/pytorch/#code_path_spec","text":"The folder paths of all the code that will be packaged. Note that *.pyc files are ignored. This is specified as follows: [{ \"python_root\": \"/some/path/to/a/python/root\", \"dirs_to_package\": [\"relative/path/to/package\"] }, ...]","title":"code_path_spec"},{"location":"packagers/pytorch/#entrypoint_package","text":"The python package containing the entrypoint (e.g. some.package.something). This must contain the entrypoint function specified below.","title":"entrypoint_package"},{"location":"packagers/pytorch/#entrypoint","text":"The name of a function contained in the entrypoint_package . This function must return a callable that takes in the inputs specified in input_spec and returns a dict containing the outputs specified in output_spec . The entrypoint function will be provided the path to a directory containing the packaged data as its first parameter. For example, a function like: def neuropod_init(data_path): def addition_model(x, y): return { \"output\": x + y } return addition_model contained in the package 'my.awesome.addition_model' would have entrypoint_package='my.awesome.addition_model' and entrypoint='neuropod_init'","title":"entrypoint"},{"location":"packagers/pytorch/#input_spec","text":"A list of dicts specifying the input to the model. For each input, if shape is set to None , no validation is done on the shape. If shape is a tuple, the dimensions of the input are validated against that tuple. A value of None for any of the dimensions means that dimension will not be checked. dtype can be any valid numpy datatype string. Example : [ {\"name\": \"x\", \"dtype\": \"float32\", \"shape\": (None,)}, {\"name\": \"y\", \"dtype\": \"float32\", \"shape\": (None,)}, ]","title":"input_spec"},{"location":"packagers/pytorch/#output_spec","text":"A list of dicts specifying the output of the model. See the documentation for the input_spec parameter for more details. Example : [ {\"name\": \"out\", \"dtype\": \"float32\", \"shape\": (None,)}, ]","title":"output_spec"},{"location":"packagers/pytorch/#requirements","text":"default: None An optional string containing the runtime requirements of this model (specified in a format that pip understands) Example : tensorflow=1.15.0 numpy=1.8","title":"requirements"},{"location":"packagers/pytorch/#platform_version_semver","text":"default: * The versions of the platform (e.g. Torch, TensorFlow, etc) that this model is compatible with specified as semver range. See https://semver.org/, https://docs.npmjs.com/misc/semver#ranges or https://docs.npmjs.com/misc/semver#advanced-range-syntax for examples and more info. Default is * (any version is okay). When this model is loaded, Neuropod will load it with a backend that is compatible with the specified versions ranges or throw an error if no compatible backends are installed. This can be used to ensure a model always runs with a particular version of a framework. Example : 1.13.1 or > 1.13.1 or 1.4.0 - 1.6.0","title":"platform_version_semver"},{"location":"packagers/pytorch/#input_tensor_device","text":"default: None A dict mapping input tensor names to the device that the model expects them to be on. This can either be GPU or CPU . Any tensors in input_spec not specified in this mapping will use the default_input_tensor_device specified below. If a GPU is selected at inference time, Neuropod will move tensors to the appropriate devices before running the model. Otherwise, it will attempt to run the model on CPU and move all tensors (and the model) to CPU. See the docstring for load_neuropod for more info. Example : {\"x\": \"GPU\"}","title":"input_tensor_device"},{"location":"packagers/pytorch/#default_input_tensor_device","text":"default: GPU The default device that input tensors are expected to be on. This can either be GPU or CPU .","title":"default_input_tensor_device"},{"location":"packagers/pytorch/#custom_ops","text":"default: [] A list of paths to custom op shared libraries to include in the packaged neuropod. Note: Including custom ops ties your neuropod to the specific platform (e.g. Mac, Linux) that the custom ops were built for. It is the user's responsibility to ensure that their custom ops are built for the correct platform. Example : [\"/path/to/my/custom_op.so\"]","title":"custom_ops"},{"location":"packagers/pytorch/#package_as_zip","text":"default: True Whether to package the neuropod as a single file or as a directory.","title":"package_as_zip"},{"location":"packagers/pytorch/#test_input_data","text":"default: None Optional sample input data. This is a dict mapping input names to values. If this is provided, inference will be run in an isolated environment immediately after packaging to ensure that the neuropod was created successfully. Must be provided if test_expected_out is provided. Throws a ValueError if inference failed. Example : { \"x\": np.arange(5), \"y\": np.arange(5), }","title":"test_input_data"},{"location":"packagers/pytorch/#test_expected_out","text":"default: None Optional expected output. Throws a ValueError if the output of model inference does not match the expected output. Example : { \"out\": np.arange(5) + np.arange(5) }","title":"test_expected_out"},{"location":"packagers/pytorch/#persist_test_data","text":"default: True Optionally saves the test data within the packaged neuropod.","title":"persist_test_data"},{"location":"packagers/tensorflow/","text":"create_tensorflow_neuropod \u00b6 Packages a TensorFlow model as a neuropod package. create_tensorflow_neuropod( neuropod_path, model_name, input_spec, output_spec, node_name_mapping = None, frozen_graph_path = None, graph_def = None, saved_model_dir = None, trackable_obj = None, init_op_names = [], platform_version_semver = *, input_tensor_device = None, default_input_tensor_device = GPU, custom_ops = [], package_as_zip = True, test_input_data = None, test_expected_out = None, persist_test_data = True, ) Params: \u00b6 neuropod_path \u00b6 The output neuropod path model_name \u00b6 The name of the model input_spec \u00b6 A list of dicts specifying the input to the model. For each input, if shape is set to None , no validation is done on the shape. If shape is a tuple, the dimensions of the input are validated against that tuple. A value of None for any of the dimensions means that dimension will not be checked. dtype can be any valid numpy datatype string. Example : [ {\"name\": \"x\", \"dtype\": \"float32\", \"shape\": (None,)}, {\"name\": \"y\", \"dtype\": \"float32\", \"shape\": (None,)}, ] output_spec \u00b6 A list of dicts specifying the output of the model. See the documentation for the input_spec parameter for more details. Example : [ {\"name\": \"out\", \"dtype\": \"float32\", \"shape\": (None,)}, ] node_name_mapping \u00b6 default: None Mapping from a neuropod input/output name to a node in the graph. The :0 is optional. Required unless using a saved model. Example : { \"x\": \"some_namespace/in_x:0\", \"y\": \"some_namespace/in_y:0\", \"out\": \"some_namespace/out:0\", } frozen_graph_path \u00b6 default: None The path to a frozen tensorflow graph. Exactly one of frozen_graph_path , graph_def , saved_model_dir and trackable_obj must be provided. graph_def \u00b6 default: None A tensorflow GraphDef object. Exactly one of frozen_graph_path , graph_def , saved_model_dir and trackable_obj must be provided. saved_model_dir \u00b6 default: None The path to a tensorflow saved model dir. Exactly one of frozen_graph_path , graph_def , saved_model_dir and trackable_obj must be provided. Note: this is only tested with TF 2.x at the moment trackable_obj \u00b6 default: None A trackable object that can be passed to tf.saved_model.save . For more control over the saved model, you can create one yourself and pass in the path using saved_model_dir . Exactly one of frozen_graph_path , graph_def , saved_model_dir and trackable_obj must be provided. Note: this is only tested with TF 2.x at the moment init_op_names \u00b6 default: [] A list of initialization operator names. These operations are evaluated in the session used for inference right after the session is created. These operators may be used for initialization of variables. platform_version_semver \u00b6 default: * The versions of the platform (e.g. Torch, TensorFlow, etc) that this model is compatible with specified as semver range. See https://semver.org/, https://docs.npmjs.com/misc/semver#ranges or https://docs.npmjs.com/misc/semver#advanced-range-syntax for examples and more info. Default is * (any version is okay). When this model is loaded, Neuropod will load it with a backend that is compatible with the specified versions ranges or throw an error if no compatible backends are installed. This can be used to ensure a model always runs with a particular version of a framework. Example : 1.13.1 or > 1.13.1 or 1.4.0 - 1.6.0 input_tensor_device \u00b6 default: None A dict mapping input tensor names to the device that the model expects them to be on. This can either be GPU or CPU . Any tensors in input_spec not specified in this mapping will use the default_input_tensor_device specified below. If a GPU is selected at inference time, Neuropod will move tensors to the appropriate devices before running the model. Otherwise, it will attempt to run the model on CPU and move all tensors (and the model) to CPU. See the docstring for load_neuropod for more info. Example : {\"x\": \"GPU\"} default_input_tensor_device \u00b6 default: GPU The default device that input tensors are expected to be on. This can either be GPU or CPU . custom_ops \u00b6 default: [] A list of paths to custom op shared libraries to include in the packaged neuropod. Note: Including custom ops ties your neuropod to the specific platform (e.g. Mac, Linux) that the custom ops were built for. It is the user's responsibility to ensure that their custom ops are built for the correct platform. Example : [\"/path/to/my/custom_op.so\"] package_as_zip \u00b6 default: True Whether to package the neuropod as a single file or as a directory. test_input_data \u00b6 default: None Optional sample input data. This is a dict mapping input names to values. If this is provided, inference will be run in an isolated environment immediately after packaging to ensure that the neuropod was created successfully. Must be provided if test_expected_out is provided. Throws a ValueError if inference failed. Example : { \"x\": np.arange(5), \"y\": np.arange(5), } test_expected_out \u00b6 default: None Optional expected output. Throws a ValueError if the output of model inference does not match the expected output. Example : { \"out\": np.arange(5) + np.arange(5) } persist_test_data \u00b6 default: True Optionally saves the test data within the packaged neuropod.","title":"TensorFlow"},{"location":"packagers/tensorflow/#create_tensorflow_neuropod","text":"Packages a TensorFlow model as a neuropod package. create_tensorflow_neuropod( neuropod_path, model_name, input_spec, output_spec, node_name_mapping = None, frozen_graph_path = None, graph_def = None, saved_model_dir = None, trackable_obj = None, init_op_names = [], platform_version_semver = *, input_tensor_device = None, default_input_tensor_device = GPU, custom_ops = [], package_as_zip = True, test_input_data = None, test_expected_out = None, persist_test_data = True, )","title":"create_tensorflow_neuropod"},{"location":"packagers/tensorflow/#params","text":"","title":"Params:"},{"location":"packagers/tensorflow/#neuropod_path","text":"The output neuropod path","title":"neuropod_path"},{"location":"packagers/tensorflow/#model_name","text":"The name of the model","title":"model_name"},{"location":"packagers/tensorflow/#input_spec","text":"A list of dicts specifying the input to the model. For each input, if shape is set to None , no validation is done on the shape. If shape is a tuple, the dimensions of the input are validated against that tuple. A value of None for any of the dimensions means that dimension will not be checked. dtype can be any valid numpy datatype string. Example : [ {\"name\": \"x\", \"dtype\": \"float32\", \"shape\": (None,)}, {\"name\": \"y\", \"dtype\": \"float32\", \"shape\": (None,)}, ]","title":"input_spec"},{"location":"packagers/tensorflow/#output_spec","text":"A list of dicts specifying the output of the model. See the documentation for the input_spec parameter for more details. Example : [ {\"name\": \"out\", \"dtype\": \"float32\", \"shape\": (None,)}, ]","title":"output_spec"},{"location":"packagers/tensorflow/#node_name_mapping","text":"default: None Mapping from a neuropod input/output name to a node in the graph. The :0 is optional. Required unless using a saved model. Example : { \"x\": \"some_namespace/in_x:0\", \"y\": \"some_namespace/in_y:0\", \"out\": \"some_namespace/out:0\", }","title":"node_name_mapping"},{"location":"packagers/tensorflow/#frozen_graph_path","text":"default: None The path to a frozen tensorflow graph. Exactly one of frozen_graph_path , graph_def , saved_model_dir and trackable_obj must be provided.","title":"frozen_graph_path"},{"location":"packagers/tensorflow/#graph_def","text":"default: None A tensorflow GraphDef object. Exactly one of frozen_graph_path , graph_def , saved_model_dir and trackable_obj must be provided.","title":"graph_def"},{"location":"packagers/tensorflow/#saved_model_dir","text":"default: None The path to a tensorflow saved model dir. Exactly one of frozen_graph_path , graph_def , saved_model_dir and trackable_obj must be provided. Note: this is only tested with TF 2.x at the moment","title":"saved_model_dir"},{"location":"packagers/tensorflow/#trackable_obj","text":"default: None A trackable object that can be passed to tf.saved_model.save . For more control over the saved model, you can create one yourself and pass in the path using saved_model_dir . Exactly one of frozen_graph_path , graph_def , saved_model_dir and trackable_obj must be provided. Note: this is only tested with TF 2.x at the moment","title":"trackable_obj"},{"location":"packagers/tensorflow/#init_op_names","text":"default: [] A list of initialization operator names. These operations are evaluated in the session used for inference right after the session is created. These operators may be used for initialization of variables.","title":"init_op_names"},{"location":"packagers/tensorflow/#platform_version_semver","text":"default: * The versions of the platform (e.g. Torch, TensorFlow, etc) that this model is compatible with specified as semver range. See https://semver.org/, https://docs.npmjs.com/misc/semver#ranges or https://docs.npmjs.com/misc/semver#advanced-range-syntax for examples and more info. Default is * (any version is okay). When this model is loaded, Neuropod will load it with a backend that is compatible with the specified versions ranges or throw an error if no compatible backends are installed. This can be used to ensure a model always runs with a particular version of a framework. Example : 1.13.1 or > 1.13.1 or 1.4.0 - 1.6.0","title":"platform_version_semver"},{"location":"packagers/tensorflow/#input_tensor_device","text":"default: None A dict mapping input tensor names to the device that the model expects them to be on. This can either be GPU or CPU . Any tensors in input_spec not specified in this mapping will use the default_input_tensor_device specified below. If a GPU is selected at inference time, Neuropod will move tensors to the appropriate devices before running the model. Otherwise, it will attempt to run the model on CPU and move all tensors (and the model) to CPU. See the docstring for load_neuropod for more info. Example : {\"x\": \"GPU\"}","title":"input_tensor_device"},{"location":"packagers/tensorflow/#default_input_tensor_device","text":"default: GPU The default device that input tensors are expected to be on. This can either be GPU or CPU .","title":"default_input_tensor_device"},{"location":"packagers/tensorflow/#custom_ops","text":"default: [] A list of paths to custom op shared libraries to include in the packaged neuropod. Note: Including custom ops ties your neuropod to the specific platform (e.g. Mac, Linux) that the custom ops were built for. It is the user's responsibility to ensure that their custom ops are built for the correct platform. Example : [\"/path/to/my/custom_op.so\"]","title":"custom_ops"},{"location":"packagers/tensorflow/#package_as_zip","text":"default: True Whether to package the neuropod as a single file or as a directory.","title":"package_as_zip"},{"location":"packagers/tensorflow/#test_input_data","text":"default: None Optional sample input data. This is a dict mapping input names to values. If this is provided, inference will be run in an isolated environment immediately after packaging to ensure that the neuropod was created successfully. Must be provided if test_expected_out is provided. Throws a ValueError if inference failed. Example : { \"x\": np.arange(5), \"y\": np.arange(5), }","title":"test_input_data"},{"location":"packagers/tensorflow/#test_expected_out","text":"default: None Optional expected output. Throws a ValueError if the output of model inference does not match the expected output. Example : { \"out\": np.arange(5) + np.arange(5) }","title":"test_expected_out"},{"location":"packagers/tensorflow/#persist_test_data","text":"default: True Optionally saves the test data within the packaged neuropod.","title":"persist_test_data"},{"location":"packagers/torchscript/","text":"create_torchscript_neuropod \u00b6 Packages a TorchScript model as a neuropod package. create_torchscript_neuropod( neuropod_path, model_name, input_spec, output_spec, module = None, module_path = None, platform_version_semver = *, input_tensor_device = None, default_input_tensor_device = GPU, custom_ops = [], package_as_zip = True, test_input_data = None, test_expected_out = None, persist_test_data = True, ) Params: \u00b6 neuropod_path \u00b6 The output neuropod path model_name \u00b6 The name of the model input_spec \u00b6 A list of dicts specifying the input to the model. For each input, if shape is set to None , no validation is done on the shape. If shape is a tuple, the dimensions of the input are validated against that tuple. A value of None for any of the dimensions means that dimension will not be checked. dtype can be any valid numpy datatype string. Example : [ {\"name\": \"x\", \"dtype\": \"float32\", \"shape\": (None,)}, {\"name\": \"y\", \"dtype\": \"float32\", \"shape\": (None,)}, ] output_spec \u00b6 A list of dicts specifying the output of the model. See the documentation for the input_spec parameter for more details. Example : [ {\"name\": \"out\", \"dtype\": \"float32\", \"shape\": (None,)}, ] module \u00b6 default: None An instance of a PyTorch ScriptModule. This model should return the outputs as a dictionary. If this is not provided, module_path must be set. For example, a model may output something like this: { \"output1\": value1, \"output2\": value2, } module_path \u00b6 default: None The path to a ScriptModule that was already exported using torch.jit.save . If this is not provided, module must be set. platform_version_semver \u00b6 default: * The versions of the platform (e.g. Torch, TensorFlow, etc) that this model is compatible with specified as semver range. See https://semver.org/, https://docs.npmjs.com/misc/semver#ranges or https://docs.npmjs.com/misc/semver#advanced-range-syntax for examples and more info. Default is * (any version is okay). When this model is loaded, Neuropod will load it with a backend that is compatible with the specified versions ranges or throw an error if no compatible backends are installed. This can be used to ensure a model always runs with a particular version of a framework. Example : 1.13.1 or > 1.13.1 or 1.4.0 - 1.6.0 input_tensor_device \u00b6 default: None A dict mapping input tensor names to the device that the model expects them to be on. This can either be GPU or CPU . Any tensors in input_spec not specified in this mapping will use the default_input_tensor_device specified below. If a GPU is selected at inference time, Neuropod will move tensors to the appropriate devices before running the model. Otherwise, it will attempt to run the model on CPU and move all tensors (and the model) to CPU. See the docstring for load_neuropod for more info. Example : {\"x\": \"GPU\"} default_input_tensor_device \u00b6 default: GPU The default device that input tensors are expected to be on. This can either be GPU or CPU . custom_ops \u00b6 default: [] A list of paths to custom op shared libraries to include in the packaged neuropod. Note: Including custom ops ties your neuropod to the specific platform (e.g. Mac, Linux) that the custom ops were built for. It is the user's responsibility to ensure that their custom ops are built for the correct platform. Example : [\"/path/to/my/custom_op.so\"] package_as_zip \u00b6 default: True Whether to package the neuropod as a single file or as a directory. test_input_data \u00b6 default: None Optional sample input data. This is a dict mapping input names to values. If this is provided, inference will be run in an isolated environment immediately after packaging to ensure that the neuropod was created successfully. Must be provided if test_expected_out is provided. Throws a ValueError if inference failed. Example : { \"x\": np.arange(5), \"y\": np.arange(5), } test_expected_out \u00b6 default: None Optional expected output. Throws a ValueError if the output of model inference does not match the expected output. Example : { \"out\": np.arange(5) + np.arange(5) } persist_test_data \u00b6 default: True Optionally saves the test data within the packaged neuropod.","title":"TorchScript"},{"location":"packagers/torchscript/#create_torchscript_neuropod","text":"Packages a TorchScript model as a neuropod package. create_torchscript_neuropod( neuropod_path, model_name, input_spec, output_spec, module = None, module_path = None, platform_version_semver = *, input_tensor_device = None, default_input_tensor_device = GPU, custom_ops = [], package_as_zip = True, test_input_data = None, test_expected_out = None, persist_test_data = True, )","title":"create_torchscript_neuropod"},{"location":"packagers/torchscript/#params","text":"","title":"Params:"},{"location":"packagers/torchscript/#neuropod_path","text":"The output neuropod path","title":"neuropod_path"},{"location":"packagers/torchscript/#model_name","text":"The name of the model","title":"model_name"},{"location":"packagers/torchscript/#input_spec","text":"A list of dicts specifying the input to the model. For each input, if shape is set to None , no validation is done on the shape. If shape is a tuple, the dimensions of the input are validated against that tuple. A value of None for any of the dimensions means that dimension will not be checked. dtype can be any valid numpy datatype string. Example : [ {\"name\": \"x\", \"dtype\": \"float32\", \"shape\": (None,)}, {\"name\": \"y\", \"dtype\": \"float32\", \"shape\": (None,)}, ]","title":"input_spec"},{"location":"packagers/torchscript/#output_spec","text":"A list of dicts specifying the output of the model. See the documentation for the input_spec parameter for more details. Example : [ {\"name\": \"out\", \"dtype\": \"float32\", \"shape\": (None,)}, ]","title":"output_spec"},{"location":"packagers/torchscript/#module","text":"default: None An instance of a PyTorch ScriptModule. This model should return the outputs as a dictionary. If this is not provided, module_path must be set. For example, a model may output something like this: { \"output1\": value1, \"output2\": value2, }","title":"module"},{"location":"packagers/torchscript/#module_path","text":"default: None The path to a ScriptModule that was already exported using torch.jit.save . If this is not provided, module must be set.","title":"module_path"},{"location":"packagers/torchscript/#platform_version_semver","text":"default: * The versions of the platform (e.g. Torch, TensorFlow, etc) that this model is compatible with specified as semver range. See https://semver.org/, https://docs.npmjs.com/misc/semver#ranges or https://docs.npmjs.com/misc/semver#advanced-range-syntax for examples and more info. Default is * (any version is okay). When this model is loaded, Neuropod will load it with a backend that is compatible with the specified versions ranges or throw an error if no compatible backends are installed. This can be used to ensure a model always runs with a particular version of a framework. Example : 1.13.1 or > 1.13.1 or 1.4.0 - 1.6.0","title":"platform_version_semver"},{"location":"packagers/torchscript/#input_tensor_device","text":"default: None A dict mapping input tensor names to the device that the model expects them to be on. This can either be GPU or CPU . Any tensors in input_spec not specified in this mapping will use the default_input_tensor_device specified below. If a GPU is selected at inference time, Neuropod will move tensors to the appropriate devices before running the model. Otherwise, it will attempt to run the model on CPU and move all tensors (and the model) to CPU. See the docstring for load_neuropod for more info. Example : {\"x\": \"GPU\"}","title":"input_tensor_device"},{"location":"packagers/torchscript/#default_input_tensor_device","text":"default: GPU The default device that input tensors are expected to be on. This can either be GPU or CPU .","title":"default_input_tensor_device"},{"location":"packagers/torchscript/#custom_ops","text":"default: [] A list of paths to custom op shared libraries to include in the packaged neuropod. Note: Including custom ops ties your neuropod to the specific platform (e.g. Mac, Linux) that the custom ops were built for. It is the user's responsibility to ensure that their custom ops are built for the correct platform. Example : [\"/path/to/my/custom_op.so\"]","title":"custom_ops"},{"location":"packagers/torchscript/#package_as_zip","text":"default: True Whether to package the neuropod as a single file or as a directory.","title":"package_as_zip"},{"location":"packagers/torchscript/#test_input_data","text":"default: None Optional sample input data. This is a dict mapping input names to values. If this is provided, inference will be run in an isolated environment immediately after packaging to ensure that the neuropod was created successfully. Must be provided if test_expected_out is provided. Throws a ValueError if inference failed. Example : { \"x\": np.arange(5), \"y\": np.arange(5), }","title":"test_input_data"},{"location":"packagers/torchscript/#test_expected_out","text":"default: None Optional expected output. Throws a ValueError if the output of model inference does not match the expected output. Example : { \"out\": np.arange(5) + np.arange(5) }","title":"test_expected_out"},{"location":"packagers/torchscript/#persist_test_data","text":"default: True Optionally saves the test data within the packaged neuropod.","title":"persist_test_data"}]}