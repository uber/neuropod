{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Neuropod What is Neuropod? Neuropod is a library that provides a uniform interface to run deep learning models from multiple frameworks in C++ and Python. Neuropod makes it easy for researchers to build models in a framework of their choosing while also simplifying productionization of these models. It currently supports TensorFlow, PyTorch, TorchScript, and Keras. Why use Neuropod? Run models from any supported framework using one API Running a TensorFlow model looks exactly like running a PyTorch model. x = np . array ([ 1 , 2 , 3 , 4 ]) y = np . array ([ 5 , 6 , 7 , 8 ]) for model_path in [ TF_ADDITION_MODEL_PATH , PYTORCH_ADDITION_MODEL_PATH ]: # Load the model neuropod = load_neuropod ( model_path ) # Run inference results = neuropod . infer ({ x : x , y : y }) # array([6, 8, 10, 12]) print results [ out ] See the tutorial , Python guide , or C++ guide for more examples. Some benefits of this include: All of your inference code is framework agnostic. You can easily switch between deep learning frameworks if necessary without changing runtime code. Avoid the learning curve of using the C++ libtorch API and the C/C++ TF API Any Neuropod model can be run from both C++ and Python (even PyTorch models that have not been converted to TorchScript). Define a Problem API This lets you focus more on the problem you're solving rather than the framework you're using to solve it. For example, if you define a problem API for 2d object detection, any model that implements it can reuse all the existing inference code and infrastructure for that problem. INPUT_SPEC = [ # BGR image { name : image , dtype : uint8 , shape : ( 1200 , 1920 , 3 )}, ] OUTPUT_SPEC = [ # shape: (num_detections, 4): (xmin, ymin, xmax, ymax) # These values are in units of pixels. The origin is the top left corner # with positive X to the right and positive Y towards the bottom of the image { name : boxes , dtype : float32 , shape : ( num_detections , 4 )}, # The list of classes that the network can output # This must be some subset of [ vehicle , person , motorcycle , bicycle ] { name : supported_object_classes , dtype : string , shape : ( num_classes ,)}, # The probability of each class for each detection # These should all be floats between 0 and 1 { name : object_class_probability , dtype : float32 , shape : ( num_detections , num_classes )}, ] This lets you Build a single metrics pipeline for a problem Easily compare models solving the same problem (even if they're in different frameworks) Build optimized inference code that can run any model that solves a particular problem Swap out models that solve the same problem at runtime with no code change (even if the models are from different frameworks) Run fast experiments See the tutorial for more details. Build generic tools and pipelines If you have several models that take in a similar set of inputs, you can build and optimize one framework-agnostic input generation pipeline and share it across models. Other benefits Fully self-contained models (including custom ops) Efficient zero-copy operations Tested on platforms including Mac, Linux, Linux (GPU) Four or five versions of each supported framework Five versions of Python Model isolation with out-of-process execution Use multiple different versions of frameworks in the same application Ex: Experimental models using Torch nightly along with models using Torch 1.1.0 Switch from running in-process to running out-of-process with one line of code Getting started See the basic introduction tutorial for an overview of how to get started with Neuropod. The Python guide and C++ guide go into more detail on running Neuropod models.","title":"Welcome"},{"location":"#neuropod","text":"","title":"Neuropod"},{"location":"#what-is-neuropod","text":"Neuropod is a library that provides a uniform interface to run deep learning models from multiple frameworks in C++ and Python. Neuropod makes it easy for researchers to build models in a framework of their choosing while also simplifying productionization of these models. It currently supports TensorFlow, PyTorch, TorchScript, and Keras.","title":"What is Neuropod?"},{"location":"#why-use-neuropod","text":"","title":"Why use Neuropod?"},{"location":"#run-models-from-any-supported-framework-using-one-api","text":"Running a TensorFlow model looks exactly like running a PyTorch model. x = np . array ([ 1 , 2 , 3 , 4 ]) y = np . array ([ 5 , 6 , 7 , 8 ]) for model_path in [ TF_ADDITION_MODEL_PATH , PYTORCH_ADDITION_MODEL_PATH ]: # Load the model neuropod = load_neuropod ( model_path ) # Run inference results = neuropod . infer ({ x : x , y : y }) # array([6, 8, 10, 12]) print results [ out ] See the tutorial , Python guide , or C++ guide for more examples. Some benefits of this include: All of your inference code is framework agnostic. You can easily switch between deep learning frameworks if necessary without changing runtime code. Avoid the learning curve of using the C++ libtorch API and the C/C++ TF API Any Neuropod model can be run from both C++ and Python (even PyTorch models that have not been converted to TorchScript).","title":"Run models from any supported framework using one API"},{"location":"#define-a-problem-api","text":"This lets you focus more on the problem you're solving rather than the framework you're using to solve it. For example, if you define a problem API for 2d object detection, any model that implements it can reuse all the existing inference code and infrastructure for that problem. INPUT_SPEC = [ # BGR image { name : image , dtype : uint8 , shape : ( 1200 , 1920 , 3 )}, ] OUTPUT_SPEC = [ # shape: (num_detections, 4): (xmin, ymin, xmax, ymax) # These values are in units of pixels. The origin is the top left corner # with positive X to the right and positive Y towards the bottom of the image { name : boxes , dtype : float32 , shape : ( num_detections , 4 )}, # The list of classes that the network can output # This must be some subset of [ vehicle , person , motorcycle , bicycle ] { name : supported_object_classes , dtype : string , shape : ( num_classes ,)}, # The probability of each class for each detection # These should all be floats between 0 and 1 { name : object_class_probability , dtype : float32 , shape : ( num_detections , num_classes )}, ] This lets you Build a single metrics pipeline for a problem Easily compare models solving the same problem (even if they're in different frameworks) Build optimized inference code that can run any model that solves a particular problem Swap out models that solve the same problem at runtime with no code change (even if the models are from different frameworks) Run fast experiments See the tutorial for more details.","title":"Define a Problem API"},{"location":"#build-generic-tools-and-pipelines","text":"If you have several models that take in a similar set of inputs, you can build and optimize one framework-agnostic input generation pipeline and share it across models.","title":"Build generic tools and pipelines"},{"location":"#other-benefits","text":"Fully self-contained models (including custom ops) Efficient zero-copy operations Tested on platforms including Mac, Linux, Linux (GPU) Four or five versions of each supported framework Five versions of Python Model isolation with out-of-process execution Use multiple different versions of frameworks in the same application Ex: Experimental models using Torch nightly along with models using Torch 1.1.0 Switch from running in-process to running out-of-process with one line of code","title":"Other benefits"},{"location":"#getting-started","text":"See the basic introduction tutorial for an overview of how to get started with Neuropod. The Python guide and C++ guide go into more detail on running Neuropod models.","title":"Getting started"},{"location":"cppguide/","text":"C++ Guide This guide walks through the Neuropod C++ API in detail and goes over many different ways of working with models. Tip The Neuropod runtime interface is identical for all frameworks so this guide applies for models from all supported frameworks (including TensorFlow, PyTorch, Keras, and TorchScript) Loading a Neuropod The simplest way to load a neuropod is as follows: #include neuropod/neuropod.hh Neuropod neuropod ( PATH_TO_MY_MODEL ); where PATH_TO_MY_MODEL is the path to a model exported using one of the packagers. Options You can also provide runtime options when loading a model. To select what device to run the model on, you can specify the visible_device option: neuropod :: RuntimeOptions opts ; // Set the visible device to the first GPU opts . visible_device = Device :: GPU0 ; Neuropod neuropod ( PATH_TO_MY_MODEL , opts ); This defaults to GPU0 . If no GPUs are available, Neuropod will attempt to fallback to CPU. Setting opts.visible_device = Device::CPU will force the model to run on CPU. For more details, see all the options here Get the inputs and outputs of a model To get the inputs and outputs of a model, you can do this: Neuropod neuropod ( PATH_TO_MY_MODEL ); // Both of these are `std::vector TensorSpec ` const auto inputs = neuropod . get_inputs (); const auto outputs = neuropod . get_outputs (); for ( const auto item : inputs ) { // A `TensorSpec` is a struct with a `name`, `dims` and `type` std :: cout Tensor name: item . name std :: endl ; } For more details, see TensorSpec Tensor Types The following tensor types are supported: float double string int8 int16 int32 int64 uint8 uint16 uint32 uint64 Note uint16 , uint32 , and uint64 are not supported by PyTorch or TorchScript. See the supported type list in the PyTorch documentation. Note TorchScript does not have support for string tensors so we represent them as lists of strings. Therefore TorchScript Neuropod models only support 1D string \"tensors\". See here for example usage. Creating tensors Tip See the Efficient Tensor Creation page for a guide on the approach that best fits your use case. There are many different ways to create NeuropodTensor s, but all of them start with an allocator . To get an allocator for a loaded model, you can do something like: Neuropod neuropod ( PATH_TO_MY_MODEL ); auto allocator = neuropod . get_tensor_allocator (); For scenarios where a model isn't loaded (e.g. unit tests), you can use a generic tensor allocator: #include neuropod/core/generic_tensor.hh auto allocator = neuropod :: get_generic_tensor_allocator (); Allocate new memory For this, we just need the dimensions and type of the tensor we want to allocate. auto tensor = allocator - allocate_tensor float ({ 1 , 2 , 3 }); You can also manually specify the type without using a templated function auto tensor = allocator - allocate_tensor ({ 1 , 2 , 3 }, neuropod :: FLOAT_TENSOR ); To do something useful with these tensors, see the Interacting with Tensors section below. From existing memory Neuropod provides a way to wrap existing memory and use it in a zero-copy way. To do this, it needs four things: The dimension of the tensor to create The type of tensor to create A pointer to the data to wrap Note: this should be 64 byte aligned A deleter function This deleter function is called once Neuropod (and the underlying libraries) are done using this data. Until this function is called, it is not safe to deallocate the data. It is very important to pass in a correct deleter to make sure the memory doesn't get prematurely deallocated. Some examples are below. cv::Mat cv :: Mat image = ... // An image from somewhere auto tensor = allocator - tensor_from_memory uint8_t ( // Dimensions { 1 , image . rows , image . cols , image . channels ()}, // Data image . data , // Deleter [ image ]( void * unused ) { // By capturing `image` in this deleter, we ensure // that the underlying data does not get deallocated // before we re done with the tensor. } ); Tip You can also specify the type without using a templated function cv :: Mat image = ... // An image from somewhere auto tensor = allocator - tensor_from_memory ( // Dimensions { 1 , image . rows , image . cols , image . channels ()}, // Tensor Type get_tensor_type_from_cv_mat ( image ), // Data image . data , // Deleter [ image ]( void * unused ) {} ); Note Utilities for wrapping types from common libraries will be added in a future release. Eigen #include neuropod/conversions/eigen.hh auto tensor = allocator - allocate_tensor float ({ 1 , 2 , 3 }); // Returns an `Eigen::Map` auto eigen_map = neuropod :: as_eigen ( * tensor ); See the Eigen docs for more details. Note If you're not using the features of Eigen and just need simple element access, use accessors instead. Factory functions These functions are useful for creating test data. zeros Returns a tensor of type T and shape input_dims filled with zeros auto zeros = allocator - zeros T ( input_dims ); ones Returns a tensor of type T and shape input_dims filled with ones auto ones = allocator - ones T ( input_dims ); full Returns a tensor of type T and shape input_dims filled with fill_value auto full = allocator - full T ( input_dims , fill_value ); randn Returns a tensor of type T and shape input_dims filled with random numbers from a normal distribution with mean mean and standard deviation stddev . auto full = allocator - randn T ( input_dims , mean = 0 , stddev = 1 ); arange Returns a 1D tensor of type T containing a sequence of numbers starting at start with a step size of step . auto range1 = allocator - arange T ( end ); auto range2 = allocator - arange T ( start , end , step = 1 ); Example: // 0, 1, 2, 3, 4 auto range1 = allocator - arange float ( 5 ); // 2, 3, 4, 5 auto range2 = allocator - arange float ( 2 , 6 ); // 0, 2, 4, 6, 8 auto range3 = allocator - arange float ( 0 , 10 , 2 ); eye Returns an identity matrix of type T and shape ( M , N ). This matrix has ones on the diagonal and zeros everywhere else. auto eye1 = allocator - eye T ( M , N ); Example: // 1, 0, 0, 0, // 0, 1, 0, 0, // 0, 0, 1, 0, // 0, 0, 0, 1 auto eye1 = allocator - eye float ( 4 , 4 ); // 1, 0, 0, 0, 0, 0, 0, // 0, 1, 0, 0, 0, 0, 0, // 0, 0, 1, 0, 0, 0, 0 auto eye2 = allocator - eye float ( 3 , 7 ); Interacting with Tensors This section goes over various ways of interacting with existing tensors. Types of tensors Neuropod has a few different ways of representing tensors: NeuropodValue , NeuropodTensor , and TypedNeuropodTensor T NeuropodValue is the base type and represents any value that the library can store and pass around. A NeuropodTensor is a NeuropodValue that is a tensor. This adds metadata functionality (dimensions, type, num elements, etc.), but does not allow data access. A TypedNeuropodTensor T is a NeuropodTensor of a specific type. This level of the hierarchy adds type-safe data access. This is what the class hierarchy looks like: graph TD; NeuropodValue -- NeuropodTensor; NeuropodValue -.- non_tensor[Non-Tensor types]; NeuropodTensor -- TypedNeuropodTensor ltT gt; style non_tensor opacity:0.1,fill:#aaa,stroke:#333,stroke-width:4px To convert from a NeuropodValue to a NeuropodTensor , you can use as_tensor() . auto my_value = ... auto my_tensor = my_value - as_tensor (); To convert from a NeuropodValue or NeuropodTensor to a TypedNeuropodTensor of a specific type, you can use as_typed_tensor T () . This does a type-checked downcast to the requested type and throws an error if the requested type does not match the actual type of the tensor. auto my_value = ... auto my_float_tensor = my_value - as_typed_tensor float (); // This will throw an error auto my_uint_tensor = my_value - as_typed_tensor uint8_t (); The sections below will go over some more usage and examples. Note Most use cases will not require usage of these methods (as the factories and templated allocators already return TypedNeuropodTensor s). Tip Generally, data access requires a TypedNeuropodTensor T and metadata access requires at least a NeuropodTensor Copy data into a tensor Requires TypedNeuropodTensor T If you want to copy in data (and can't wrap it using the tensor_from_memory API above), you can do something like this: float * my_data = ...; size_t num_elements = ...; tensor - copy_from ( my_data , num_elements ); You can also copy data from a vector: std :: vector float my_data ; tensor - copy_from ( my_data ); Directly set/get data Requires TypedNeuropodTensor T You can do this using the accessor interface, which is very similar to PyTorch's accessor interface. auto tensor = allocator - allocate_tensor float ({ 6 , 6 }); // 2 is the number of dimensions of this tensor auto accessor = tensor - accessor 2 (); accessor [ 5 ][ 3 ] = 1.0 ; Range-based for loops work with accessors as well: auto tensor = allocator - allocate_tensor float ({ 3 , 5 }); // 2 is the number of dimensions of this tensor auto accessor = tensor - accessor 2 (); for ( const auto row : accessor ) { for ( const auto item : row ) { // Do something } } Example with string tensors: auto tensor = allocator - allocate_tensor std :: string ({ 3 , 5 }); // 2 is the number of dimensions of this tensor auto accessor = tensor - accessor 2 (); for ( int i = 0 ; i 3 ; i ++ ) { for ( int j = 0 ; j 5 ; j ++ ) { accessor [ i ][ j ] = std :: to_string ( i * 5 + j ); } } Individual element access with accessors is very efficient and comparable to raw pointer operations during an optimized build. Note See the Efficient Tensor Creation page for a guide on the approach that best fits your use case. Using Get the dimensions of a tensor Requires NeuropodTensor const auto dims = tensor - get_dims (); Get the number of elements in a tensor Requires NeuropodTensor auto num_elements = tensor - get_num_elements (); Get the type of a tensor Requires NeuropodTensor auto tensor_type = tensor - get_tensor_type (); Get a raw pointer to the underlying data Requires TypedNeuropodTensor T auto data = tensor - get_raw_data_ptr (); Note This method does not work for String tensors. Use accessors instead Get the data as a vector Requires TypedNeuropodTensor T auto data = tensor - get_data_as_vector (); Warning This method performs a copy Inference The basic inference method of a neuropod looks like this: std :: unique_ptr NeuropodValueMap infer ( const NeuropodValueMap inputs ); A NeuropodValueMap is just a map from std::string to std::shared_ptr NeuropodValue Interacting with it is identical to interacting with a std::unordered_map . For example: // Get an allocator auto alloctor = neuropod . get_tensor_allocator (); // Create some tensors auto x = allocator - randn float ({ 5 , 5 }); auto y = allocator - ones float ({ 5 , 5 }); // Run inference const auto output_data = neuropod . infer ({ { x , x }, { y , y } }); // Get the outputs auto z = output_data - at ( z ) - as_typed_tensor float (); You can also get a subset of the model's output by providing a list of requested outputs: std :: unique_ptr NeuropodValueMap infer ( const NeuropodValueMap inputs , const std :: vector std :: string requested_outputs ); For example, if you wanted to return a map containing only the tensor \"z\", you could do this: const auto output_data = neuropod . infer ( input_data , { z }); Serialization All built-in NeuropodValue types are serializable. Furthermore, NeuropodValueMap is also serializable. // A stream to serialize to. Any ostream is allowed, but we use a // stringstream in this example std :: stringstream ss ; neuropod :: NeuropodValueMap data = ...; neuropod :: serialize ( my_stream , data ); Similarly, deserializing is just as easy. auto deserialized = neuropod :: deserialize neuropod :: NeuropodValueMap ( ss , allocator ); Note Serialization and deserialization works across Python and C++. See the Python bindings docs for more Warning The goal for this API is to support transient serialization. There are no guarantees about backwards compatibility so this API should not be used for long term storage of data","title":"C++ Guide"},{"location":"cppguide/#c-guide","text":"This guide walks through the Neuropod C++ API in detail and goes over many different ways of working with models. Tip The Neuropod runtime interface is identical for all frameworks so this guide applies for models from all supported frameworks (including TensorFlow, PyTorch, Keras, and TorchScript)","title":"C++ Guide"},{"location":"cppguide/#loading-a-neuropod","text":"The simplest way to load a neuropod is as follows: #include neuropod/neuropod.hh Neuropod neuropod ( PATH_TO_MY_MODEL ); where PATH_TO_MY_MODEL is the path to a model exported using one of the packagers.","title":"Loading a Neuropod"},{"location":"cppguide/#options","text":"You can also provide runtime options when loading a model. To select what device to run the model on, you can specify the visible_device option: neuropod :: RuntimeOptions opts ; // Set the visible device to the first GPU opts . visible_device = Device :: GPU0 ; Neuropod neuropod ( PATH_TO_MY_MODEL , opts ); This defaults to GPU0 . If no GPUs are available, Neuropod will attempt to fallback to CPU. Setting opts.visible_device = Device::CPU will force the model to run on CPU. For more details, see all the options here","title":"Options"},{"location":"cppguide/#get-the-inputs-and-outputs-of-a-model","text":"To get the inputs and outputs of a model, you can do this: Neuropod neuropod ( PATH_TO_MY_MODEL ); // Both of these are `std::vector TensorSpec ` const auto inputs = neuropod . get_inputs (); const auto outputs = neuropod . get_outputs (); for ( const auto item : inputs ) { // A `TensorSpec` is a struct with a `name`, `dims` and `type` std :: cout Tensor name: item . name std :: endl ; } For more details, see TensorSpec","title":"Get the inputs and outputs of a model"},{"location":"cppguide/#tensor-types","text":"The following tensor types are supported: float double string int8 int16 int32 int64 uint8 uint16 uint32 uint64 Note uint16 , uint32 , and uint64 are not supported by PyTorch or TorchScript. See the supported type list in the PyTorch documentation. Note TorchScript does not have support for string tensors so we represent them as lists of strings. Therefore TorchScript Neuropod models only support 1D string \"tensors\". See here for example usage.","title":"Tensor Types"},{"location":"cppguide/#creating-tensors","text":"Tip See the Efficient Tensor Creation page for a guide on the approach that best fits your use case. There are many different ways to create NeuropodTensor s, but all of them start with an allocator . To get an allocator for a loaded model, you can do something like: Neuropod neuropod ( PATH_TO_MY_MODEL ); auto allocator = neuropod . get_tensor_allocator (); For scenarios where a model isn't loaded (e.g. unit tests), you can use a generic tensor allocator: #include neuropod/core/generic_tensor.hh auto allocator = neuropod :: get_generic_tensor_allocator ();","title":"Creating tensors"},{"location":"cppguide/#allocate-new-memory","text":"For this, we just need the dimensions and type of the tensor we want to allocate. auto tensor = allocator - allocate_tensor float ({ 1 , 2 , 3 }); You can also manually specify the type without using a templated function auto tensor = allocator - allocate_tensor ({ 1 , 2 , 3 }, neuropod :: FLOAT_TENSOR ); To do something useful with these tensors, see the Interacting with Tensors section below.","title":"Allocate new memory"},{"location":"cppguide/#from-existing-memory","text":"Neuropod provides a way to wrap existing memory and use it in a zero-copy way. To do this, it needs four things: The dimension of the tensor to create The type of tensor to create A pointer to the data to wrap Note: this should be 64 byte aligned A deleter function This deleter function is called once Neuropod (and the underlying libraries) are done using this data. Until this function is called, it is not safe to deallocate the data. It is very important to pass in a correct deleter to make sure the memory doesn't get prematurely deallocated. Some examples are below.","title":"From existing memory"},{"location":"cppguide/#cvmat","text":"cv :: Mat image = ... // An image from somewhere auto tensor = allocator - tensor_from_memory uint8_t ( // Dimensions { 1 , image . rows , image . cols , image . channels ()}, // Data image . data , // Deleter [ image ]( void * unused ) { // By capturing `image` in this deleter, we ensure // that the underlying data does not get deallocated // before we re done with the tensor. } ); Tip You can also specify the type without using a templated function cv :: Mat image = ... // An image from somewhere auto tensor = allocator - tensor_from_memory ( // Dimensions { 1 , image . rows , image . cols , image . channels ()}, // Tensor Type get_tensor_type_from_cv_mat ( image ), // Data image . data , // Deleter [ image ]( void * unused ) {} ); Note Utilities for wrapping types from common libraries will be added in a future release.","title":"cv::Mat"},{"location":"cppguide/#eigen","text":"#include neuropod/conversions/eigen.hh auto tensor = allocator - allocate_tensor float ({ 1 , 2 , 3 }); // Returns an `Eigen::Map` auto eigen_map = neuropod :: as_eigen ( * tensor ); See the Eigen docs for more details. Note If you're not using the features of Eigen and just need simple element access, use accessors instead.","title":"Eigen"},{"location":"cppguide/#factory-functions","text":"These functions are useful for creating test data.","title":"Factory functions"},{"location":"cppguide/#zeros","text":"Returns a tensor of type T and shape input_dims filled with zeros auto zeros = allocator - zeros T ( input_dims );","title":"zeros"},{"location":"cppguide/#ones","text":"Returns a tensor of type T and shape input_dims filled with ones auto ones = allocator - ones T ( input_dims );","title":"ones"},{"location":"cppguide/#full","text":"Returns a tensor of type T and shape input_dims filled with fill_value auto full = allocator - full T ( input_dims , fill_value );","title":"full"},{"location":"cppguide/#randn","text":"Returns a tensor of type T and shape input_dims filled with random numbers from a normal distribution with mean mean and standard deviation stddev . auto full = allocator - randn T ( input_dims , mean = 0 , stddev = 1 );","title":"randn"},{"location":"cppguide/#arange","text":"Returns a 1D tensor of type T containing a sequence of numbers starting at start with a step size of step . auto range1 = allocator - arange T ( end ); auto range2 = allocator - arange T ( start , end , step = 1 ); Example: // 0, 1, 2, 3, 4 auto range1 = allocator - arange float ( 5 ); // 2, 3, 4, 5 auto range2 = allocator - arange float ( 2 , 6 ); // 0, 2, 4, 6, 8 auto range3 = allocator - arange float ( 0 , 10 , 2 );","title":"arange"},{"location":"cppguide/#eye","text":"Returns an identity matrix of type T and shape ( M , N ). This matrix has ones on the diagonal and zeros everywhere else. auto eye1 = allocator - eye T ( M , N ); Example: // 1, 0, 0, 0, // 0, 1, 0, 0, // 0, 0, 1, 0, // 0, 0, 0, 1 auto eye1 = allocator - eye float ( 4 , 4 ); // 1, 0, 0, 0, 0, 0, 0, // 0, 1, 0, 0, 0, 0, 0, // 0, 0, 1, 0, 0, 0, 0 auto eye2 = allocator - eye float ( 3 , 7 );","title":"eye"},{"location":"cppguide/#interacting-with-tensors","text":"This section goes over various ways of interacting with existing tensors.","title":"Interacting with Tensors"},{"location":"cppguide/#types-of-tensors","text":"Neuropod has a few different ways of representing tensors: NeuropodValue , NeuropodTensor , and TypedNeuropodTensor T NeuropodValue is the base type and represents any value that the library can store and pass around. A NeuropodTensor is a NeuropodValue that is a tensor. This adds metadata functionality (dimensions, type, num elements, etc.), but does not allow data access. A TypedNeuropodTensor T is a NeuropodTensor of a specific type. This level of the hierarchy adds type-safe data access. This is what the class hierarchy looks like: graph TD; NeuropodValue -- NeuropodTensor; NeuropodValue -.- non_tensor[Non-Tensor types]; NeuropodTensor -- TypedNeuropodTensor ltT gt; style non_tensor opacity:0.1,fill:#aaa,stroke:#333,stroke-width:4px To convert from a NeuropodValue to a NeuropodTensor , you can use as_tensor() . auto my_value = ... auto my_tensor = my_value - as_tensor (); To convert from a NeuropodValue or NeuropodTensor to a TypedNeuropodTensor of a specific type, you can use as_typed_tensor T () . This does a type-checked downcast to the requested type and throws an error if the requested type does not match the actual type of the tensor. auto my_value = ... auto my_float_tensor = my_value - as_typed_tensor float (); // This will throw an error auto my_uint_tensor = my_value - as_typed_tensor uint8_t (); The sections below will go over some more usage and examples. Note Most use cases will not require usage of these methods (as the factories and templated allocators already return TypedNeuropodTensor s). Tip Generally, data access requires a TypedNeuropodTensor T and metadata access requires at least a NeuropodTensor","title":"Types of tensors"},{"location":"cppguide/#copy-data-into-a-tensor","text":"Requires TypedNeuropodTensor T If you want to copy in data (and can't wrap it using the tensor_from_memory API above), you can do something like this: float * my_data = ...; size_t num_elements = ...; tensor - copy_from ( my_data , num_elements ); You can also copy data from a vector: std :: vector float my_data ; tensor - copy_from ( my_data );","title":"Copy data into a tensor"},{"location":"cppguide/#directly-setget-data","text":"Requires TypedNeuropodTensor T You can do this using the accessor interface, which is very similar to PyTorch's accessor interface. auto tensor = allocator - allocate_tensor float ({ 6 , 6 }); // 2 is the number of dimensions of this tensor auto accessor = tensor - accessor 2 (); accessor [ 5 ][ 3 ] = 1.0 ; Range-based for loops work with accessors as well: auto tensor = allocator - allocate_tensor float ({ 3 , 5 }); // 2 is the number of dimensions of this tensor auto accessor = tensor - accessor 2 (); for ( const auto row : accessor ) { for ( const auto item : row ) { // Do something } } Example with string tensors: auto tensor = allocator - allocate_tensor std :: string ({ 3 , 5 }); // 2 is the number of dimensions of this tensor auto accessor = tensor - accessor 2 (); for ( int i = 0 ; i 3 ; i ++ ) { for ( int j = 0 ; j 5 ; j ++ ) { accessor [ i ][ j ] = std :: to_string ( i * 5 + j ); } } Individual element access with accessors is very efficient and comparable to raw pointer operations during an optimized build. Note See the Efficient Tensor Creation page for a guide on the approach that best fits your use case. Using","title":"Directly set/get data"},{"location":"cppguide/#get-the-dimensions-of-a-tensor","text":"Requires NeuropodTensor const auto dims = tensor - get_dims ();","title":"Get the dimensions of a tensor"},{"location":"cppguide/#get-the-number-of-elements-in-a-tensor","text":"Requires NeuropodTensor auto num_elements = tensor - get_num_elements ();","title":"Get the number of elements in a tensor"},{"location":"cppguide/#get-the-type-of-a-tensor","text":"Requires NeuropodTensor auto tensor_type = tensor - get_tensor_type ();","title":"Get the type of a tensor"},{"location":"cppguide/#get-a-raw-pointer-to-the-underlying-data","text":"Requires TypedNeuropodTensor T auto data = tensor - get_raw_data_ptr (); Note This method does not work for String tensors. Use accessors instead","title":"Get a raw pointer to the underlying data"},{"location":"cppguide/#get-the-data-as-a-vector","text":"Requires TypedNeuropodTensor T auto data = tensor - get_data_as_vector (); Warning This method performs a copy","title":"Get the data as a vector"},{"location":"cppguide/#inference","text":"The basic inference method of a neuropod looks like this: std :: unique_ptr NeuropodValueMap infer ( const NeuropodValueMap inputs ); A NeuropodValueMap is just a map from std::string to std::shared_ptr NeuropodValue Interacting with it is identical to interacting with a std::unordered_map . For example: // Get an allocator auto alloctor = neuropod . get_tensor_allocator (); // Create some tensors auto x = allocator - randn float ({ 5 , 5 }); auto y = allocator - ones float ({ 5 , 5 }); // Run inference const auto output_data = neuropod . infer ({ { x , x }, { y , y } }); // Get the outputs auto z = output_data - at ( z ) - as_typed_tensor float (); You can also get a subset of the model's output by providing a list of requested outputs: std :: unique_ptr NeuropodValueMap infer ( const NeuropodValueMap inputs , const std :: vector std :: string requested_outputs ); For example, if you wanted to return a map containing only the tensor \"z\", you could do this: const auto output_data = neuropod . infer ( input_data , { z });","title":"Inference"},{"location":"cppguide/#serialization","text":"All built-in NeuropodValue types are serializable. Furthermore, NeuropodValueMap is also serializable. // A stream to serialize to. Any ostream is allowed, but we use a // stringstream in this example std :: stringstream ss ; neuropod :: NeuropodValueMap data = ...; neuropod :: serialize ( my_stream , data ); Similarly, deserializing is just as easy. auto deserialized = neuropod :: deserialize neuropod :: NeuropodValueMap ( ss , allocator ); Note Serialization and deserialization works across Python and C++. See the Python bindings docs for more Warning The goal for this API is to support transient serialization. There are no guarantees about backwards compatibility so this API should not be used for long term storage of data","title":"Serialization"},{"location":"developing/","text":"Building Neuropod Neuropod uses Bazel as a build system. There are a few ways to build the project: Natively on Linux and Mac In Docker (preferred) Natively on Linux and Mac The following sets up a local environment for building and testing: # Install system dependencies (e.g. bazel) ./build/install_system_deps.sh # Install python dependencies (e.g. numpy) # Note: This creates a virtualenv for neuropod and installs all deps in it ./build/install_python_deps.sh After the above steps, you can run the following scripts: # Build ./build/build.sh # Run tests ./build/test.sh # Or ./build/test_gpu.sh to run all tests Note The above commands run all python code in the virtualenv created by install_python_deps.sh . You do not need to manually activate the virtualenv. In Docker (preferred) ./build/docker_build.sh Internally, this uses the build scripts mentioned above, but provides better isolation between builds. Also, compared to a naive docker build, this command preserves the bazel cache. This ensures that subsequent builds run as quickly as possible. Debugging/interactively building In order to debug and/or experiment, it may be useful to build interactively within Docker: # Will launch bash in a docker container containing all Neuropod dependencies ./build/docker_build.sh -i # Run these commands inside the container in order to build and test ./build/build.sh ./build/test.sh Tests Neuropod has a set of tests implemented in C++ and a set of tests implemented in Python. Test coverage is described below: Location Covers C++ Library Covers Python Library C++ Tests source/neuropod/tests/* x Python Tests source/python/neuropod/tests/test_* x x GPU-only Python Tests source/python/neuropod/tests/gpu_test_* x x The Python tests run against both the Python and C++ libraries by using python bindings. This means that many tests only need to be written in Python. C++ tests can have the following tags: gpu : Only run this test when running GPU tests requires_ld_library_path : Set the LD_LIBRARY_PATH and PATH environment variables so the backends and multiprocess worker are available. This is useful for tests that run a model using OPE. no_trace_logging : Don't set the log level to TRACE when running this test. This is useful to avoid lots of output when running benchmarks. CI Build Matrix Our build matrix is defined as all combinations of the following: Platform: Ubuntu 16.04 GPU (in Docker) - Buildkite Ubuntu 16.04 CPU (in Docker) - Buildkite Mac CPU (native) - Travis CI Framework versions (each row of the table): CUDA TF Torch Python 9.0 1.12.0 1.1.0 2.7 10.0 1.13.1 1.2.0 3.5 10.0 1.14.0 1.3.0 3.6 10.0 1.15.0 1.4.0 3.7 10.1 - 1.5.0 3.8 We also have the following ad-hoc builds: A lint + docs + static analysis build (Buildkite) A native Ubuntu 16.04 build (outside of docker) to make sure that we don't accidentally break workflows of users not using docker This is a total of 17 builds (3 * 5 + 2) running in CI The current build matrix is defined in build/ci_matrix.py Code coverage is run on all Buildkite Linux builds Lint and Static Analysis We run the following lint tools in CI: clang-format for C++ formatting Buildifier for Bazel BUILD and .bzl file formatting Black and Flake8 for Python lint To show all lint errors and warnings locally, you can run ./tools/lint.sh . To attempt to automatically fix any issues that can be automatically fixed, run ./tools/autofix.sh . We also run the following static analysis tools in CI: Infer for C++ static analysis clang-tidy for C++ lint (Not yet enabled. See here ) These tools tend to be fairly slow so we don't currently provide a way to run them locally. Future additions We're also planning on adding the following configurations to the build matrix: Configs: ASAN Contributing See the contributing guide here","title":"Developing Neuropod"},{"location":"developing/#building-neuropod","text":"Neuropod uses Bazel as a build system. There are a few ways to build the project: Natively on Linux and Mac In Docker (preferred)","title":"Building Neuropod"},{"location":"developing/#natively-on-linux-and-mac","text":"The following sets up a local environment for building and testing: # Install system dependencies (e.g. bazel) ./build/install_system_deps.sh # Install python dependencies (e.g. numpy) # Note: This creates a virtualenv for neuropod and installs all deps in it ./build/install_python_deps.sh After the above steps, you can run the following scripts: # Build ./build/build.sh # Run tests ./build/test.sh # Or ./build/test_gpu.sh to run all tests Note The above commands run all python code in the virtualenv created by install_python_deps.sh . You do not need to manually activate the virtualenv.","title":"Natively on Linux and Mac"},{"location":"developing/#in-docker-preferred","text":"./build/docker_build.sh Internally, this uses the build scripts mentioned above, but provides better isolation between builds. Also, compared to a naive docker build, this command preserves the bazel cache. This ensures that subsequent builds run as quickly as possible.","title":"In Docker (preferred)"},{"location":"developing/#debugginginteractively-building","text":"In order to debug and/or experiment, it may be useful to build interactively within Docker: # Will launch bash in a docker container containing all Neuropod dependencies ./build/docker_build.sh -i # Run these commands inside the container in order to build and test ./build/build.sh ./build/test.sh","title":"Debugging/interactively building"},{"location":"developing/#tests","text":"Neuropod has a set of tests implemented in C++ and a set of tests implemented in Python. Test coverage is described below: Location Covers C++ Library Covers Python Library C++ Tests source/neuropod/tests/* x Python Tests source/python/neuropod/tests/test_* x x GPU-only Python Tests source/python/neuropod/tests/gpu_test_* x x The Python tests run against both the Python and C++ libraries by using python bindings. This means that many tests only need to be written in Python. C++ tests can have the following tags: gpu : Only run this test when running GPU tests requires_ld_library_path : Set the LD_LIBRARY_PATH and PATH environment variables so the backends and multiprocess worker are available. This is useful for tests that run a model using OPE. no_trace_logging : Don't set the log level to TRACE when running this test. This is useful to avoid lots of output when running benchmarks.","title":"Tests"},{"location":"developing/#ci","text":"","title":"CI"},{"location":"developing/#build-matrix","text":"Our build matrix is defined as all combinations of the following: Platform: Ubuntu 16.04 GPU (in Docker) - Buildkite Ubuntu 16.04 CPU (in Docker) - Buildkite Mac CPU (native) - Travis CI Framework versions (each row of the table): CUDA TF Torch Python 9.0 1.12.0 1.1.0 2.7 10.0 1.13.1 1.2.0 3.5 10.0 1.14.0 1.3.0 3.6 10.0 1.15.0 1.4.0 3.7 10.1 - 1.5.0 3.8 We also have the following ad-hoc builds: A lint + docs + static analysis build (Buildkite) A native Ubuntu 16.04 build (outside of docker) to make sure that we don't accidentally break workflows of users not using docker This is a total of 17 builds (3 * 5 + 2) running in CI The current build matrix is defined in build/ci_matrix.py Code coverage is run on all Buildkite Linux builds","title":"Build Matrix"},{"location":"developing/#lint-and-static-analysis","text":"We run the following lint tools in CI: clang-format for C++ formatting Buildifier for Bazel BUILD and .bzl file formatting Black and Flake8 for Python lint To show all lint errors and warnings locally, you can run ./tools/lint.sh . To attempt to automatically fix any issues that can be automatically fixed, run ./tools/autofix.sh . We also run the following static analysis tools in CI: Infer for C++ static analysis clang-tidy for C++ lint (Not yet enabled. See here ) These tools tend to be fairly slow so we don't currently provide a way to run them locally.","title":"Lint and Static Analysis"},{"location":"developing/#future-additions","text":"We're also planning on adding the following configurations to the build matrix: Configs: ASAN","title":"Future additions"},{"location":"developing/#contributing","text":"See the contributing guide here","title":"Contributing"},{"location":"installing/","text":"Installing Neuropod Note Neuropod requires macOS or Linux. Python Neuropod can be installed using pip: pip install neuropod To run models, you must also install packages for \"backends\". These are fully self-contained packages that let Neuropod run models with specific versions of frameworks regardless of the version installed in your python environment. The following commands can be used to install the official backends: # Torch CPU pip install neuropod-backend-torchscript-1-1-0-cpu -f https://download.neuropod.ai/whl/stable.html pip install neuropod-backend-torchscript-1-2-0-cpu -f https://download.neuropod.ai/whl/stable.html pip install neuropod-backend-torchscript-1-3-0-cpu -f https://download.neuropod.ai/whl/stable.html pip install neuropod-backend-torchscript-1-4-0-cpu -f https://download.neuropod.ai/whl/stable.html # Torch GPU pip install neuropod-backend-torchscript-1-1-0-gpu-cuda-9-0 -f https://download.neuropod.ai/whl/stable.html pip install neuropod-backend-torchscript-1-2-0-gpu-cuda-10-0 -f https://download.neuropod.ai/whl/stable.html pip install neuropod-backend-torchscript-1-3-0-gpu-cuda-10-0 -f https://download.neuropod.ai/whl/stable.html pip install neuropod-backend-torchscript-1-4-0-gpu-cuda-10-0 -f https://download.neuropod.ai/whl/stable.html pip install neuropod-backend-torchscript-1-5-0-gpu-cuda-10-1 -f https://download.neuropod.ai/whl/stable.html # TF CPU pip install neuropod-backend-tensorflow-1-12-0-cpu -f https://download.neuropod.ai/whl/stable.html pip install neuropod-backend-tensorflow-1-13-1-cpu -f https://download.neuropod.ai/whl/stable.html pip install neuropod-backend-tensorflow-1-14-0-cpu -f https://download.neuropod.ai/whl/stable.html pip install neuropod-backend-tensorflow-1-15-0-cpu -f https://download.neuropod.ai/whl/stable.html # TF GPU pip install neuropod-backend-tensorflow-1-12-0-gpu-cuda-9-0 -f https://download.neuropod.ai/whl/stable.html pip install neuropod-backend-tensorflow-1-13-1-gpu-cuda-10-0 -f https://download.neuropod.ai/whl/stable.html pip install neuropod-backend-tensorflow-1-14-0-gpu-cuda-10-0 -f https://download.neuropod.ai/whl/stable.html pip install neuropod-backend-tensorflow-1-15-0-gpu-cuda-10-0 -f https://download.neuropod.ai/whl/stable.html # Python pip install neuropod-backend-python-27 -f https://download.neuropod.ai/whl/stable.html pip install neuropod-backend-python-35 -f https://download.neuropod.ai/whl/stable.html pip install neuropod-backend-python-36 -f https://download.neuropod.ai/whl/stable.html pip install neuropod-backend-python-37 -f https://download.neuropod.ai/whl/stable.html pip install neuropod-backend-python-38 -f https://download.neuropod.ai/whl/stable.html Multiple backends can be installed for a given framework and Neuropod will select the correct one when loading a model. An error will be thrown if none of the installed backends match the model's requirements. C++ Prebuilts can be downloaded from the releases page. The libneuropod-[os]-[version].tar.gz files contain the main Neuropod library. The rest of the tar files contain the backends. These can be installed by adding them to your library path or directly linking your application to them. Note For the C++ interface, currently, only one version for each framework can be installed at a time. This is temporary with a more stable installation process coming soon. See the basic introduction for more information on getting started.","title":"Installing"},{"location":"installing/#installing-neuropod","text":"Note Neuropod requires macOS or Linux.","title":"Installing Neuropod"},{"location":"installing/#python","text":"Neuropod can be installed using pip: pip install neuropod To run models, you must also install packages for \"backends\". These are fully self-contained packages that let Neuropod run models with specific versions of frameworks regardless of the version installed in your python environment. The following commands can be used to install the official backends: # Torch CPU pip install neuropod-backend-torchscript-1-1-0-cpu -f https://download.neuropod.ai/whl/stable.html pip install neuropod-backend-torchscript-1-2-0-cpu -f https://download.neuropod.ai/whl/stable.html pip install neuropod-backend-torchscript-1-3-0-cpu -f https://download.neuropod.ai/whl/stable.html pip install neuropod-backend-torchscript-1-4-0-cpu -f https://download.neuropod.ai/whl/stable.html # Torch GPU pip install neuropod-backend-torchscript-1-1-0-gpu-cuda-9-0 -f https://download.neuropod.ai/whl/stable.html pip install neuropod-backend-torchscript-1-2-0-gpu-cuda-10-0 -f https://download.neuropod.ai/whl/stable.html pip install neuropod-backend-torchscript-1-3-0-gpu-cuda-10-0 -f https://download.neuropod.ai/whl/stable.html pip install neuropod-backend-torchscript-1-4-0-gpu-cuda-10-0 -f https://download.neuropod.ai/whl/stable.html pip install neuropod-backend-torchscript-1-5-0-gpu-cuda-10-1 -f https://download.neuropod.ai/whl/stable.html # TF CPU pip install neuropod-backend-tensorflow-1-12-0-cpu -f https://download.neuropod.ai/whl/stable.html pip install neuropod-backend-tensorflow-1-13-1-cpu -f https://download.neuropod.ai/whl/stable.html pip install neuropod-backend-tensorflow-1-14-0-cpu -f https://download.neuropod.ai/whl/stable.html pip install neuropod-backend-tensorflow-1-15-0-cpu -f https://download.neuropod.ai/whl/stable.html # TF GPU pip install neuropod-backend-tensorflow-1-12-0-gpu-cuda-9-0 -f https://download.neuropod.ai/whl/stable.html pip install neuropod-backend-tensorflow-1-13-1-gpu-cuda-10-0 -f https://download.neuropod.ai/whl/stable.html pip install neuropod-backend-tensorflow-1-14-0-gpu-cuda-10-0 -f https://download.neuropod.ai/whl/stable.html pip install neuropod-backend-tensorflow-1-15-0-gpu-cuda-10-0 -f https://download.neuropod.ai/whl/stable.html # Python pip install neuropod-backend-python-27 -f https://download.neuropod.ai/whl/stable.html pip install neuropod-backend-python-35 -f https://download.neuropod.ai/whl/stable.html pip install neuropod-backend-python-36 -f https://download.neuropod.ai/whl/stable.html pip install neuropod-backend-python-37 -f https://download.neuropod.ai/whl/stable.html pip install neuropod-backend-python-38 -f https://download.neuropod.ai/whl/stable.html Multiple backends can be installed for a given framework and Neuropod will select the correct one when loading a model. An error will be thrown if none of the installed backends match the model's requirements.","title":"Python"},{"location":"installing/#c","text":"Prebuilts can be downloaded from the releases page. The libneuropod-[os]-[version].tar.gz files contain the main Neuropod library. The rest of the tar files contain the backends. These can be installed by adding them to your library path or directly linking your application to them. Note For the C++ interface, currently, only one version for each framework can be installed at a time. This is temporary with a more stable installation process coming soon. See the basic introduction for more information on getting started.","title":"C++"},{"location":"pyguide/","text":"Python Guide This guide walks through loading a Neuropod and running inference from Python Tip The Neuropod runtime interface is identical for all frameworks so this guide applies for models from all supported frameworks (including TensorFlow, PyTorch, Keras, and TorchScript) Packaging a Neuropod See the basic introduction guide for examples of how to create Neuropod models in all the supported frameworks. Loading a Neuropod from neuropod.loader import load_neuropod neuropod = load_neuropod ( PATH_TO_MY_MODEL ) You can also use load_neuropod as a context manager: from neuropod.loader import load_neuropod with load_neuropod ( PATH_TO_MY_MODEL ) as neuropod : # Do something here pass Options You can also provide runtime options when loading a model. To select what device to run the model on, you can supply a visible_gpu argument. This is the index of the GPU that this Neuropod should run on (if any). It can either be None or a nonnegative integer. Setting this to None will attempt to run this model on CPU. # Run on CPU neuropod = load_neuropod ( PATH_TO_MY_MODEL , visible_gpu = None ) # Run on the second GPU neuropod = load_neuropod ( PATH_TO_MY_MODEL , visible_gpu = 1 ) Get the inputs and outputs of a model The inputs and outputs of a model are available via the inputs and outputs property. with load_neuropod ( PATH_TO_MY_MODEL ) as neuropod : # This is a list of dicts containing the name , dtype , and shape # of the input print ( neuropod . inputs , neuropod . outputs ) Inference The infer method of a model is used to run inference. The input to this method is a dict mapping input names to values. This must match the input spec in the neuropod config for the loaded model. Note All the keys in this dict must be strings and all the values must be numpy arrays The output of infer is a dict mapping output names to values. This is checked to ensure that it matches the spec in the neuropod config for the loaded model. All the keys in this dict are strings and all the values are numpy arrays. x = np . array ([ 1 , 2 , 3 , 4 ]) y = np . array ([ 5 , 6 , 7 , 8 ]) with load_neuropod ( ADDITION_MODEL_PATH ) as neuropod : results = neuropod . infer ({ x : x , y : y }) # array([6, 8, 10, 12]) print results [ out ] Serialization import numpy as np from neuropod import neuropod_native # An array to serialize tensor = np . arange ( 5 ) # Convert a numpy array to a NeuropodTensor and serialize it serialized_bytes = neuropod_native . serialize ( tensor ) # Deserialize a string of bytes to a NeuropodTensor # (and return it as a numpy array) deserialized = neuropod_native . deserialize ( serialized_bytes ) # array([0, 1, 2, 3, 4]) print ( deserialized ) Under the hood, the serialization code converts between numpy arrays and C++ NeuropodTensor objects (in a zero-copy way). It then uses the C++ serialization functionality to serialize/deserialize. Note Serialization and deserialization works across Python and C++. This means you can serialize tensors in C++ and deserialize in Python or vice-versa Warning The goal for this API is to support transient serialization. There are no guarantees about backwards compatibility so this API should not be used for long term storage of data","title":"Python Guide"},{"location":"pyguide/#python-guide","text":"This guide walks through loading a Neuropod and running inference from Python Tip The Neuropod runtime interface is identical for all frameworks so this guide applies for models from all supported frameworks (including TensorFlow, PyTorch, Keras, and TorchScript)","title":"Python Guide"},{"location":"pyguide/#packaging-a-neuropod","text":"See the basic introduction guide for examples of how to create Neuropod models in all the supported frameworks.","title":"Packaging a Neuropod"},{"location":"pyguide/#loading-a-neuropod","text":"from neuropod.loader import load_neuropod neuropod = load_neuropod ( PATH_TO_MY_MODEL ) You can also use load_neuropod as a context manager: from neuropod.loader import load_neuropod with load_neuropod ( PATH_TO_MY_MODEL ) as neuropod : # Do something here pass","title":"Loading a Neuropod"},{"location":"pyguide/#options","text":"You can also provide runtime options when loading a model. To select what device to run the model on, you can supply a visible_gpu argument. This is the index of the GPU that this Neuropod should run on (if any). It can either be None or a nonnegative integer. Setting this to None will attempt to run this model on CPU. # Run on CPU neuropod = load_neuropod ( PATH_TO_MY_MODEL , visible_gpu = None ) # Run on the second GPU neuropod = load_neuropod ( PATH_TO_MY_MODEL , visible_gpu = 1 )","title":"Options"},{"location":"pyguide/#get-the-inputs-and-outputs-of-a-model","text":"The inputs and outputs of a model are available via the inputs and outputs property. with load_neuropod ( PATH_TO_MY_MODEL ) as neuropod : # This is a list of dicts containing the name , dtype , and shape # of the input print ( neuropod . inputs , neuropod . outputs )","title":"Get the inputs and outputs of a model"},{"location":"pyguide/#inference","text":"The infer method of a model is used to run inference. The input to this method is a dict mapping input names to values. This must match the input spec in the neuropod config for the loaded model. Note All the keys in this dict must be strings and all the values must be numpy arrays The output of infer is a dict mapping output names to values. This is checked to ensure that it matches the spec in the neuropod config for the loaded model. All the keys in this dict are strings and all the values are numpy arrays. x = np . array ([ 1 , 2 , 3 , 4 ]) y = np . array ([ 5 , 6 , 7 , 8 ]) with load_neuropod ( ADDITION_MODEL_PATH ) as neuropod : results = neuropod . infer ({ x : x , y : y }) # array([6, 8, 10, 12]) print results [ out ]","title":"Inference"},{"location":"pyguide/#serialization","text":"import numpy as np from neuropod import neuropod_native # An array to serialize tensor = np . arange ( 5 ) # Convert a numpy array to a NeuropodTensor and serialize it serialized_bytes = neuropod_native . serialize ( tensor ) # Deserialize a string of bytes to a NeuropodTensor # (and return it as a numpy array) deserialized = neuropod_native . deserialize ( serialized_bytes ) # array([0, 1, 2, 3, 4]) print ( deserialized ) Under the hood, the serialization code converts between numpy arrays and C++ NeuropodTensor objects (in a zero-copy way). It then uses the C++ serialization functionality to serialize/deserialize. Note Serialization and deserialization works across Python and C++. This means you can serialize tensors in C++ and deserialize in Python or vice-versa Warning The goal for this API is to support transient serialization. There are no guarantees about backwards compatibility so this API should not be used for long term storage of data","title":"Serialization"},{"location":"tutorial/","text":"Neuropod Tutorial In this tutorial, we\u2019re going to build a simple Neuropod model for addition in TensorFlow, PyTorch, and TorchScript. We'll also show how to run inference from Python and C++. Almost all of the examples/code in this tutorial come from the Neuropod unit and integration tests. Please read through them for complete working examples. The Neuropod packaging and inference interfaces also have comprehensive docstrings and provide a more detailed usage of the API than this tutorial. Make sure to follow the instructions for installing Neuropod before continuing Package a Model The first step for packaging a model is to define a \u201cproblem\u201d (e.g. 2d object detection). A \u201cproblem\u201d is composed of 4 things: an input_spec A list of dicts specifying the name, datatype, and shape of an input tensor an output_spec A list of dicts specifying the name, datatype, and shape of an output tensor test_input_data (optional) If provided, Neuropod will run inference immediately after packaging to verify that the model was packaged correctly. Must be provided if test_output_data is provided test_output_data (optional) If provided, Neuropod will test that the output of inference with test_input_data matches test_output_data The shape of a tensor can include None in which case any value is acceptable. You can also use \u201csymbols\u201d in these shape definitions. Every instance of that symbol must resolve to the same value at runtime. For example, here\u2019s a problem definition for our addition model: INPUT_SPEC = [ # A one dimensional tensor of any size with dtype float32 { name : x , dtype : float32 , shape : ( num_inputs ,)}, # A one dimensional tensor of the same size with dtype float32 { name : y , dtype : float32 , shape : ( num_inputs ,)}, ] OUTPUT_SPEC = [ # The sum of the two tensors { name : out , dtype : float32 , shape : ( None ,)}, ] TEST_INPUT_DATA = { x : np . arange ( 5 , dtype = np . float32 ), y : np . arange ( 5 , dtype = np . float32 ), } TEST_EXPECTED_OUT = { out : np . arange ( 5 ) + np . arange ( 5 ) } The symbol num_inputs in the shapes of x and y must resolve to the same value at runtime. For a definition of a \u201creal\u201d problem, see the example problem definitions section in the appendix. Now that we have a problem defined, we\u2019re going to see how to package a model in each of the currently supported DL frameworks. TensorFlow There are two ways to package a TensorFlow model. One is with a GraphDef the other is with a path to a frozen graph. Both of these require a node_name_mapping that maps a tensor name in the problem definition (see above) to a node in the TensorFlow graph. See the examples below for more detail. GraphDef If we have a function that returns a GraphDef like: import tensorflow as tf def create_tf_addition_model (): A simple addition model g = tf . Graph () with g . as_default (): with tf . name_scope ( some_namespace ): x = tf . placeholder ( tf . float32 , name = in_x ) y = tf . placeholder ( tf . float32 , name = in_y ) out = tf . add ( x , y , name = out ) return g . as_graph_def () we can package the model as follows: from neuropod.packagers import create_tensorflow_neuropod create_tensorflow_neuropod ( neuropod_path = neuropod_path , model_name = addition_model , graph_def = create_tf_addition_model (), node_name_mapping = { x : some_namespace/in_x:0 , y : some_namespace/in_y:0 , out : some_namespace/out:0 , }, input_spec = addition_problem_definition . INPUT_SPEC , output_spec = addition_problem_definition . OUTPUT_SPEC , test_input_data = addition_problem_definition . TEST_INPUT_DATA , test_expected_out = addition_problem_definition . TEST_EXPECTED_OUT , ) Note create_tensorflow_neuropod runs inference with the test data immediately after creating the neuropod. Raises a ValueError if the model output does not match the expected output. Path to a Frozen Graph If you already have a frozen graph, you can package the model like this: from neuropod.packagers import create_tensorflow_neuropod create_tensorflow_neuropod ( neuropod_path = neuropod_path , model_name = addition_model , frozen_graph_path = /path/to/my/frozen.graph , node_name_mapping = { x : some_namespace/in_x:0 , y : some_namespace/in_y:0 , out : some_namespace/out:0 , }, input_spec = addition_problem_definition . INPUT_SPEC , output_spec = addition_problem_definition . OUTPUT_SPEC , test_input_data = addition_problem_definition . TEST_INPUT_DATA , test_expected_out = addition_problem_definition . TEST_EXPECTED_OUT , ) Note create_tensorflow_neuropod runs inference with the test data immediately after creating the neuropod. Raises a ValueError if the model output does not match the expected output. PyTorch Tip Packaging a PyTorch model is a bit more complicated because you need python code and the weights in order to run the network. Converting your model to TorchScript is recommended if possible. In order to create a PyTorch neuropod package, we need to follow a few guidelines: Absolute imports (e.g. import torch ) are okay as long as your runtime environment has the package installed For Python 3, all other imports within your package must be relative This type of neuropod package is a bit less flexible than TensorFlow/TorchScript/Keras packages because absolute imports introduce a dependency on the runtime environment. This will be improved in a future release. Let's say our addition model looks like this (and is stored at /my/model/code/dir/main.py ): import torch import torch.nn as nn class AdditionModel ( nn . Module ): def forward ( self , x , y ): return { out : x + y } def get_model ( data_root ): return AdditionModel () In order to package it, we need 4 things: The paths to any data we want to store (e.g. the model weights) The path to the python_root of the code along with relative paths for any dirs within the python_root we want to package An entrypoint function that returns a model given a path to the packaged data. See the docstring for create_pytorch_neuropod for more details and examples. The dependencies of our model. These should be python packages. Tip See the API docs for create_pytorch_neuropod for detailed descriptions of every parameter For our model: We don't need to store any data (because our model has no weights) Our python root is /my/model/code/dir and we want to store all the code in it Our entrypoint function is get_model and our entrypoint_package is main (because the code is in main.py in the python root) This translates to the following: from neuropod.packagers import create_pytorch_neuropod create_pytorch_neuropod ( neuropod_path = neuropod_path , model_name = addition_model , data_paths = [], code_path_spec = [{ python_root : /my/model/code/dir , dirs_to_package : [ # Package everything in the python_root ], }], entrypoint_package = main , entrypoint = get_model , input_spec = addition_problem_definition . INPUT_SPEC , output_spec = addition_problem_definition . OUTPUT_SPEC , test_input_data = addition_problem_definition . TEST_INPUT_DATA , test_expected_out = addition_problem_definition . TEST_EXPECTED_OUT , ) Note create_pytorch_neuropod runs inference with the test data immediately after creating the neuropod. Raises a ValueError if the model output does not match the expected output. TorchScript TorchScript is much easier to package than PyTorch (since we don't need to store any python code). If we have an addition model that looks like: import torch class AdditionModel ( torch . jit . ScriptModule ): A simple addition model @torch . jit . script_method def forward ( self , x , y ): return { out : x + y } We can package it by running: from neuropod.packagers import create_torchscript_neuropod create_torchscript_neuropod ( neuropod_path = neuropod_path , model_name = addition_model , module = AdditionModel (), input_spec = addition_problem_definition . INPUT_SPEC , output_spec = addition_problem_definition . OUTPUT_SPEC , test_input_data = addition_problem_definition . TEST_INPUT_DATA , test_expected_out = addition_problem_definition . TEST_EXPECTED_OUT , ) Note create_torchscript_neuropod runs inference with the test data immediately after creating the neuropod. Raises a ValueError if the model output does not match the expected output. Keras If we have a Keras addition model that looks like: def create_keras_addition_model (): A simple addition model x = Input ( batch_shape = ( None ,), name = x ) y = Input ( batch_shape = ( None ,), name = y ) out = Add ( name = out )([ x , y ]) model = Model ( inputs = [ x , y ], outputs = [ out ]) return model We can package it by running: from neuropod.packagers import create_keras_neuropod create_keras_neuropod ( neuropod_path = neuropod_path , model_name = addition_model , sess = tf . keras . backend . get_session (), model = create_keras_addition_model (), input_spec = addition_problem_definition . INPUT_SPEC , output_spec = addition_problem_definition . OUTPUT_SPEC , test_input_data = addition_problem_definition . TEST_INPUT_DATA , test_expected_out = addition_problem_definition . TEST_EXPECTED_OUT , ) Note create_keras_neuropod runs inference with the test data immediately after creating the neuropod. Raises a ValueError if the model output does not match the expected output. Python Packaging aribtrary Python code has the same interface as packaging PyTorch above. For an example, see the PyTorch section above and use create_python_neuropod instead of create_pytorch_neuropod Run Inference Inference is the exact same no matter what the underlying DL framework is From Python x = np . array ([ 1 , 2 , 3 , 4 ]) y = np . array ([ 5 , 6 , 7 , 8 ]) with load_neuropod ( ADDITION_MODEL_PATH ) as neuropod : results = neuropod . infer ({ x : x , y : y }) # array([6, 8, 10, 12]) print results [ out ] From C++ #include neuropod/neuropod.hh int main () { const std :: vector int64_t shape = { 4 }; // To show two different ways of adding data, one of our inputs is an array // and the other is a vector. const float [] x_data = { 1 , 2 , 3 , 4 }; const std :: vector float y_data = { 5 , 6 , 7 , 8 }; // Load the neuropod Neuropod neuropod ( ADDITION_MODEL_PATH ); // Add the input data using two different signatures of `copy_from` // (one with a pointer and size, one with a vector) auto x_tensor = neuropod . allocate_tensor float ( shape ); x_tensor - copy_from ( x_data , 4 ); auto y_tensor = neuropod . allocate_tensor float ( shape ); y_tensor - copy_from ( y_data ); // Run inference const auto output_data = neuropod . infer ({ { x , x_tensor }, { y , y_tensor } }); const auto out_tensor = output_data - at ( out ); // {6, 8, 10, 12} const auto out_vector = out_tensor - as_typed_tensor float () - get_data_as_vector (); // {4} const auto out_shape = out_tensor - get_dims (); } Note This shows basic usage of the C++ API. For more flexible and memory-efficient usage, please see the C++ API docs Appendix Example Problem Definitions The problem definition for 2d object detection may look something like this: INPUT_SPEC = [ # BGR image { name : image , dtype : uint8 , shape : ( 1200 , 1920 , 3 )}, ] OUTPUT_SPEC = [ # shape: (num_detections, 4): (xmin, ymin, xmax, ymax) # These values are in units of pixels. The origin is the top left corner # with positive X to the right and positive Y towards the bottom of the image { name : boxes , dtype : float32 , shape : ( num_detections , 4 )}, # The list of classes that the network can output # This must be some subset of [ vehicle , person , motorcycle , bicycle ] { name : supported_object_classes , dtype : string , shape : ( num_classes ,)}, # The probability of each class for each detection # These should all be floats between 0 and 1 { name : object_class_probability , dtype : float32 , shape : ( num_detections , num_classes )}, ]","title":"Basic Introduction"},{"location":"tutorial/#neuropod-tutorial","text":"In this tutorial, we\u2019re going to build a simple Neuropod model for addition in TensorFlow, PyTorch, and TorchScript. We'll also show how to run inference from Python and C++. Almost all of the examples/code in this tutorial come from the Neuropod unit and integration tests. Please read through them for complete working examples. The Neuropod packaging and inference interfaces also have comprehensive docstrings and provide a more detailed usage of the API than this tutorial. Make sure to follow the instructions for installing Neuropod before continuing","title":"Neuropod Tutorial"},{"location":"tutorial/#package-a-model","text":"The first step for packaging a model is to define a \u201cproblem\u201d (e.g. 2d object detection). A \u201cproblem\u201d is composed of 4 things: an input_spec A list of dicts specifying the name, datatype, and shape of an input tensor an output_spec A list of dicts specifying the name, datatype, and shape of an output tensor test_input_data (optional) If provided, Neuropod will run inference immediately after packaging to verify that the model was packaged correctly. Must be provided if test_output_data is provided test_output_data (optional) If provided, Neuropod will test that the output of inference with test_input_data matches test_output_data The shape of a tensor can include None in which case any value is acceptable. You can also use \u201csymbols\u201d in these shape definitions. Every instance of that symbol must resolve to the same value at runtime. For example, here\u2019s a problem definition for our addition model: INPUT_SPEC = [ # A one dimensional tensor of any size with dtype float32 { name : x , dtype : float32 , shape : ( num_inputs ,)}, # A one dimensional tensor of the same size with dtype float32 { name : y , dtype : float32 , shape : ( num_inputs ,)}, ] OUTPUT_SPEC = [ # The sum of the two tensors { name : out , dtype : float32 , shape : ( None ,)}, ] TEST_INPUT_DATA = { x : np . arange ( 5 , dtype = np . float32 ), y : np . arange ( 5 , dtype = np . float32 ), } TEST_EXPECTED_OUT = { out : np . arange ( 5 ) + np . arange ( 5 ) } The symbol num_inputs in the shapes of x and y must resolve to the same value at runtime. For a definition of a \u201creal\u201d problem, see the example problem definitions section in the appendix. Now that we have a problem defined, we\u2019re going to see how to package a model in each of the currently supported DL frameworks.","title":"Package a Model"},{"location":"tutorial/#tensorflow","text":"There are two ways to package a TensorFlow model. One is with a GraphDef the other is with a path to a frozen graph. Both of these require a node_name_mapping that maps a tensor name in the problem definition (see above) to a node in the TensorFlow graph. See the examples below for more detail.","title":"TensorFlow"},{"location":"tutorial/#graphdef","text":"If we have a function that returns a GraphDef like: import tensorflow as tf def create_tf_addition_model (): A simple addition model g = tf . Graph () with g . as_default (): with tf . name_scope ( some_namespace ): x = tf . placeholder ( tf . float32 , name = in_x ) y = tf . placeholder ( tf . float32 , name = in_y ) out = tf . add ( x , y , name = out ) return g . as_graph_def () we can package the model as follows: from neuropod.packagers import create_tensorflow_neuropod create_tensorflow_neuropod ( neuropod_path = neuropod_path , model_name = addition_model , graph_def = create_tf_addition_model (), node_name_mapping = { x : some_namespace/in_x:0 , y : some_namespace/in_y:0 , out : some_namespace/out:0 , }, input_spec = addition_problem_definition . INPUT_SPEC , output_spec = addition_problem_definition . OUTPUT_SPEC , test_input_data = addition_problem_definition . TEST_INPUT_DATA , test_expected_out = addition_problem_definition . TEST_EXPECTED_OUT , ) Note create_tensorflow_neuropod runs inference with the test data immediately after creating the neuropod. Raises a ValueError if the model output does not match the expected output.","title":"GraphDef"},{"location":"tutorial/#path-to-a-frozen-graph","text":"If you already have a frozen graph, you can package the model like this: from neuropod.packagers import create_tensorflow_neuropod create_tensorflow_neuropod ( neuropod_path = neuropod_path , model_name = addition_model , frozen_graph_path = /path/to/my/frozen.graph , node_name_mapping = { x : some_namespace/in_x:0 , y : some_namespace/in_y:0 , out : some_namespace/out:0 , }, input_spec = addition_problem_definition . INPUT_SPEC , output_spec = addition_problem_definition . OUTPUT_SPEC , test_input_data = addition_problem_definition . TEST_INPUT_DATA , test_expected_out = addition_problem_definition . TEST_EXPECTED_OUT , ) Note create_tensorflow_neuropod runs inference with the test data immediately after creating the neuropod. Raises a ValueError if the model output does not match the expected output.","title":"Path to a Frozen Graph"},{"location":"tutorial/#pytorch","text":"Tip Packaging a PyTorch model is a bit more complicated because you need python code and the weights in order to run the network. Converting your model to TorchScript is recommended if possible. In order to create a PyTorch neuropod package, we need to follow a few guidelines: Absolute imports (e.g. import torch ) are okay as long as your runtime environment has the package installed For Python 3, all other imports within your package must be relative This type of neuropod package is a bit less flexible than TensorFlow/TorchScript/Keras packages because absolute imports introduce a dependency on the runtime environment. This will be improved in a future release. Let's say our addition model looks like this (and is stored at /my/model/code/dir/main.py ): import torch import torch.nn as nn class AdditionModel ( nn . Module ): def forward ( self , x , y ): return { out : x + y } def get_model ( data_root ): return AdditionModel () In order to package it, we need 4 things: The paths to any data we want to store (e.g. the model weights) The path to the python_root of the code along with relative paths for any dirs within the python_root we want to package An entrypoint function that returns a model given a path to the packaged data. See the docstring for create_pytorch_neuropod for more details and examples. The dependencies of our model. These should be python packages. Tip See the API docs for create_pytorch_neuropod for detailed descriptions of every parameter For our model: We don't need to store any data (because our model has no weights) Our python root is /my/model/code/dir and we want to store all the code in it Our entrypoint function is get_model and our entrypoint_package is main (because the code is in main.py in the python root) This translates to the following: from neuropod.packagers import create_pytorch_neuropod create_pytorch_neuropod ( neuropod_path = neuropod_path , model_name = addition_model , data_paths = [], code_path_spec = [{ python_root : /my/model/code/dir , dirs_to_package : [ # Package everything in the python_root ], }], entrypoint_package = main , entrypoint = get_model , input_spec = addition_problem_definition . INPUT_SPEC , output_spec = addition_problem_definition . OUTPUT_SPEC , test_input_data = addition_problem_definition . TEST_INPUT_DATA , test_expected_out = addition_problem_definition . TEST_EXPECTED_OUT , ) Note create_pytorch_neuropod runs inference with the test data immediately after creating the neuropod. Raises a ValueError if the model output does not match the expected output.","title":"PyTorch"},{"location":"tutorial/#torchscript","text":"TorchScript is much easier to package than PyTorch (since we don't need to store any python code). If we have an addition model that looks like: import torch class AdditionModel ( torch . jit . ScriptModule ): A simple addition model @torch . jit . script_method def forward ( self , x , y ): return { out : x + y } We can package it by running: from neuropod.packagers import create_torchscript_neuropod create_torchscript_neuropod ( neuropod_path = neuropod_path , model_name = addition_model , module = AdditionModel (), input_spec = addition_problem_definition . INPUT_SPEC , output_spec = addition_problem_definition . OUTPUT_SPEC , test_input_data = addition_problem_definition . TEST_INPUT_DATA , test_expected_out = addition_problem_definition . TEST_EXPECTED_OUT , ) Note create_torchscript_neuropod runs inference with the test data immediately after creating the neuropod. Raises a ValueError if the model output does not match the expected output.","title":"TorchScript"},{"location":"tutorial/#keras","text":"If we have a Keras addition model that looks like: def create_keras_addition_model (): A simple addition model x = Input ( batch_shape = ( None ,), name = x ) y = Input ( batch_shape = ( None ,), name = y ) out = Add ( name = out )([ x , y ]) model = Model ( inputs = [ x , y ], outputs = [ out ]) return model We can package it by running: from neuropod.packagers import create_keras_neuropod create_keras_neuropod ( neuropod_path = neuropod_path , model_name = addition_model , sess = tf . keras . backend . get_session (), model = create_keras_addition_model (), input_spec = addition_problem_definition . INPUT_SPEC , output_spec = addition_problem_definition . OUTPUT_SPEC , test_input_data = addition_problem_definition . TEST_INPUT_DATA , test_expected_out = addition_problem_definition . TEST_EXPECTED_OUT , ) Note create_keras_neuropod runs inference with the test data immediately after creating the neuropod. Raises a ValueError if the model output does not match the expected output.","title":"Keras"},{"location":"tutorial/#python","text":"Packaging aribtrary Python code has the same interface as packaging PyTorch above. For an example, see the PyTorch section above and use create_python_neuropod instead of create_pytorch_neuropod","title":"Python"},{"location":"tutorial/#run-inference","text":"Inference is the exact same no matter what the underlying DL framework is","title":"Run Inference"},{"location":"tutorial/#from-python","text":"x = np . array ([ 1 , 2 , 3 , 4 ]) y = np . array ([ 5 , 6 , 7 , 8 ]) with load_neuropod ( ADDITION_MODEL_PATH ) as neuropod : results = neuropod . infer ({ x : x , y : y }) # array([6, 8, 10, 12]) print results [ out ]","title":"From Python"},{"location":"tutorial/#from-c","text":"#include neuropod/neuropod.hh int main () { const std :: vector int64_t shape = { 4 }; // To show two different ways of adding data, one of our inputs is an array // and the other is a vector. const float [] x_data = { 1 , 2 , 3 , 4 }; const std :: vector float y_data = { 5 , 6 , 7 , 8 }; // Load the neuropod Neuropod neuropod ( ADDITION_MODEL_PATH ); // Add the input data using two different signatures of `copy_from` // (one with a pointer and size, one with a vector) auto x_tensor = neuropod . allocate_tensor float ( shape ); x_tensor - copy_from ( x_data , 4 ); auto y_tensor = neuropod . allocate_tensor float ( shape ); y_tensor - copy_from ( y_data ); // Run inference const auto output_data = neuropod . infer ({ { x , x_tensor }, { y , y_tensor } }); const auto out_tensor = output_data - at ( out ); // {6, 8, 10, 12} const auto out_vector = out_tensor - as_typed_tensor float () - get_data_as_vector (); // {4} const auto out_shape = out_tensor - get_dims (); } Note This shows basic usage of the C++ API. For more flexible and memory-efficient usage, please see the C++ API docs","title":"From C++"},{"location":"tutorial/#appendix","text":"","title":"Appendix"},{"location":"tutorial/#example-problem-definitions","text":"The problem definition for 2d object detection may look something like this: INPUT_SPEC = [ # BGR image { name : image , dtype : uint8 , shape : ( 1200 , 1920 , 3 )}, ] OUTPUT_SPEC = [ # shape: (num_detections, 4): (xmin, ymin, xmax, ymax) # These values are in units of pixels. The origin is the top left corner # with positive X to the right and positive Y towards the bottom of the image { name : boxes , dtype : float32 , shape : ( num_detections , 4 )}, # The list of classes that the network can output # This must be some subset of [ vehicle , person , motorcycle , bicycle ] { name : supported_object_classes , dtype : string , shape : ( num_classes ,)}, # The probability of each class for each detection # These should all be floats between 0 and 1 { name : object_class_probability , dtype : float32 , shape : ( num_detections , num_classes )}, ]","title":"Example Problem Definitions"},{"location":"advanced/efficient_tensor_creation/","text":"There are a handful of ways of creating tensors from exisiting data in C++ and they all have different tradeoffs between simplicity and performance. This document goes over some approaches and their tradeoffs. Tip Make sure to read the C++ guide before continuing Writing your data directly into a tensor This is preferable if you can do it. Instead of copying data or wrapping existing data, write your data directly into a tensor. Pros This'll work without copies under the hood for both in-process and out-of-process execution Has no memory alignment requirements No need to work with deleters Cons It can require a lot of refactoring of an existing application in order to make this work well Examples You could receive data directly into a tensor's underlying buffer: // Allocate a tensor auto tensor = allocator - allocate_tensor float ({ 6 , 6 }); // Get a pointer to the underlying buffer auto data = tensor - get_raw_data_ptr (); // Some function that writes data directly into this buffer recv_message_into_buffer ( data ); Or you could manually fill in a tensor: // Allocate a tensor auto tensor = allocator - allocate_tensor float ({ 256 , 256 }); const auto dims = tensor - get_dims (); // Get an accessor auto accessor = tensor - accessor 2 (); // Write data directly into it for ( int i = 0 ; i dims [ 0 ]; i ++ ) { for ( int j = 0 ; j dims [ 1 ]; j ++ ) { accessor [ i ][ j ] = i * j ; } } You could even parallelize that with TBB: // Allocate a tensor auto tensor = allocator - allocate_tensor float ({ 256 , 256 }); const auto dims = tensor - get_dims (); // Get an accessor auto accessor = tensor - accessor 2 (); // Write data into the tensor in parallel tbb :: parallel_for ( // Parallelize in blocks of 16 by 16 tbb : blocked_range2d size_t ( 0 , dims [ 0 ], 16 , 0 , dims [ 1 ], 16 ), // Run this lambda in parallel for each block in the range above [ ]( const blocked_range2d size_t r ) { for ( size_t i = r . rows (). begin (); i != r . rows (). end (); i ++ ) { for ( size_t j = r . cols (). begin (); j != r . cols (). end (); j ++ ) { accessor [ i ][ j ] = i * j ; } } } ); Wrapping existing memory This works well if you already have your data in a buffer somewhere. Pros This'll work without copies during in-process execution Easy to do if you already have data Cons Need an understanding of what deleters are and how to use them correctly For efficient usage with TF, the data needs to be 64 byte aligned Note: this isn't a hard requirement, but TF may copy unaligned data under the hood Compared to #1, this makes an extra copy during out-of-process execution Examples Wrapping data from a cv::Mat : cv :: Mat image = ... // An image from somewhere auto tensor = allocator - tensor_from_memory uint8_t ( // Dimensions { 1 , image . rows , image . cols , image . channels ()}, // Data image . data , // Deleter [ image ]( void * unused ) { // By capturing `image` in this deleter, we ensure // that the underlying data does not get deallocated // before we re done with the tensor. } ); Copying data into a tensor Pros Very easy to do No memory alignment requirements No need to work with deleters Cons Always makes an extra copy during in-process execution Compared to #1, this makes an extra copy during out-of-process execution (although this copy is explicitly written by the user) Examples Copying from a cv::Mat : cv :: Mat image = ... // An image from somewhere auto tensor = allocator - allocate_tensor uint8_t ( // Dimensions { 1 , image . rows , image . cols , image . channels ()} ); // Copy data into the tensor tensor - copy_from ( image . data , tensor - get_num_elements ()); Which one should I use? In general, the order of approaches in terms of performance is the following: Writing data directly into a tensor Wrapping existing memory Copying data into a tensor That said, profiling is your friend. The tradeoff between simplicity and performance is also different for large vs small tensors since copies are cheaper for small tensors.","title":"Efficient Tensor Creation"},{"location":"advanced/efficient_tensor_creation/#writing-your-data-directly-into-a-tensor","text":"This is preferable if you can do it. Instead of copying data or wrapping existing data, write your data directly into a tensor.","title":"Writing your data directly into a tensor"},{"location":"advanced/efficient_tensor_creation/#pros","text":"This'll work without copies under the hood for both in-process and out-of-process execution Has no memory alignment requirements No need to work with deleters","title":"Pros"},{"location":"advanced/efficient_tensor_creation/#cons","text":"It can require a lot of refactoring of an existing application in order to make this work well","title":"Cons"},{"location":"advanced/efficient_tensor_creation/#examples","text":"You could receive data directly into a tensor's underlying buffer: // Allocate a tensor auto tensor = allocator - allocate_tensor float ({ 6 , 6 }); // Get a pointer to the underlying buffer auto data = tensor - get_raw_data_ptr (); // Some function that writes data directly into this buffer recv_message_into_buffer ( data ); Or you could manually fill in a tensor: // Allocate a tensor auto tensor = allocator - allocate_tensor float ({ 256 , 256 }); const auto dims = tensor - get_dims (); // Get an accessor auto accessor = tensor - accessor 2 (); // Write data directly into it for ( int i = 0 ; i dims [ 0 ]; i ++ ) { for ( int j = 0 ; j dims [ 1 ]; j ++ ) { accessor [ i ][ j ] = i * j ; } } You could even parallelize that with TBB: // Allocate a tensor auto tensor = allocator - allocate_tensor float ({ 256 , 256 }); const auto dims = tensor - get_dims (); // Get an accessor auto accessor = tensor - accessor 2 (); // Write data into the tensor in parallel tbb :: parallel_for ( // Parallelize in blocks of 16 by 16 tbb : blocked_range2d size_t ( 0 , dims [ 0 ], 16 , 0 , dims [ 1 ], 16 ), // Run this lambda in parallel for each block in the range above [ ]( const blocked_range2d size_t r ) { for ( size_t i = r . rows (). begin (); i != r . rows (). end (); i ++ ) { for ( size_t j = r . cols (). begin (); j != r . cols (). end (); j ++ ) { accessor [ i ][ j ] = i * j ; } } } );","title":"Examples"},{"location":"advanced/efficient_tensor_creation/#wrapping-existing-memory","text":"This works well if you already have your data in a buffer somewhere.","title":"Wrapping existing memory"},{"location":"advanced/efficient_tensor_creation/#pros_1","text":"This'll work without copies during in-process execution Easy to do if you already have data","title":"Pros"},{"location":"advanced/efficient_tensor_creation/#cons_1","text":"Need an understanding of what deleters are and how to use them correctly For efficient usage with TF, the data needs to be 64 byte aligned Note: this isn't a hard requirement, but TF may copy unaligned data under the hood Compared to #1, this makes an extra copy during out-of-process execution","title":"Cons"},{"location":"advanced/efficient_tensor_creation/#examples_1","text":"Wrapping data from a cv::Mat : cv :: Mat image = ... // An image from somewhere auto tensor = allocator - tensor_from_memory uint8_t ( // Dimensions { 1 , image . rows , image . cols , image . channels ()}, // Data image . data , // Deleter [ image ]( void * unused ) { // By capturing `image` in this deleter, we ensure // that the underlying data does not get deallocated // before we re done with the tensor. } );","title":"Examples"},{"location":"advanced/efficient_tensor_creation/#copying-data-into-a-tensor","text":"","title":"Copying data into a tensor"},{"location":"advanced/efficient_tensor_creation/#pros_2","text":"Very easy to do No memory alignment requirements No need to work with deleters","title":"Pros"},{"location":"advanced/efficient_tensor_creation/#cons_2","text":"Always makes an extra copy during in-process execution Compared to #1, this makes an extra copy during out-of-process execution (although this copy is explicitly written by the user)","title":"Cons"},{"location":"advanced/efficient_tensor_creation/#examples_2","text":"Copying from a cv::Mat : cv :: Mat image = ... // An image from somewhere auto tensor = allocator - allocate_tensor uint8_t ( // Dimensions { 1 , image . rows , image . cols , image . channels ()} ); // Copy data into the tensor tensor - copy_from ( image . data , tensor - get_num_elements ());","title":"Examples"},{"location":"advanced/efficient_tensor_creation/#which-one-should-i-use","text":"In general, the order of approaches in terms of performance is the following: Writing data directly into a tensor Wrapping existing memory Copying data into a tensor That said, profiling is your friend. The tradeoff between simplicity and performance is also different for large vs small tensors since copies are cheaper for small tensors.","title":"Which one should I use?"},{"location":"advanced/ope/","text":"Out-of-Process Execution Neuropod can run models in different processes using an optimized shared memory implementation with extremely low overhead (~100 to 500 microseconds). To run a model in another process, set the use_ope option when loading a model: neuropod :: RuntimeOptions opts ; opts . use_ope = true ; Neuropod model ( neuropod_path , opts ); Nothing else should need to change. There are many potential benefits of this approach: Run models that require different versions of Torch or TF from the same \"master\" process ( in progress ) Pin the worker process to a specific core to reduce variability in inference time ( in progress ) Isolate models from each other and from the rest of your program Avoid sharing the GIL between multiple python models in the same process The worker process can also be run in a docker container to provide even more isolation. For more details and options, see the OPEOptions struct inside RuntimeOptions .","title":"Out-of-process Execution"},{"location":"advanced/ope/#out-of-process-execution","text":"Neuropod can run models in different processes using an optimized shared memory implementation with extremely low overhead (~100 to 500 microseconds). To run a model in another process, set the use_ope option when loading a model: neuropod :: RuntimeOptions opts ; opts . use_ope = true ; Neuropod model ( neuropod_path , opts ); Nothing else should need to change. There are many potential benefits of this approach: Run models that require different versions of Torch or TF from the same \"master\" process ( in progress ) Pin the worker process to a specific core to reduce variability in inference time ( in progress ) Isolate models from each other and from the rest of your program Avoid sharing the GIL between multiple python models in the same process The worker process can also be run in a docker container to provide even more isolation. For more details and options, see the OPEOptions struct inside RuntimeOptions .","title":"Out-of-Process Execution"},{"location":"packagers/keras/","text":"create_keras_neuropod Packages a Keras model as a neuropod package. Currently, only the TensorFlow backend is supported. create_keras_neuropod( neuropod_path, model_name, sess, model, node_name_mapping = None, input_spec = None, output_spec = None, input_tensor_device = None, default_input_tensor_device = GPU, custom_ops = [], package_as_zip = True, test_input_data = None, test_expected_out = None, persist_test_data = True, ) Params: neuropod_path The output neuropod path model_name The name of the model sess A TensorFlow session containing weights (usually keras.backend.get_session() ). model A Keras model object. node_name_mapping default: None Optional mapping from a neuropod input/output name to a name of Keras input/output Example : { x : input_1 , out : fc1000 , } Defaults to using Keras input/output names as neuropod input/output names. input_spec default: None A list of dicts specifying the input to the model. For each input, if shape is set to None , no validation is done on the shape. If shape is a tuple, the dimensions of the input are validated against that tuple. A value of None for any of the dimensions means that dimension will not be checked. dtype can be any valid numpy datatype string. Example : [ { name : x , dtype : float32 , shape : (None,)}, { name : y , dtype : float32 , shape : (None,)}, ] output_spec default: None A list of dicts specifying the output of the model. See the documentation for the input_spec parameter for more details. Example : [ { name : out , dtype : float32 , shape : (None,)}, ] input_tensor_device default: None A dict mapping input tensor names to the device that the model expects them to be on. This can either be GPU or CPU . Any tensors in input_spec not specified in this mapping will use the default_input_tensor_device specified below. If a GPU is selected at inference time, Neuropod will move tensors to the appropriate devices before running the model. Otherwise, it will attempt to run the model on CPU and move all tensors (and the model) to CPU. See the docstring for load_neuropod for more info. Example : { x : GPU } default_input_tensor_device default: GPU The default device that input tensors are expected to be on. This can either be GPU or CPU . custom_ops default: [] A list of paths to custom op shared libraries to include in the packaged neuropod. Note: Including custom ops ties your neuropod to the specific platform (e.g. Mac, Linux) that the custom ops were built for. It is the user's responsibility to ensure that their custom ops are built for the correct platform. Example : [ /path/to/my/custom_op.so ] package_as_zip default: True Whether to package the neuropod as a single file or as a directory. test_input_data default: None Optional sample input data. This is a dict mapping input names to values. If this is provided, inference will be run in an isolated environment immediately after packaging to ensure that the neuropod was created successfully. Must be provided if test_expected_out is provided. Throws a ValueError if inference failed. Example : { x : np.arange(5), y : np.arange(5), } test_expected_out default: None Optional expected output. Throws a ValueError if the output of model inference does not match the expected output. Example : { out : np.arange(5) + np.arange(5) } persist_test_data default: True Optionally saves the test data within the packaged neuropod.","title":"Keras"},{"location":"packagers/keras/#create_keras_neuropod","text":"Packages a Keras model as a neuropod package. Currently, only the TensorFlow backend is supported. create_keras_neuropod( neuropod_path, model_name, sess, model, node_name_mapping = None, input_spec = None, output_spec = None, input_tensor_device = None, default_input_tensor_device = GPU, custom_ops = [], package_as_zip = True, test_input_data = None, test_expected_out = None, persist_test_data = True, )","title":"create_keras_neuropod"},{"location":"packagers/keras/#params","text":"","title":"Params:"},{"location":"packagers/keras/#neuropod_path","text":"The output neuropod path","title":"neuropod_path"},{"location":"packagers/keras/#model_name","text":"The name of the model","title":"model_name"},{"location":"packagers/keras/#sess","text":"A TensorFlow session containing weights (usually keras.backend.get_session() ).","title":"sess"},{"location":"packagers/keras/#model","text":"A Keras model object.","title":"model"},{"location":"packagers/keras/#node_name_mapping","text":"default: None Optional mapping from a neuropod input/output name to a name of Keras input/output Example : { x : input_1 , out : fc1000 , } Defaults to using Keras input/output names as neuropod input/output names.","title":"node_name_mapping"},{"location":"packagers/keras/#input_spec","text":"default: None A list of dicts specifying the input to the model. For each input, if shape is set to None , no validation is done on the shape. If shape is a tuple, the dimensions of the input are validated against that tuple. A value of None for any of the dimensions means that dimension will not be checked. dtype can be any valid numpy datatype string. Example : [ { name : x , dtype : float32 , shape : (None,)}, { name : y , dtype : float32 , shape : (None,)}, ]","title":"input_spec"},{"location":"packagers/keras/#output_spec","text":"default: None A list of dicts specifying the output of the model. See the documentation for the input_spec parameter for more details. Example : [ { name : out , dtype : float32 , shape : (None,)}, ]","title":"output_spec"},{"location":"packagers/keras/#input_tensor_device","text":"default: None A dict mapping input tensor names to the device that the model expects them to be on. This can either be GPU or CPU . Any tensors in input_spec not specified in this mapping will use the default_input_tensor_device specified below. If a GPU is selected at inference time, Neuropod will move tensors to the appropriate devices before running the model. Otherwise, it will attempt to run the model on CPU and move all tensors (and the model) to CPU. See the docstring for load_neuropod for more info. Example : { x : GPU }","title":"input_tensor_device"},{"location":"packagers/keras/#default_input_tensor_device","text":"default: GPU The default device that input tensors are expected to be on. This can either be GPU or CPU .","title":"default_input_tensor_device"},{"location":"packagers/keras/#custom_ops","text":"default: [] A list of paths to custom op shared libraries to include in the packaged neuropod. Note: Including custom ops ties your neuropod to the specific platform (e.g. Mac, Linux) that the custom ops were built for. It is the user's responsibility to ensure that their custom ops are built for the correct platform. Example : [ /path/to/my/custom_op.so ]","title":"custom_ops"},{"location":"packagers/keras/#package_as_zip","text":"default: True Whether to package the neuropod as a single file or as a directory.","title":"package_as_zip"},{"location":"packagers/keras/#test_input_data","text":"default: None Optional sample input data. This is a dict mapping input names to values. If this is provided, inference will be run in an isolated environment immediately after packaging to ensure that the neuropod was created successfully. Must be provided if test_expected_out is provided. Throws a ValueError if inference failed. Example : { x : np.arange(5), y : np.arange(5), }","title":"test_input_data"},{"location":"packagers/keras/#test_expected_out","text":"default: None Optional expected output. Throws a ValueError if the output of model inference does not match the expected output. Example : { out : np.arange(5) + np.arange(5) }","title":"test_expected_out"},{"location":"packagers/keras/#persist_test_data","text":"default: True Optionally saves the test data within the packaged neuropod.","title":"persist_test_data"},{"location":"packagers/pytorch/","text":"create_python_neuropod Packages arbitrary python code as a neuropod package. create_python_neuropod( neuropod_path, model_name, data_paths, code_path_spec, entrypoint_package, entrypoint, input_spec, output_spec, input_tensor_device = None, default_input_tensor_device = GPU, custom_ops = [], package_as_zip = True, test_input_data = None, test_expected_out = None, persist_test_data = True, ) Params: neuropod_path The output neuropod path model_name The name of the model data_paths A list of dicts containing the paths to any data files that needs to be packaged. Example : [{ path: /path/to/myfile.txt , packaged_name: newfilename.txt }] code_path_spec The folder paths of all the code that will be packaged. Note that *.pyc files are ignored. This is specified as follows: [{ python_root : /some/path/to/a/python/root , dirs_to_package : [ relative/path/to/package ] }, ...] entrypoint_package The python package containing the entrypoint (e.g. some.package.something). This must contain the entrypoint function specified below. entrypoint The name of a function contained in the entrypoint_package . This function must return a callable that takes in the inputs specified in input_spec and returns a dict containing the outputs specified in output_spec . The entrypoint function will be provided the path to a directory containing the packaged data as its first parameter. For example, a function like: def neuropod_init ( data_path ): def addition_model ( x , y ): return { output : x + y } return addition_model contained in the package 'my.awesome.addition_model' would have entrypoint_package='my.awesome.addition_model' and entrypoint='neuropod_init' input_spec A list of dicts specifying the input to the model. For each input, if shape is set to None , no validation is done on the shape. If shape is a tuple, the dimensions of the input are validated against that tuple. A value of None for any of the dimensions means that dimension will not be checked. dtype can be any valid numpy datatype string. Example : [ { name : x , dtype : float32 , shape : (None,)}, { name : y , dtype : float32 , shape : (None,)}, ] output_spec A list of dicts specifying the output of the model. See the documentation for the input_spec parameter for more details. Example : [ { name : out , dtype : float32 , shape : (None,)}, ] input_tensor_device default: None A dict mapping input tensor names to the device that the model expects them to be on. This can either be GPU or CPU . Any tensors in input_spec not specified in this mapping will use the default_input_tensor_device specified below. If a GPU is selected at inference time, Neuropod will move tensors to the appropriate devices before running the model. Otherwise, it will attempt to run the model on CPU and move all tensors (and the model) to CPU. See the docstring for load_neuropod for more info. Example : { x : GPU } default_input_tensor_device default: GPU The default device that input tensors are expected to be on. This can either be GPU or CPU . custom_ops default: [] A list of paths to custom op shared libraries to include in the packaged neuropod. Note: Including custom ops ties your neuropod to the specific platform (e.g. Mac, Linux) that the custom ops were built for. It is the user's responsibility to ensure that their custom ops are built for the correct platform. Example : [ /path/to/my/custom_op.so ] package_as_zip default: True Whether to package the neuropod as a single file or as a directory. test_input_data default: None Optional sample input data. This is a dict mapping input names to values. If this is provided, inference will be run in an isolated environment immediately after packaging to ensure that the neuropod was created successfully. Must be provided if test_expected_out is provided. Throws a ValueError if inference failed. Example : { x : np.arange(5), y : np.arange(5), } test_expected_out default: None Optional expected output. Throws a ValueError if the output of model inference does not match the expected output. Example : { out : np.arange(5) + np.arange(5) } persist_test_data default: True Optionally saves the test data within the packaged neuropod.","title":"PyTorch"},{"location":"packagers/pytorch/#create_python_neuropod","text":"Packages arbitrary python code as a neuropod package. create_python_neuropod( neuropod_path, model_name, data_paths, code_path_spec, entrypoint_package, entrypoint, input_spec, output_spec, input_tensor_device = None, default_input_tensor_device = GPU, custom_ops = [], package_as_zip = True, test_input_data = None, test_expected_out = None, persist_test_data = True, )","title":"create_python_neuropod"},{"location":"packagers/pytorch/#params","text":"","title":"Params:"},{"location":"packagers/pytorch/#neuropod_path","text":"The output neuropod path","title":"neuropod_path"},{"location":"packagers/pytorch/#model_name","text":"The name of the model","title":"model_name"},{"location":"packagers/pytorch/#data_paths","text":"A list of dicts containing the paths to any data files that needs to be packaged. Example : [{ path: /path/to/myfile.txt , packaged_name: newfilename.txt }]","title":"data_paths"},{"location":"packagers/pytorch/#code_path_spec","text":"The folder paths of all the code that will be packaged. Note that *.pyc files are ignored. This is specified as follows: [{ python_root : /some/path/to/a/python/root , dirs_to_package : [ relative/path/to/package ] }, ...]","title":"code_path_spec"},{"location":"packagers/pytorch/#entrypoint_package","text":"The python package containing the entrypoint (e.g. some.package.something). This must contain the entrypoint function specified below.","title":"entrypoint_package"},{"location":"packagers/pytorch/#entrypoint","text":"The name of a function contained in the entrypoint_package . This function must return a callable that takes in the inputs specified in input_spec and returns a dict containing the outputs specified in output_spec . The entrypoint function will be provided the path to a directory containing the packaged data as its first parameter. For example, a function like: def neuropod_init ( data_path ): def addition_model ( x , y ): return { output : x + y } return addition_model contained in the package 'my.awesome.addition_model' would have entrypoint_package='my.awesome.addition_model' and entrypoint='neuropod_init'","title":"entrypoint"},{"location":"packagers/pytorch/#input_spec","text":"A list of dicts specifying the input to the model. For each input, if shape is set to None , no validation is done on the shape. If shape is a tuple, the dimensions of the input are validated against that tuple. A value of None for any of the dimensions means that dimension will not be checked. dtype can be any valid numpy datatype string. Example : [ { name : x , dtype : float32 , shape : (None,)}, { name : y , dtype : float32 , shape : (None,)}, ]","title":"input_spec"},{"location":"packagers/pytorch/#output_spec","text":"A list of dicts specifying the output of the model. See the documentation for the input_spec parameter for more details. Example : [ { name : out , dtype : float32 , shape : (None,)}, ]","title":"output_spec"},{"location":"packagers/pytorch/#input_tensor_device","text":"default: None A dict mapping input tensor names to the device that the model expects them to be on. This can either be GPU or CPU . Any tensors in input_spec not specified in this mapping will use the default_input_tensor_device specified below. If a GPU is selected at inference time, Neuropod will move tensors to the appropriate devices before running the model. Otherwise, it will attempt to run the model on CPU and move all tensors (and the model) to CPU. See the docstring for load_neuropod for more info. Example : { x : GPU }","title":"input_tensor_device"},{"location":"packagers/pytorch/#default_input_tensor_device","text":"default: GPU The default device that input tensors are expected to be on. This can either be GPU or CPU .","title":"default_input_tensor_device"},{"location":"packagers/pytorch/#custom_ops","text":"default: [] A list of paths to custom op shared libraries to include in the packaged neuropod. Note: Including custom ops ties your neuropod to the specific platform (e.g. Mac, Linux) that the custom ops were built for. It is the user's responsibility to ensure that their custom ops are built for the correct platform. Example : [ /path/to/my/custom_op.so ]","title":"custom_ops"},{"location":"packagers/pytorch/#package_as_zip","text":"default: True Whether to package the neuropod as a single file or as a directory.","title":"package_as_zip"},{"location":"packagers/pytorch/#test_input_data","text":"default: None Optional sample input data. This is a dict mapping input names to values. If this is provided, inference will be run in an isolated environment immediately after packaging to ensure that the neuropod was created successfully. Must be provided if test_expected_out is provided. Throws a ValueError if inference failed. Example : { x : np.arange(5), y : np.arange(5), }","title":"test_input_data"},{"location":"packagers/pytorch/#test_expected_out","text":"default: None Optional expected output. Throws a ValueError if the output of model inference does not match the expected output. Example : { out : np.arange(5) + np.arange(5) }","title":"test_expected_out"},{"location":"packagers/pytorch/#persist_test_data","text":"default: True Optionally saves the test data within the packaged neuropod.","title":"persist_test_data"},{"location":"packagers/tensorflow/","text":"create_tensorflow_neuropod Packages a TensorFlow model as a neuropod package. create_tensorflow_neuropod( neuropod_path, model_name, node_name_mapping, input_spec, output_spec, frozen_graph_path = None, graph_def = None, init_op_names = [], input_tensor_device = None, default_input_tensor_device = GPU, custom_ops = [], package_as_zip = True, test_input_data = None, test_expected_out = None, persist_test_data = True, ) Params: neuropod_path The output neuropod path model_name The name of the model node_name_mapping Mapping from a neuropod input/output name to a node in the graph. The :0 is optional. Example : { x : some_namespace/in_x:0 , y : some_namespace/in_y:0 , out : some_namespace/out:0 , } input_spec A list of dicts specifying the input to the model. For each input, if shape is set to None , no validation is done on the shape. If shape is a tuple, the dimensions of the input are validated against that tuple. A value of None for any of the dimensions means that dimension will not be checked. dtype can be any valid numpy datatype string. Example : [ { name : x , dtype : float32 , shape : (None,)}, { name : y , dtype : float32 , shape : (None,)}, ] output_spec A list of dicts specifying the output of the model. See the documentation for the input_spec parameter for more details. Example : [ { name : out , dtype : float32 , shape : (None,)}, ] frozen_graph_path default: None The path to a frozen tensorflow graph. If this is not provided, graph_def must be set graph_def default: None A tensorflow GraphDef object. If this is not provided, frozen_graph_path must be set init_op_names default: [] A list of initialization operator names. These operations are evaluated in the session used for inference right after the session is created. These operators may be used for initialization of variables. input_tensor_device default: None A dict mapping input tensor names to the device that the model expects them to be on. This can either be GPU or CPU . Any tensors in input_spec not specified in this mapping will use the default_input_tensor_device specified below. If a GPU is selected at inference time, Neuropod will move tensors to the appropriate devices before running the model. Otherwise, it will attempt to run the model on CPU and move all tensors (and the model) to CPU. See the docstring for load_neuropod for more info. Example : { x : GPU } default_input_tensor_device default: GPU The default device that input tensors are expected to be on. This can either be GPU or CPU . custom_ops default: [] A list of paths to custom op shared libraries to include in the packaged neuropod. Note: Including custom ops ties your neuropod to the specific platform (e.g. Mac, Linux) that the custom ops were built for. It is the user's responsibility to ensure that their custom ops are built for the correct platform. Example : [ /path/to/my/custom_op.so ] package_as_zip default: True Whether to package the neuropod as a single file or as a directory. test_input_data default: None Optional sample input data. This is a dict mapping input names to values. If this is provided, inference will be run in an isolated environment immediately after packaging to ensure that the neuropod was created successfully. Must be provided if test_expected_out is provided. Throws a ValueError if inference failed. Example : { x : np.arange(5), y : np.arange(5), } test_expected_out default: None Optional expected output. Throws a ValueError if the output of model inference does not match the expected output. Example : { out : np.arange(5) + np.arange(5) } persist_test_data default: True Optionally saves the test data within the packaged neuropod.","title":"TensorFlow"},{"location":"packagers/tensorflow/#create_tensorflow_neuropod","text":"Packages a TensorFlow model as a neuropod package. create_tensorflow_neuropod( neuropod_path, model_name, node_name_mapping, input_spec, output_spec, frozen_graph_path = None, graph_def = None, init_op_names = [], input_tensor_device = None, default_input_tensor_device = GPU, custom_ops = [], package_as_zip = True, test_input_data = None, test_expected_out = None, persist_test_data = True, )","title":"create_tensorflow_neuropod"},{"location":"packagers/tensorflow/#params","text":"","title":"Params:"},{"location":"packagers/tensorflow/#neuropod_path","text":"The output neuropod path","title":"neuropod_path"},{"location":"packagers/tensorflow/#model_name","text":"The name of the model","title":"model_name"},{"location":"packagers/tensorflow/#node_name_mapping","text":"Mapping from a neuropod input/output name to a node in the graph. The :0 is optional. Example : { x : some_namespace/in_x:0 , y : some_namespace/in_y:0 , out : some_namespace/out:0 , }","title":"node_name_mapping"},{"location":"packagers/tensorflow/#input_spec","text":"A list of dicts specifying the input to the model. For each input, if shape is set to None , no validation is done on the shape. If shape is a tuple, the dimensions of the input are validated against that tuple. A value of None for any of the dimensions means that dimension will not be checked. dtype can be any valid numpy datatype string. Example : [ { name : x , dtype : float32 , shape : (None,)}, { name : y , dtype : float32 , shape : (None,)}, ]","title":"input_spec"},{"location":"packagers/tensorflow/#output_spec","text":"A list of dicts specifying the output of the model. See the documentation for the input_spec parameter for more details. Example : [ { name : out , dtype : float32 , shape : (None,)}, ]","title":"output_spec"},{"location":"packagers/tensorflow/#frozen_graph_path","text":"default: None The path to a frozen tensorflow graph. If this is not provided, graph_def must be set","title":"frozen_graph_path"},{"location":"packagers/tensorflow/#graph_def","text":"default: None A tensorflow GraphDef object. If this is not provided, frozen_graph_path must be set","title":"graph_def"},{"location":"packagers/tensorflow/#init_op_names","text":"default: [] A list of initialization operator names. These operations are evaluated in the session used for inference right after the session is created. These operators may be used for initialization of variables.","title":"init_op_names"},{"location":"packagers/tensorflow/#input_tensor_device","text":"default: None A dict mapping input tensor names to the device that the model expects them to be on. This can either be GPU or CPU . Any tensors in input_spec not specified in this mapping will use the default_input_tensor_device specified below. If a GPU is selected at inference time, Neuropod will move tensors to the appropriate devices before running the model. Otherwise, it will attempt to run the model on CPU and move all tensors (and the model) to CPU. See the docstring for load_neuropod for more info. Example : { x : GPU }","title":"input_tensor_device"},{"location":"packagers/tensorflow/#default_input_tensor_device","text":"default: GPU The default device that input tensors are expected to be on. This can either be GPU or CPU .","title":"default_input_tensor_device"},{"location":"packagers/tensorflow/#custom_ops","text":"default: [] A list of paths to custom op shared libraries to include in the packaged neuropod. Note: Including custom ops ties your neuropod to the specific platform (e.g. Mac, Linux) that the custom ops were built for. It is the user's responsibility to ensure that their custom ops are built for the correct platform. Example : [ /path/to/my/custom_op.so ]","title":"custom_ops"},{"location":"packagers/tensorflow/#package_as_zip","text":"default: True Whether to package the neuropod as a single file or as a directory.","title":"package_as_zip"},{"location":"packagers/tensorflow/#test_input_data","text":"default: None Optional sample input data. This is a dict mapping input names to values. If this is provided, inference will be run in an isolated environment immediately after packaging to ensure that the neuropod was created successfully. Must be provided if test_expected_out is provided. Throws a ValueError if inference failed. Example : { x : np.arange(5), y : np.arange(5), }","title":"test_input_data"},{"location":"packagers/tensorflow/#test_expected_out","text":"default: None Optional expected output. Throws a ValueError if the output of model inference does not match the expected output. Example : { out : np.arange(5) + np.arange(5) }","title":"test_expected_out"},{"location":"packagers/tensorflow/#persist_test_data","text":"default: True Optionally saves the test data within the packaged neuropod.","title":"persist_test_data"},{"location":"packagers/torchscript/","text":"create_torchscript_neuropod Packages a TorchScript model as a neuropod package. create_torchscript_neuropod( neuropod_path, model_name, input_spec, output_spec, module = None, module_path = None, input_tensor_device = None, default_input_tensor_device = GPU, custom_ops = [], package_as_zip = True, test_input_data = None, test_expected_out = None, persist_test_data = True, ) Params: neuropod_path The output neuropod path model_name The name of the model input_spec A list of dicts specifying the input to the model. For each input, if shape is set to None , no validation is done on the shape. If shape is a tuple, the dimensions of the input are validated against that tuple. A value of None for any of the dimensions means that dimension will not be checked. dtype can be any valid numpy datatype string. Example : [ { name : x , dtype : float32 , shape : (None,)}, { name : y , dtype : float32 , shape : (None,)}, ] output_spec A list of dicts specifying the output of the model. See the documentation for the input_spec parameter for more details. Example : [ { name : out , dtype : float32 , shape : (None,)}, ] module default: None An instance of a PyTorch ScriptModule. This model should return the outputs as a dictionary. If this is not provided, module_path must be set. For example, a model may output something like this: { output1 : value1, output2 : value2, } module_path default: None The path to a ScriptModule that was already exported using torch.jit.save . If this is not provided, module must be set. input_tensor_device default: None A dict mapping input tensor names to the device that the model expects them to be on. This can either be GPU or CPU . Any tensors in input_spec not specified in this mapping will use the default_input_tensor_device specified below. If a GPU is selected at inference time, Neuropod will move tensors to the appropriate devices before running the model. Otherwise, it will attempt to run the model on CPU and move all tensors (and the model) to CPU. See the docstring for load_neuropod for more info. Example : { x : GPU } default_input_tensor_device default: GPU The default device that input tensors are expected to be on. This can either be GPU or CPU . custom_ops default: [] A list of paths to custom op shared libraries to include in the packaged neuropod. Note: Including custom ops ties your neuropod to the specific platform (e.g. Mac, Linux) that the custom ops were built for. It is the user's responsibility to ensure that their custom ops are built for the correct platform. Example : [ /path/to/my/custom_op.so ] package_as_zip default: True Whether to package the neuropod as a single file or as a directory. test_input_data default: None Optional sample input data. This is a dict mapping input names to values. If this is provided, inference will be run in an isolated environment immediately after packaging to ensure that the neuropod was created successfully. Must be provided if test_expected_out is provided. Throws a ValueError if inference failed. Example : { x : np.arange(5), y : np.arange(5), } test_expected_out default: None Optional expected output. Throws a ValueError if the output of model inference does not match the expected output. Example : { out : np.arange(5) + np.arange(5) } persist_test_data default: True Optionally saves the test data within the packaged neuropod.","title":"TorchScript"},{"location":"packagers/torchscript/#create_torchscript_neuropod","text":"Packages a TorchScript model as a neuropod package. create_torchscript_neuropod( neuropod_path, model_name, input_spec, output_spec, module = None, module_path = None, input_tensor_device = None, default_input_tensor_device = GPU, custom_ops = [], package_as_zip = True, test_input_data = None, test_expected_out = None, persist_test_data = True, )","title":"create_torchscript_neuropod"},{"location":"packagers/torchscript/#params","text":"","title":"Params:"},{"location":"packagers/torchscript/#neuropod_path","text":"The output neuropod path","title":"neuropod_path"},{"location":"packagers/torchscript/#model_name","text":"The name of the model","title":"model_name"},{"location":"packagers/torchscript/#input_spec","text":"A list of dicts specifying the input to the model. For each input, if shape is set to None , no validation is done on the shape. If shape is a tuple, the dimensions of the input are validated against that tuple. A value of None for any of the dimensions means that dimension will not be checked. dtype can be any valid numpy datatype string. Example : [ { name : x , dtype : float32 , shape : (None,)}, { name : y , dtype : float32 , shape : (None,)}, ]","title":"input_spec"},{"location":"packagers/torchscript/#output_spec","text":"A list of dicts specifying the output of the model. See the documentation for the input_spec parameter for more details. Example : [ { name : out , dtype : float32 , shape : (None,)}, ]","title":"output_spec"},{"location":"packagers/torchscript/#module","text":"default: None An instance of a PyTorch ScriptModule. This model should return the outputs as a dictionary. If this is not provided, module_path must be set. For example, a model may output something like this: { output1 : value1, output2 : value2, }","title":"module"},{"location":"packagers/torchscript/#module_path","text":"default: None The path to a ScriptModule that was already exported using torch.jit.save . If this is not provided, module must be set.","title":"module_path"},{"location":"packagers/torchscript/#input_tensor_device","text":"default: None A dict mapping input tensor names to the device that the model expects them to be on. This can either be GPU or CPU . Any tensors in input_spec not specified in this mapping will use the default_input_tensor_device specified below. If a GPU is selected at inference time, Neuropod will move tensors to the appropriate devices before running the model. Otherwise, it will attempt to run the model on CPU and move all tensors (and the model) to CPU. See the docstring for load_neuropod for more info. Example : { x : GPU }","title":"input_tensor_device"},{"location":"packagers/torchscript/#default_input_tensor_device","text":"default: GPU The default device that input tensors are expected to be on. This can either be GPU or CPU .","title":"default_input_tensor_device"},{"location":"packagers/torchscript/#custom_ops","text":"default: [] A list of paths to custom op shared libraries to include in the packaged neuropod. Note: Including custom ops ties your neuropod to the specific platform (e.g. Mac, Linux) that the custom ops were built for. It is the user's responsibility to ensure that their custom ops are built for the correct platform. Example : [ /path/to/my/custom_op.so ]","title":"custom_ops"},{"location":"packagers/torchscript/#package_as_zip","text":"default: True Whether to package the neuropod as a single file or as a directory.","title":"package_as_zip"},{"location":"packagers/torchscript/#test_input_data","text":"default: None Optional sample input data. This is a dict mapping input names to values. If this is provided, inference will be run in an isolated environment immediately after packaging to ensure that the neuropod was created successfully. Must be provided if test_expected_out is provided. Throws a ValueError if inference failed. Example : { x : np.arange(5), y : np.arange(5), }","title":"test_input_data"},{"location":"packagers/torchscript/#test_expected_out","text":"default: None Optional expected output. Throws a ValueError if the output of model inference does not match the expected output. Example : { out : np.arange(5) + np.arange(5) }","title":"test_expected_out"},{"location":"packagers/torchscript/#persist_test_data","text":"default: True Optionally saves the test data within the packaged neuropod.","title":"persist_test_data"}]}